
---
title: "Midterm Data Exercise #1 - Continuous Outcomes"
author: "Kevin McAlister"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    df_print: kable
    theme: leonids
    highlight: github
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
---

```{r, include=FALSE}
library(ggplot2)
library(data.table)
library(ranger)
library(Metrics)
library(tidyverse)
library(glmnet)
library(FNN)
library(mgcv)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

```{r}
#setwd('/Users/billyge/Desktop/Emory Fall 2022/QTM 385-4/problem sets/ps github/midterm1')
setwd('/Users/annie/Desktop/QTM385/QTM350_Github/QTM350 Github/QTM385/midterm1')
train_df <- read.csv('midterm1_train.csv')
test_df <- read.csv('midterm1_testNoShares.csv')
```

Collaborators: Billy Ge, Latifa Tan, Annie Luo
## Question 1 - Implementing a Random Forest Model (10 pts.)
### Part 1
```{r}
#question - is it okay to further downsize the training set??? yess
#question - drop url??? yess
```

```{r}
#sample training dataset
set.seed(123)
strain_df_p1 <- train_df[sample(x=nrow(train_df), size=7500, replace=FALSE),]
stest_df_p1 <- train_df[sample(x=nrow(train_df), size=2500, replace=FALSE),]

#transform variable shares
strain_df_p1$shares <- log(strain_df_p1$shares)
stest_df_p1$shares <- log(stest_df_p1$shares)

#drop variable url
strain_df_p1 <- strain_df_p1[,-1]
stest_df_p1 <- stest_df_p1[,-1]
```

```{r}
mtry <- c(2,4,6,8,10,20,30,40,50,59)
oob <- c()
rf_mse <- c()

for (i in mtry) {
  p1p1_m <- ranger(data=strain_df_p1, formula = shares~., mtry=i, num.trees=1000)
  
  oob <- c(oob,p1p1_m$prediction.error)
  p1p1_pred <- predict(p1p1_m, data=stest_df_p1)
  rf_mse <- c(rf_mse,mse(stest_df_p1$shares, p1p1_pred$predictions))
}
```

```{r}
p1p1_df <- data.frame(mtry,oob,rf_mse)

ggplot(data=p1p1_df) +
  geom_point(aes(x=mtry, y=rf_mse)) +
  geom_line(aes(x=mtry, y=rf_mse)) +
  theme_bw()

ggplot(data=p1p1_df) +
  geom_point(aes(x=mtry, y=oob)) +
  geom_line(aes(x=mtry, y=oob)) +
  theme_bw()
```

The number of features considered for each bootstrap tree that yields the lowest OOB error is 8, whereas the number of features that yields the lowest error on the heldout test set is 20. However, in terms of mse on the heldout test set, there's no significant decrease beyond when the number of features goes above 8. Therefore, we conclude that 8 is a good choice.

In addition, the specific dataset implies that we have more decrease in variance among trees than increase in estimates bias when mtry is smaller than 8, whereas the increase in estimates bias is equal to the decrease in tree variance when mtry goes beyond 8. Therefore, the out of sample mse drops most significantly until mtry hits 8, which is our optimal choice for mtry.


### Part 2
```{r}
p1p2_m <- ranger(data=strain_df_p1, formula = shares~., mtry=8, num.trees=1000, 
                 importance='permutation')

p1p2_df <- data.frame(p1p2_m$variable.importance)
p1p2_df$var <- rownames(p1p2_df)
p1p2_df <- p1p2_df %>% rename(var_imp = p1p2_m.variable.importance)

ggplot(data=p1p2_df, aes(x=var, y=var_imp)) +
  geom_bar(stat='identity', width=0.75) + coord_flip() +
  labs(x="Features", y="Variable Importance") +
  theme_bw()
```

```{r}
p1p2_df %>% arrange(desc(var_imp)) %>% head(2) #most important
p1p2_df %>% arrange(desc(var_imp)) %>% tail(4) #least important

#true relationships
ggplot(data=strain_df_p1) +
  geom_point(aes(x=kw_avg_avg, y=shares)) +
  theme_bw()

ggplot(data=strain_df_p1) +
  geom_point(aes(x=kw_max_avg, y=shares))+
  theme_bw()

#prediction relationships
pred_r <- data.frame(kw_avg_avg = strain_df_p1$kw_avg_avg, 
                     kw_max_avg = strain_df_p1$kw_max_avg,
                     pred_shares = p1p2_m$predictions)

ggplot(data=pred_r) +
  geom_point(aes(x=kw_avg_avg, y=pred_shares)) +
  theme_bw()

ggplot(data=pred_r) +
  geom_point(aes(x=kw_max_avg, y=pred_shares)) +
  theme_bw()
```


## Question 2 - Building a Big Predictive Model (25 pts.)
```{r}
#question - how to solve the collinearity problem??? do not use gam or subset selection
#question - does GAM model have collinearity problem??? yess
#question - how to find the optimal GAM model??? nooo
```

```{r}
#sample training dataset
set.seed(123)
sam_train <- as.numeric(sample(x=nrow(train_df), size=30000, replace=FALSE))
sam_test <- setdiff(seq(1,35000,1), sam_train)
strain_df_p2 <- train_df[sam_train,]
stest_df_p2 <- train_df[sam_test,]

#transform variable shares
strain_df_p2$shares <- log(strain_df_p2$shares)
stest_df_p2$shares <- log(stest_df_p2$shares)

#drop variable url
strain_df_p2 <- strain_df_p2[,-1]
stest_df_p2 <- stest_df_p2[,-1]
```

```{r}
#According to problem 1, we take for granted that 8 is a good choice for the number of variables considered for each bootstrap tree in the random forest model in terms of OOB error and MSE on the heldout data set.
p2_rf_m <- ranger(data=strain_df_p2, formula = shares~., mtry=8, num.trees=1000)
#OOB error
p2_rf_m$prediction.error
#out of sample mse
mse(predict(p2_rf_m, data=stest_df_p2)$predictions, stest_df_p2$shares)
```

```{r}
#According to problem 1 part 2, we can choose a subset of important variables to fit a GAM model.
p1p2_df %>% arrange(desc(var_imp))
p2_gam <- gam(data=strain_df_p2, family=gaussian(), 
              shares ~ s(kw_avg_avg)+s(kw_max_avg)+s(self_reference_avg_sharess))
#in sample mse
mse(predict.gam(p2_gam, strain_df_p2), strain_df_p2$shares)
#out of sample mse
mse(predict.gam(p2_gam, stest_df_p2), stest_df_p2$shares)
```

```{r}
p2_x <- data.matrix(strain_df_p2[,-60])
p2_y <- data.matrix(strain_df_p2[,60])

p2_cv_lassom <- cv.glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=1, nfolds=10,
                          type.measure="mse")
plot(p2_cv_lassom)
print(p2_cv_lassom)
p2_optimal_lassom <- glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=1,
                            lambda=p2_cv_lassom$lambda.min)
#in sample mse
mse(p2_y, predict(p2_optimal_lassom, newx=p2_x))
#out of sample mse
mse(stest_df_p2$shares, predict(p2_optimal_lassom, newx=data.matrix(stest_df_p2[,-60])))
```

```{r}
p2_cv_ridgem <- cv.glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=0, nfolds=10,
                          type.measure="mse")
plot(p2_cv_ridgem)
print(p2_cv_ridgem)
p2_optimal_ridgem <- glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=0,
                            lambda=p2_cv_ridgem$lambda.min)
#in sample mse
mse(p2_y, predict(p2_optimal_ridgem, newx=p2_x))
#out of sample mse
mse(stest_df_p2$shares, predict(p2_optimal_ridgem, newx=data.matrix(stest_df_p2[,-60])))
```

```{r}
knn <- seq(5,1000,100)
knn_mse_out <- c()

for (i in knn) {
  knn_m <- knn.reg(train=p2_x, test=stest_df_p2[,-60], y=p2_y, k=i)
  knn_mse_out <- c(knn_mse_out,mse(knn_m$pred, stest_df_p2$shares))
}

p2_knn_df <- data.frame(knn,knn_mse_out)
ggplot(data=p2_knn_df) +
  geom_point(aes(x=knn, y=knn_mse_out)) +
  geom_line(aes(x=knn, y=knn_mse_out)) +
  theme_bw()
```

```{r}
#conclusion
p2_mse_in_sample <- c(mse(predict.gam(p2_gam, strain_df_p2), strain_df_p2$shares),
                      mse(p2_y, predict(p2_optimal_lassom, newx=p2_x)),
                      mse(p2_y, predict(p2_optimal_ridgem, newx=p2_x)))
p2_mse_out_sample <- c(mse(predict.gam(p2_gam, stest_df_p2), stest_df_p2$shares),
                       mse(stest_df_p2$shares, predict(p2_optimal_lassom, 
                                                       newx=data.matrix(stest_df_p2[,-60]))),
                       mse(stest_df_p2$shares, predict(p2_optimal_ridgem, 
                                                       newx=data.matrix(stest_df_p2[,-60]))))
p2_m <- c('GAM','RIDGE','LASSO')
data.frame(model = p2_m,
           mse_in_sample = p2_mse_in_sample,
           mse_out_sample = p2_mse_out_sample)
```

In addition, the lowest out of sample mse in the knn model is 0.79 when k equals around 100. But the random forest model clearly outperforms all four, with an OOB error of 0.7239764 and out of sample mse of 0.6875225. Hence, the random forest model with the number of variables considered for each bootstrap tree of 8 is our optimal model.

  
## Question 3 - Interpreting Your Big Predictive Model (7 pts.)

```{r}
p3_m <- ranger(data=strain_df_p2, formula = shares~., mtry=8, num.trees=1000,
               importance='permutation')

p3_df <- data.frame(p3_m$variable.importance)
p3_df$var <- rownames(p3_df)
p3_df <- p3_df %>% rename(var_imp = p3_m.variable.importance)

ggplot(data=p3_df, aes(x=var, y=var_imp)) +
  geom_bar(stat='identity', width=0.75) + coord_flip() +
  labs(x="Features", y="Variable Importance") +
  theme_bw()

p3_df %>% arrange(desc(var_imp))
```

According to the variable importance plot from above based on our optimal random forest model from problem 2 trained on a dataset of 30000 observations with the mtry value of 8, some of the most important predictors that dictate the predicted shares of an article are kw_avg_avg, kw_max_avg, self_reference_avg_sharess, kw_min_avg, LDA_02, and timedelta. Articles that are high in those features are likely to be shared more often than those that are not. However, since the random forest model is a good predictive model but not known as a good interpretable model, the important variable plot above may not be very accurate.


## Question 4 - Building a Predictive Model with a Subset of Predictors (25 pts.)
### Demonstration of our method for question 4 optimal model
```{r}
# There are mainly two parts in the work for question4: variable selection and model selection. In variable selection, we will use LASSO and random forest to have some potential optimal-subset of variables. Then, we will use splines plot from the GAM function to have a general picture of the relations between each variable and the log(share).Once we narrow down on 1/2 potential subset of variable, we will move on to the model selection part. We some analysis on what we got in the variable selection and previous work, we decide to apply non-linear model KNN and random forest to the subset of variables. With mse from each potential model, we will finalize the best model and make prediction on test data.
```

### Data ini & recode

```{r}
# take random sample data set for training & testing
set.seed(77)
train_df_q4 <- train_df[sample(x=nrow(train_df), size=7500, replace=FALSE),]
test_df_q4 <- train_df[sample(x=nrow(train_df), size=2500, replace=FALSE),]
# log(share) transformation
train_df_q4$shares <- log(train_df_q4$shares)
test_df_q4$shares <- log(test_df_q4$shares)
#drop variable url
train_df_q4 <- train_df_q4[,-1]
test_df_q4 <- test_df_q4[,-1]
# subset for later
test_df_q4_predictor <- data.matrix(train_df_q4[, -60])
test_df_q4_outcome <- data.matrix(train_df_q4$shares)
```

### Variable selection(at most 5 variables)

We will work on variables selections on LASSO and random forest, and using GAM to check the relationship between each variables and log(share).

#### LASSO variable selection

```{r}
# find optimal lambda
c_lasso_cv <- cv.glmnet(x = test_df_q4_predictor, y = test_df_q4_outcome,
    family = "gaussian", alpha = 1, nfolds = 10, type.measure = "mse")
optimalLambda <- c_lasso_cv$lambda.min # the optimal lambda that minimize MSE
lasso_optimalModel <- glmnet(x = test_df_q4_predictor, y = test_df_q4_outcome,
    family = "gaussian", alpha = 1, lambda = optimalLambda)
opt_variableList <- lasso_optimalModel$beta
opt_variableList
```

5 variables LASSO model suggests: global_rate_negative_words/ global_subjectivity/ n_unique_tokens/ LDA_00/ min_positive_polarity

#### Random Forest variables selection

```{r}
# test and see how many variables we should include
oob <- c()
rf_mse <- c()
for (i in 2:5) {
  model <- ranger(data=train_df_q4, formula = shares~., mtry=i, num.trees=1000)
  oob <- c(oob,model$prediction.error)
  testPred <- predict(model, data=test_df_q4)
  rf_mse <- c(rf_mse,mse(test_df_q4$shares, testPred$predictions))
}
mtry<-c(2,3,4,5)
RFepe <- data.frame(mtry,oob,rf_mse)
# mse suggests 5 variables
ggplot(data=RFepe) +
  geom_point(aes(x=mtry, y=rf_mse)) +
  geom_line(aes(x=mtry, y=rf_mse)) +
  theme_bw()
# oob suggest 4 variables
ggplot(data=RFepe) +
  geom_point(aes(x=mtry, y=oob)) +
  geom_line(aes(x=mtry, y=oob)) +
  theme_bw()
# RF model with 4 variables & 5 variables
RF_model_4 <- ranger(data=train_df_q4, formula = shares~., mtry=4, num.trees=1000,importance='permutation')
RF_model_5 <- ranger(data=train_df_q4, formula = shares~., mtry=5, num.trees=1000,importance='permutation')
# find the top-importance variables for each model
# we won't worry about the negative importance here as they're all generally small
varImportance4 <- data.frame(var_importance=RF_model_4$variable.importance)
arrange(varImportance4, desc(var_importance))
varImportance5 <- data.frame(var_importance=RF_model_5$variable.importance)
arrange(varImportance5, desc(var_importance))
```

4 variable RF model suggests: kw_avg_avg/ self_reference_avg_sharess /kw_max_avg / n_unique_tokens / (5th: kw_min_avg) 
5 variable RF model suggests: kw_avg_avg/ self_reference_avg_sharess/ kw_min_avg/ kw_max_avg / self_reference_min_shares

#### Variables selection sub-conclusion
```{r}
# check suggested variable with spline and decide the final variable subset
checkGam<-gam(shares ~ s(kw_avg_avg) + s(self_reference_avg_sharess) + s(kw_min_avg) + s(kw_max_avg)+s(self_reference_min_shares)+s(global_rate_negative_words)+s(global_subjectivity)+s(n_unique_tokens)+s(LDA_00)+s(min_positive_polarity),data=train_df_q4,family = gaussian())
plot(checkGam,pages=1)
```
We might don't want to approach data analysis on this data set with linear approaches, since most splines don't really show much linear relationship with share. we will move on with knn and random forest with both 4-variable and 5-variable data set

### Model selection
#### data recode with selected subset of data
```{r}
train_df_4RFvar<-data.frame(shares=train_df_q4$shares,kw_avg_avg=train_df_q4$kw_avg_avg,self_reference_avg_sharess=train_df_q4$self_reference_avg_sharess,kw_max_avg=train_df_q4$kw_max_avg,n_unique_tokens=train_df_q4$n_unique_tokens)
test_df_4RFvar<-data.frame(shares=test_df_q4$shares,kw_avg_avg=test_df_q4$kw_avg_avg,self_reference_avg_sharess=test_df_q4$self_reference_avg_sharess,kw_max_avg=test_df_q4$kw_max_avg,n_unique_tokens=test_df_q4$n_unique_tokens)
train_df_5RFvar<-data.frame(shares=train_df_q4$shares,kw_avg_avg=train_df_q4$kw_avg_avg,self_reference_avg_sharess=train_df_q4$self_reference_avg_sharess,kw_min_avg=train_df_q4$kw_min_avg,kw_max_avg=train_df_q4$kw_max_avg,self_reference_min_shares=train_df_q4$self_reference_min_shares)
test_df_5RFvar<-data.frame(shares=test_df_q4$shares,kw_avg_avg=test_df_q4$kw_avg_avg,self_reference_avg_sharess=test_df_q4$self_reference_avg_sharess,kw_min_avg=test_df_q4$kw_min_avg,kw_max_avg=test_df_q4$kw_max_avg,self_reference_min_shares=test_df_q4$self_reference_min_shares)
# list for oob and mse
mse<-c()
oob<-c()
modelName<-c()
# predictor and outcome df based on train_df_4RFvar and train_df_5RFvar
# train set
train_df_4RFvar_predictor <- data.matrix(train_df_4RFvar[, -1])
train_df_4RFvar_outcome <- data.matrix(train_df_4RFvar$shares)
train_df_5RFvar_predictor <- data.matrix(train_df_5RFvar[, -1])
train_df_5RFvar_outcome <- data.matrix(train_df_5RFvar$shares)
# test set
test_df_4RFvar_predictor <- data.matrix(test_df_4RFvar[, -1])
test_df_4RFvar_outcome <- data.matrix(test_df_4RFvar$shares)
test_df_5RFvar_predictor <- data.matrix(test_df_5RFvar[, -1])
test_df_5RFvar_outcome <- data.matrix(test_df_5RFvar$shares)
```

#### Random Forest model
```{r}
# random forest
RFmodel_selected_4var<-ranger(data=train_df_4RFvar, formula = shares~., mtry=4, num.trees=1000)
RFmodel_selected_5var<-ranger(data=train_df_5RFvar, formula = shares~., mtry=5, num.trees=1000)
# OOB
OOB_RF4var<-RFmodel_selected_4var$prediction.error
OOB_RF5var<-RFmodel_selected_5var$prediction.error
# out of sample MSE
MSE_RF4var<-mse(predict(RFmodel_selected_4var, data=test_df_4RFvar)$predictions, test_df_4RFvar$shares)
MSE_RF5var<-mse(predict(RFmodel_selected_5var, data=test_df_5RFvar)$predictions, test_df_5RFvar$shares)
oob<-c(oob,OOB_RF4var,OOB_RF5var)
mse<-c(mse,MSE_RF4var,MSE_RF5var)
modelName<-c(modelName,"RF4var","RF5var")
```

#### KNN model
```{r}
# find the optimal k for knn
knn_mTest<-function(train_pred,train_outcome,test_pred,test_outcome,k){
  knn <- k
  knn_mse_outOfSample <- c()
  for (i in knn) {
    knn_m <- knn.reg(train=train_pred, test=test_pred, y=train_outcome, k=i)
    knn_mse_outOfSample <- c(knn_mse_outOfSample,mse(knn_m$pred, test_outcome))
  }
  p2_knn_df <- data.frame(knn,knn_mse_outOfSample)
  ggplot(data=p2_knn_df) +
    geom_point(aes(x=knn, y=knn_mse_outOfSample)) +
    geom_line(aes(x=knn, y=knn_mse_outOfSample)) +
    theme_bw()
}
# kList<-seq(5,1000,100)# optimal k=105
kList<-seq(5,110,1)# k=33 for 5 var model; k=92 for 4 var model
knn_mTest(train_df_4RFvar_predictor,train_df_4RFvar_outcome,test_df_4RFvar_predictor,test_df_4RFvar_outcome,kList)
knn_mTest(train_df_5RFvar_predictor,train_df_5RFvar_outcome,test_df_5RFvar_predictor,test_df_5RFvar_outcome,kList)
# KNN model for 4Var and 5Var
knn_4Var<- knn.reg(train=train_df_4RFvar_predictor, test=test_df_4RFvar_predictor, y=train_df_4RFvar_outcome, k=92)
knn_4Var_mseOut <- mse(knn_4Var$pred, test_df_4RFvar_outcome)
knn_5Var<- knn.reg(train=train_df_5RFvar_predictor, test=test_df_5RFvar_predictor, y=train_df_5RFvar_outcome, k=33)
knn_5Var_mseOut <- mse(knn_5Var$pred, test_df_5RFvar_outcome)
mse<-c(mse,knn_4Var_mseOut,knn_5Var_mseOut)
modelName<-c(modelName,"knn4Var","knn5Var")
```


#### Model selection sub-conclusion
We randomly take three seeds(77/123/95) and compare the mse for the potential-optimal models we have so far. It turns out that Random Forest models genarally perform better than the KNN ones. We finalized on the random forest model with 4 selected variable performs the best across all seeds(though there is no significant difference between RF4variables and RF5variables).
```{r}
msePrint<-data.frame(modelName,mse)
seed77MSE<-msePrint
#seed123MSE<-msePrint
#seed95MSE<-msePrint
```

### Prediction on midterm1_testNoShares.csv
With our work earlier on, we now have our optimal model(RFmodel_selected_4var): Random Forest model on kw_avg_avg/ self_reference_avg_sharess /kw_max_avg / n_unique_tokens
```{r}
bestModel<-RFmodel_selected_4var
test_df_predictor<-data.frame(kw_avg_avg=test_df$kw_avg_avg,self_reference_avg_sharess=test_df$self_reference_avg_sharess,kw_max_avg=test_df$kw_max_avg,n_unique_tokens=test_df$n_unique_tokens)
bestPredShares<-predict(bestModel, data=test_df_predictor)$predictions
outPutFile<-data.frame(ID=test_df$ID,shares_Pred=bestPredShares)
# save the .csv
write.csv(outPutFile,"Midterm1Q3_Predictions.csv", row.names = FALSE)
```

## Question 5 - Interpreting the Subset Model (8 pts.)
```{r}
# plot the variable importance
bestM<-ranger(data=train_df_4RFvar, formula = shares~., mtry=4, num.trees=1000,importance='permutation')
p5_df <- data.frame(bestM$variable.importance)
p5_df$var <- rownames(p5_df)
p5_df <- p5_df %>% rename(var_imp = bestM.variable.importance)
ggplot(data=p5_df, aes(x=var, y=var_imp)) +
  geom_bar(stat='identity', width=0.75) + coord_flip() +
  labs(x="Features", y="Variable Importance") +
  theme_bw()
p5_df %>% arrange(desc(var_imp))
# variables: 
# kw_avg_avg/ self_reference_avg_sharess /kw_max_avg / n_unique_tokens
# bestModel
```

From question 4, we choose random forest model for good prediction performance, while we sacrifice the interpretability of the model. With our best model on our selected variables, we could only tell the importance of each selected variables shown above. We could see that the Avg. keyword (max. shares)[kw_max_avg] and Avg. keyword (avg. shares)[kw_avg_avg] takes a huge part of the variable importance, while the model also suugests some importance on Avg. shares of referenced articles in Mashable [self_reference_avg_sharess] and Rate of unique words in the content [n_unique_tokens]. The result is quite similar with what we got in the full model: all of our four varibels in Q4 are also in the top 8 importance of the full model; 3 out of 4 are the most importance among all variables in full model. 

Since random forest doesn't return any coefficient as the linear models do, we don't really know for sure whether these variables are positively OR negatively correlate to the outcome[shares]. With our best educational guess, we might take a conjecture that highest number of shares articles will be those which: 1) keywords with good amount of shares(both top number of shares and average number of shares) in the past; 2) reference on high-shares articles; 3) using good amount of high rate unique words. The lowest number of shares articles will be those don't do well on our conjecture.

Actionable rules: 
1) have a good idea about popular "keywords" on Mashable and use those keywords as frequent as possible.If not sure what are those "keywords", use diverse vocabulary so that you will hit some by chance.
2) include reference to the popular articles on Mashable. If not sure about what's the *most popular ones, maybe refer to as many articles as possible such that you will somehow hit one.
3) use unique vocabulary with huge diversity.

why we not expect the rule set makes sense:
There are many confounding variables that we are unable to include in our model. For example, the recommend-article-to-reader algorithm might have bias on some types of articles and thus recommend those articles more frequently. In this way, those articles with high exposure to the public will naturally have larger audiences group and higher amount of shares.

## Question 6 - Building a Predictive Model with a Single Predictor (15 pts.)
### Initial sampling and recoding
```{r}
#sample training dataset
set.seed(456)
sam_train_p6 <- as.numeric(sample(x=nrow(train_df), size=30000, replace=FALSE))
sam_test_p6 <- setdiff(seq(1,35000,1), sam_train_p6)
strain_df_p6 <- train_df[sam_train_p6,]
stest_df_p6 <- train_df[sam_test_p6,]

#transform variable shares
strain_df_p6$shares <- log(strain_df_p6$shares)
stest_df_p6$shares <- log(stest_df_p6$shares)

#drop variable url
strain_df_p6 <- strain_df_p6[,-1]
stest_df_p6 <- stest_df_p6[,-1]
```

We start by getting some sense on what might be a good choice of the uni-variable. Since OLS is dominated by ridge and lasso, we will not use OLS. Since GAM with one "s(variable)" term is the same with spline model, we will only use the gam function. We will use a for-loop to iterate all 59 variables on the rest of possible models we learned--KNN, ridge, lasso, polynomial expansion, spline, LOESS, RF with mtry = 1, and compute and compare in-sample and out-of-sample EPE to pick out best model and best variable.

### KNN
```{r}
#out-of-sample
knn_p6_mse_out <- c()
optimalk<-c()
variableIndex<- c()
#test for all 59 variables
knn <- seq(5,1000,50)
for (j in 1:59) {
  knn_mse_out_k <- c()
  for (i in knn){ #choose k for each variable
    knn_model_k <- knn.reg(train=as.data.frame(strain_df_p6[,j]), test=as.data.frame(stest_df_p6[,j]), y=as.data.frame(strain_df_p6[,60]), k=i)
    knn_mse_out_k <- c(knn_mse_out_k, mse(knn_model_k$pred, stest_df_p6$shares))
  }
  knn_p6_m <- knn.reg(train=as.data.frame(strain_df_p6[,j]), test=as.data.frame(stest_df_p6[,j]), y=as.data.frame(strain_df_p6[,60]), k=knn[which.min(knn_mse_out_k)])
  optimalk<-c(optimalk,knn[which.min(knn_mse_out_k)])
  variableIndex<-c(variableIndex,j)
  knn_p6_mse_out <- c(knn_p6_mse_out,mse(knn_p6_m$pred, stest_df_p6$shares))
}
p6_knn_out_df <- data.frame(var = colnames(strain_df_p6[,-60]), knn_p6_mse_out)
p6_knn_out_df %>% arrange(knn_p6_mse_out)
optimalK_var_list<-data.frame(varIndex=variableIndex,optK=optimalk)
#most ideal variable is kw_avg_avg based on out-of-sample EPE

#in-sample using KCV
knn_reg_kfold <- function(x, y, k, folds){
  set.seed(456)
  #sample
  sam6_KNN_kfold_train <- as.numeric(sample(nrow(as.data.frame(x)), size=ceiling(nrow(as.data.frame(x))/folds), replace=FALSE))
  sam6_KNN_kfold_test <- setdiff(seq(1,30000,1), sam6_KNN_kfold_train)
  #model for prediction error
  p6_p <- c()
  for (i in 1:folds){
    x_train <- x[sam6_KNN_kfold_train,]
    x_test <- x[sam6_KNN_kfold_test,]
    y_train <- y[sam6_KNN_kfold_train]
    y_test <- y[sam6_KNN_kfold_test]
    p6_KNN_kfoldm <- knn.reg(train = as.data.frame(x_train), test = as.data.frame(x_test), y = as.data.frame(y_train), k = k)
    p6_p[i] <- mse(p6_KNN_kfoldm$pred, as.numeric(unlist(y_test)))
  }
  return(p6_p[1]/folds)
}
p6_kfold_5 <- c()
p6_kfold_10 <- c()
for (j in 1:59) {
  print(j)
  p6_kfold_5[j] <- knn_reg_kfold(strain_df_p6[,j], strain_df_p6[,60], optimalk[j], 5)
  p6_kfold_10[j] <- knn_reg_kfold(strain_df_p6[,j], strain_df_p6[,60], optimalk[j], 10)
}
p6_knn_in_df <- data.frame(var = colnames(strain_df_p6[,-60]), p6_kfold_5, p6_kfold_10)
p6_knn_in_df %>% arrange(p6_kfold_5)
p6_knn_in_df %>% arrange(p6_kfold_10)

```
For KNN, we found that "kw_avg_avg" is a good uni-predictor for out-of-sample EPE, and "data_channel_is_world" is a good uni-predictor for in-sample EPE both for 5 and 10 fold CV. Since variables picked based on in-sample and out-of-sample EPE are very different, we will go with "kw_avg_avg" as other models all agree on "kw_avg_avg" being the best univariable.
```{r}
colnames(strain_df_p6) #weekday_is_wednesday is when 27th variable
p6_result <- data.frame(model = "KNN", var = p6_knn_out_df[27,1], mse_out = p6_knn_out_df[27,2], mse_in = p6_knn_in_df[27,2])
p6_result
```
We recorded the EPE of KNN model using weekday_is_wednesday above. We can see that the in-sample EPE is surprisingly good, but since its oos EPE doesn't look ideal, we will wait and see other models. For easier comparison, we only recorded 5fold CV to be out in sample mse, since it's only slightly higher than 10fold.

### LASSO
```{r}
#scientific lambda
p6_cv_lasso <- cv.glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=1, nfolds=10,
                          type.measure="mse")
p6_lasso <- glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=1,
                            lambda=p6_cv_lasso$lambda.min)
p6_lasso$beta
cf1 <- c()
cf2 <- c()
for (i in 1:59) {
  cf1 <- append(cf1, rownames(p6_lasso$beta)[i])
  cf2 <- append(cf2, unname(p6_lasso$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
  geom_bar(stat="identity", width=0.75) + coord_flip() +
  labs(x="Features", y="Coefficient Estimate") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="red", size = 12),
        axis.title.y = element_text(face="bold", colour="red", size = 12))
#variable we chose is global_rate_positive_words (46th var) since its absolute value of coefficient is biggest
#p6_cv_lasso_op <- cv.glmnet(x=data.matrix(stest_df_p6[,46]), y=p6_y, family="gaussian", alpha=1,
#                            nfolds=10, type.measure="mse")
#p6_lasso_op <- glmnet(x=data.matrix(stest_df_p6[,46]), y=p6_y, family="gaussian", alpha=1,
#                      lambda=p6_cv_lasso_op$lambda.min)
```
While we found that, according to LASSO, "global_rate_positive_words" is a good variable, we cannot run LASSO model based on only this one variable. Since for other models, we would use a for loop to test all variables, LASSO cannot help us with variable selection. For the same reason, we will not use Ridge.

### Polynomial Expansion
We continue to use strain_df_p6 and stest_df_p6
```{r}
#We choose LOOCV as there is less worry about bias and variance than other approaches and it's asymptotically unbiased for the in-sample EPE.

#select variable based on in sample EPE
lm_pred_metrics <- function(fit_model) {
  LOOCV <- mean((fit_model$residuals / (1-hatvalues(fit_model)))^2)
  return (LOOCV)
}
#exclude binary variables and variables with small number of possible values since degree of polynomial cannot exceed number of possible values
index <- c(1:3, 7:12, 19:30, 39:59)
p6_poly_mse <- c()
p6_poly_deg <- c()
for (i in index){
  p6_poly_1 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],1))
  p6_poly_2 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],2))
  p6_poly_3 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],3))
  p6_poly_4 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],4))
  p6_poly_5 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],5))
  p6_poly_6 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],6))
  p6_poly_7 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],7))
  p6_poly_8 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],8))
  p6_poly_df <- cbind(lm_pred_metrics(p6_poly_1), lm_pred_metrics(p6_poly_2),
                        lm_pred_metrics(p6_poly_3), lm_pred_metrics(p6_poly_4),
                        lm_pred_metrics(p6_poly_5), lm_pred_metrics(p6_poly_6),
                        lm_pred_metrics(p6_poly_7), lm_pred_metrics(p6_poly_8))
  p6_poly_mse <- c(p6_poly_mse, min(p6_poly_df))
  p6_poly_deg <- c(p6_poly_deg, which.min(p6_poly_df))
}
p6_poly <- data.frame(mse = p6_poly_mse, degree = p6_poly_deg)
which.min(p6_poly$mse) #18th obs. in p6_poly df
min(p6_poly$mse) #lowest mse is 0.8092535, degree is 7.
index[which.min(p6_poly$mse)] #27th variable in strain_df_p6, which is kw_avg_avg
```
From above, we see that, if we were to use a polynomial model, we would use variable "kw_avg_avg" and set function degree to be 7.
```{r}
#in sample EPE is:
p6_poly_optimal <- lm(data = strain_df_p6, shares~poly(kw_avg_avg,7))
p6_poly_in <- lm_pred_metrics(p6_poly_optimal)
p6_poly_in
#out of sample EPE is:
p6_poly_pred <- p6_poly_optimal %>% predict(stest_df_p6)
p6_poly_out <- mse(p6_poly_pred, stest_df_p6$shares)
p6_poly_out
#compile into final result
p6_poly_result <- data.frame(model = "Polynomial_deg_7", var = "kw_avg_avg", mse_out = p6_poly_out, mse_in = p6_poly_in)
p6_result <- rbind(p6_result, p6_poly_result)
p6_result
```

### Spline
We continue to use strain_df_p6 and stest_df_p6
```{r}
strain_df_p6 <- as.data.frame(strain_df_p6)
stest_df_p6 <- as.data.frame(stest_df_p6)
#in sample
#non-binary variables
index_nb_gam <- c(1:4, 6:12, 19:30, 39:59)
nonbinary_s <- c()
for (i in index_nb_gam){
  p6_nb_spline_m <- gam(shares ~ s(strain_df_p6[,i]),data=strain_df_p6,family = gaussian())
  nonbinary_s<-c(nonbinary_s, p6_nb_spline_m$gcv.ubre.dev)
}
min(nonbinary_s) #0.805739
index_nb_gam[which.min(nonbinary_s)] #27th variable is kw_avg_avg (seems to be better candidate)
#binary variables
index_b_gam <- c(5, 13:18, 31:38)
binary_s <- c()
for (i in index_b_gam){
  p6_b_spline_m <- gam(shares ~ strain_df_p6[,i],data=strain_df_p6,family = gaussian())
  binary_s<-c(binary_s, p6_b_spline_m$gcv.ubre.dev)
}
min(binary_s) #0.8459762
#since non-binary performs better based on in sample EPE, we use kw_avg_avg for spline
p6_spline_optimal <- gam(shares ~ s(kw_avg_avg),data=strain_df_p6,family = gaussian())
p6_spline_in <- p6_spline_optimal$gcv.ubre.dev
p6_spline_in
#out of sample
p6_spline_out<-mse(stest_df_p6$shares,predict.gam(p6_spline_optimal,stest_df_p6))
p6_spline_out
#compile into final result
p6_spline_result <- data.frame(model = "Spline", var = "kw_avg_avg", mse_out = p6_spline_out, mse_in = p6_spline_in)
p6_result <- rbind(p6_result, p6_spline_result)
p6_result
```

### Random Forest
```{r}
# pick best variable
RF_model_1 <- ranger(data=strain_df_p6, formula = shares~., mtry=1, num.trees=1000,importance='permutation')
varImportance1 <- data.frame(var_importance=RF_model_1$variable.importance)
arrange(varImportance1, desc(var_importance)) #most important variable is kw_avg_avg

#RF model
p6_rf_train <- strain_df_p6 %>% select(kw_avg_avg, shares)
p6_rf_test <- stest_df_p6 %>% select(kw_avg_avg, shares)
RFmodel_p6<-ranger(data=p6_rf_train, formula = shares~., mtry=1, num.trees=1000)
# OOB
OOB_p6<-RFmodel_p6$prediction.error
OOB_p6 #in-sample EPE is 1.07784
# out of sample MSE
p6_RF_out<-mse(predict(RFmodel_p6, data=p6_rf_test)$predictions, p6_rf_test$shares)
p6_RF_out #out-of-sample EPE is 1.089439
p6_RF_result <- data.frame(model = "Random Forest", var = "kw_avg_avg", mse_out = p6_RF_out, mse_in = OOB_p6)
p6_result <- rbind(p6_result, p6_RF_result)
p6_result
```

### Conclusion and Plot
Based on p6_result table, we can see that all methods produced "kw_avg_avg" as the most important variable for univariable predictions, so we will choose this variable. While all out-of-sample EPE are similar across models but KNN has a significantly lower in-sample EPE, we will not choose KNN since the variable we chose for KNN was quite arbiturary in the sense that the in-sample EPE of KNN didn't really produce "kw_avg_avg" as the most ideal predictor. Random Forest overall performs worse than others. Polynomial expansion and Spline (GAM) have more or less the same mse, and we choose to go with Spline (GAM) and we use "kw_avg_avg." 
```{r}
p6_optimal <- gam(shares ~ s(kw_avg_avg),data=strain_df_p6,family = gaussian())
#training set
ggplot(data=strain_df_p6, aes(x=kw_avg_avg, y=shares)) +
  geom_point()+
  geom_point(size=0.8, aes(x=kw_avg_avg,y=p6_optimal$fitted.values), color='red')
#test set
p6_pred <- predict(p6_optimal, newdata = stest_df_p6)
ggplot(data=stest_df_p6, aes(x=kw_avg_avg, y=shares)) +
  geom_point()+
  geom_point(size=0.8, aes(x=kw_avg_avg,y=p6_pred), color='red')
```
CSV:
```{r}
p6PredShares<-predict(p6_optimal, newdata=test_df)
p6outPutFile<-data.frame(ID=test_df$ID,shares_Pred=p6PredShares)
# save the .csv
write.csv(p6outPutFile,"Midterm1Q6_Predictions.csv", row.names = FALSE)
print(Midterm1Q6_Predictions.csv)
```

## Question 7 - Do your models perform well on out-of-sample data? (10 pts.)

For each model, I'll compute a root mean squared prediction error using your submitted predictions and the true log number of shares for the articles in the hidden test set.  Across all models turned in, points will be allocated as follows:

  1. For Q1 and Q3, let $\text{RMSE}_{\text{Min}}$ be the minimum prediction error over the submitted models.  Let $\text{RMSE}_i$ be your predictive error.  Your prediction points will be computed as:
  
  $$4 \times \frac{\text{RMSE}_{\text{Min}}}{\text{RMSE}_{i}}$$
  
  2. For Q5 it will be:
  
  $$2 \times \frac{\text{RMSE}_{\text{Min}}}{\text{RMSE}_{i}}$$

The goal here is to add a low-stakes incentive and fun "competition" to try to optimize your predictive models.  **I expect that the performance across groups/students will be quite close, so this won't greatly affect your final grade.**

