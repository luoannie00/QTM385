
---
title: "Midterm Data Exercise #1 - Continuous Outcomes"
author: "Kevin McAlister"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    df_print: kable
    theme: leonids
    highlight: github
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
---

```{r, include=FALSE}
library(ggplot2)
library(data.table)
library(ranger)
library(Metrics)
library(tidyverse)
library(glmnet)
library(FNN)
library(mgcv)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

```{r}
#setwd('/Users/billyge/Desktop/Emory Fall 2022/QTM 385-4/problem sets/ps github/midterm1')
#setwd('/Users/annie/Desktop/QTM385/QTM350_Github/QTM350 Github/QTM385/midterm1')
train_df <- read.csv('midterm1_train.csv')
test_df <- read.csv('midterm1_testNoShares.csv')
```

Collaborators: Billy Ge, Latifa Tan, Annie Luo

This is the first midterm data analysis exercise for QTM 385 - Introduction to Statistical Learning.  This exercise will use a single data set to ask a few questions related to creating predictive models for a continuous outcome.  This data set is larger and messier than those that have been given for the problem set exercises.  It is intended to provide you with a "real-world" scenario you might see if attempting to build a predictive model for a question you care about.  This assignment is worth 12.5% of your final grade and your final solutions should be submitted by 11:59 PM on October 17th.  Your final submission should include a .Rmd/.md/.ipynb file, a rendered .pdf/.html version of your notebook, and three .csv files including your predictions for an unseen test set.

Unlike the problem sets, there will be little guidance as to what methods to use and how to interpret results.  The idea is that you can treat this like a real analysis exercise - applying different models, seeing what works and doesn't work, presenting the best possible model but being transparent about the downsides of you choice.  For each of the three tasks, you should consider a variety of potential methods and choose a single one as your "best" model.

A review of fitting methods that we've covered in this course (thus far):

  1. Linear regression via OLS
  2. Polynomial regression
  3. K-nearest neighbors regression
  4. Shrinkage Methods (Ridge and LASSO)
  5. Splines for univariate relationships
  6. Local linear regression (LOESS)
  7. Generalized Additive Models (GAMs)
  8. Bagged Regression Trees and Random Forests 
  9. (For the adventurous) Boosted Regression Trees
  
A review of concepts that should be central in your answers:

  1. Training Error vs. Test Error
  2. Train/Validation/Test Splits
  3. Expected Prediction Error
  4. Cross-validation
  5. Predictor Selection
  6. Linear vs. Nonlinear Relationships
  7. Non-additive Errors
  8. Bagging
  9. (For the adventurous) Boosting
  
***

## Data Set Overview

This assignment revolves around a data set of 39,644 articles published by the website [mashable.com](https://mashable.com/) in 2013 and 2014.  Mashable is an international multi-platform media and entertainment company that can be seen as a news aggregator; the site publishes its own articles about current events and topics.  In many ways, Mashable was (and still is, but it's under different ownership now) a content aggregator.  The goal of Mashable was to provide an aggregate of important daily "news" and to generate ad-revenue via advertising clicks.  Therefore, the profitability of the website was directly tied to its ability to attract users to the site.  The most common way users were attracted was by shared articles passing through various social media outlets.  More shares leads to more ad revenue!

The data set includes article urls, the number of shares on social media via the site's provided sharing mechanisms, and a collection of 59 covariates related to the length and content of the articles.  It is important to note that all 59 of these covariates could potentially be measured *prior to publication*.  The reason for this can be seen in the original paper that collected and used this data set - ["A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News" by Kelwin Fernandes, Pedro Vinagre, and Paulo Cortez](https://repositorium.sdum.uminho.pt/bitstream/1822/39169/1/main.pdf).  In this paper, the authors want to demonstrate that predictive models can be used to create actionable suggestions to increase the number of shares that an article receives.  In doing so, they first build a predictive model that is expected to perform well on unseen data (articles that have not yet been written) and use this to create a rules set that provides expected improvements for new articles.  The proposed IDSS is operationalized over keywords - summary words that are used to categorize an article.  However, it is easy to see how a system of this sort could be used to create content suggestions for the writers, themselves, to write the "clickbaitiest" articles possible.

The idea of interpretable and actionable rules sets is central to the study of fairness and bias in machine learning and actionable prediction models.  Though most of the models we've discussed in class are of the interpretable variety (KNN regression being the exception) - we can viably see how a predictor increases or decreases the resulting prediction - the resulting prediction surfaces can be very complex.  These methods allow "transparent" decision making processes and ensure that we're not making decision based on fragile or unethical logical links (redlining for mortgages, for example).  While this data set does not present any avenues for ethically dubious prediction methods (I don't think, at least), you may find associations that are clearly non-causal.  You'll discuss these as part of one of the problems.

Many of the covariates presented as processed natural language processing (NLP) metrics that are intended to capture the topic, positive/negative sentiment (polarity), and subjectivity of the article.  I'll briefly summarize these concepts here:

  1. Topics are presented using LDA dimensions that sum to 1.  Latent Dirichlet allocation is a common method of finding common "themes" and word associations across a large corpus of text documents.  The authors ran an LDA analysis and settled on 5 "topics" for the entire data set.  Each document is assumed to be a mixture of these 5 topics and can be described by a weight that corresponds to how much each document contains each topic.  A document with weight 1 on a single topic means that it is wholly decribed by that one topic while weight zero means that there is no part of the document that speaks to the topic.  Since this is a mixture, the weights sum to one.  There is no guarantee that the LDA topics are coherent, but you can find similarities between articles that are close to each topic.  Should you use one of the LDA dimensions in your models (especially your small models), you may want to undertake the task of figuring out what each of them means.  **A warning: since the LDA weights for each article sum to 1, placing all 5 into a model that doesn't select away collinear features will run into issues!**
  
  2. Polarity is a measure of the positiveness or negativeness of a document.  This is typically computed using a dictionary of positive and negative words and then using associational methods for determining whether words that are not recognized are positive or negative.  For this data set, negative polarity means that the document is largely negative while positive polarity means that the document is largely positive.  Related measures look for the rate of positive and negative words, title polarity, and conditional polarity for only positive or negative words.
  
  3. Subjectivity is a measure of how "fact-based" an article is.  Values of 0 mean that the document is fully fact-based while values of 1 mean that the document is wholly opinion based.  The machinery of the subjectivity measure largely mimics that of the polarity measure starting with a dictionary of fact vs. opinion based words and using associations to cover new words.
  
The rest of the covariates relate to specific features of the articles - keyword usage, data channels, etc.  A short description of each covariate can be seen in the `OnlineNewsPopularity.names` file included with this assignment.  Further descriptions of the data set can be found in the original paper.  The paper is relatively short, so I highly recommend reading it to get a sense of what the authors are doing and how the features are used to produce this automated decision making system.
  
A final thing to watch out for are two sets of unordered categorical predictors - day of publication and data channel.  All of these categorical predictors are already "dummied out", so you don't need to do anything special to make them usable.  However, be cautious if building a linear model!  The day of the week dummies are also accompanied by `is_weekend` which is just a linear combination of the other predictors.  The data channel predictors are binary variables that tell which large topic each article falls under - one of lifestyle, entertainment, business, social media, technology, world news/issues, and viral content.  Viral content was dropped from the predictor set.  As best I can tell, any article that has a zero on all other data channels falls into the viral content category.  You can include this variable in your models if you'd like by adding a column of appropriate indicators.

***

## Overall Goal

For this assignment, you'll take on the role of a data scientist attempting to understand and **predict** what articles are most likely to to garner the most shares.  You will have access to a training set of 35,000 observations each corresponding to a specific article.  Each observation has 1) the article url, 2) the number of shares, and 3) 59 covariates related to the length and content of the article.  Your goal is to build predictive models that will predict the number of shares for an article on data that was not used to train the model.

Your grade on this assignment will be a combination of:

  1. Explaining your model building process
  2. Assessing the out-of-sample predictive abilities for your chosen models
  3. Interpreting your predictive models (plots and similar exercises)
  4. Generating predictions using a test set of features **where you will not know the true outcome**
  
You do not need to try all prediction methods for each question.  Rather, I would like you to justify why you aren't considering certain methods.  For example, you can justify not using OLS by saying that we know that Ridge is guaranteed to produce a lower mean squared prediction error than OLS in almost any situation.  This is sufficient reasoning!

We've discussed multiple ways of approximating the unseeable expected prediction error quantity.  The first set of approaches leverage the training data **only** to compute an estimate of the EPE - these methods include cross-validation and out-of-bag approaches.  The second method leverages an actual held out test data set that can be used to truly assess the EPE - the data is large enough to hold out a subset!  Given the size of this data set, I highly recommend that you use a combination of the two to make any model choice decisions.

Most of the methods we've discussed in class will move quite quickly on data of this size.  KNN regression, bagged regression trees, random forests, and boosted trees will take a little longer.  If you run into problems with the scale of the data, feel free to take a random subset of the training data.  You won't be penalized for this reduction in data size, but your models are unlikely to score as well as the large $N$ models in terms of actual out-of-sample predictive ability.  You may also reduce the size of the data set to **tune** any hyperparameters for any specific model.  This may reduce the computational time needed to complete the assignment.  All said, I'm looking for students to attempt to produce good predictive models for the outcome but I am well aware of the computational limitations that you all will experience.  Just try your best!

I have a hidden test set of 4,644 articles that were randomly selected from the full data set that will be used to assess the predictive accuracy of your chosen approaches.  You will provide three .csv files that include your predictions for the unseen test data set using the three models you choose in this exercise.  The test data set, included as `midterm1_testNoShares`, has all of the features of the original data set along with an additional `ID` column.  Your .csv files should only include two columns: the `ID` of the observation and the `prediction` made using your chosen method.      

***

## Question 1 - Implementing a Random Forest Model (10 pts.)

Let's walk through the process of implementing and interpreting a random forest model in this first question.

### Part 1

Begin by taking a random sample of 7,500 training observations and use this as your training set.  Similarly, take a random sample of 2,500 training observations and use this as a heldout test set.  

Using your reduced training set, implement a series of random forest using all predictors that creates predictions for the **log number of shares**.  For each random forest, you should generate 1,000 bootstrap trees.  To try to "tune" the random forest, vary the number of variables to consider in each bootstrap tree (`mtry` in `ranger`).  At a minimum, consider `mtry = c(2,4,6,8,10,20,30,40,50,59)`.

*Note*: The bagged tree with all 59 predictors may take a few minutes to run.

For each value of `mtry`, record the OOB error estimate and compute the MSE for your held out test data set.  Create a plot that shows these two curves as a function of the number of variables considered in each tree.

Which number of features considered for each bootstrap tree yields the lowest OOB error?  What about the lowest error on your heldout test set?

What does this say about the bias/variance tradeoff for this specific data set?  Does out-of-sample predictive accuracy favor higher variance among trees or lower bias in the estimates?

```{r}
#question - is it okay to further downsize the training set??? yess
#question - drop url??? yess
```

```{r}
#sample training dataset
set.seed(123)
strain_df_p1 <- train_df[sample(x=nrow(train_df), size=7500, replace=FALSE),]
stest_df_p1 <- train_df[sample(x=nrow(train_df), size=2500, replace=FALSE),]

#transform variable shares
strain_df_p1$shares <- log(strain_df_p1$shares)
stest_df_p1$shares <- log(stest_df_p1$shares)

#drop variable url
strain_df_p1 <- strain_df_p1[,-1]
stest_df_p1 <- stest_df_p1[,-1]
```

```{r}
mtry <- c(2,4,6,8,10,20,30,40,50,59)
oob <- c()
rf_mse <- c()

for (i in mtry) {
  p1p1_m <- ranger(data=strain_df_p1, formula = shares~., mtry=i, num.trees=1000)
  
  oob <- c(oob,p1p1_m$prediction.error)
  p1p1_pred <- predict(p1p1_m, data=stest_df_p1)
  rf_mse <- c(rf_mse,mse(stest_df_p1$shares, p1p1_pred$predictions))
}
```

```{r}
p1p1_df <- data.frame(mtry,oob,rf_mse)

ggplot(data=p1p1_df) +
  geom_point(aes(x=mtry, y=rf_mse)) +
  geom_line(aes(x=mtry, y=rf_mse)) +
  theme_bw()

ggplot(data=p1p1_df) +
  geom_point(aes(x=mtry, y=oob)) +
  geom_line(aes(x=mtry, y=oob)) +
  theme_bw()
```

The number of features considered for each bootstrap tree that yields the lowest OOB error is 8, whereas the number of features that yields the lowest error on the heldout test set is 20. However, in terms of mse on the heldout test set, there's no significant decrease beyond when the number of features goes above 8. Therefore, we conclude that 8 is a good choice.

In addition, the specific dataset implies that we have more decrease in variance among trees than increase in estimates bias when mtry is smaller than 8, whereas the increase in estimates bias is equal to the decrease in tree variance when mtry goes beyond 8. Therefore, the out of sample mse drops most significantly until mtry hits 8, which is our optimal choice for mtry.


### Part 2

Using your chosen value for the number of variables to consider in your random forest, compute the permutation importance of the features for your subset of the training data.  Create a graph that demonstrates which variables are most important for the regression tree and which ones are least important.

Explore the relationship between the two most important predictors and the log number of shares.  Can you parse out the relationship between these variables and the predictions?

*Hint*: Try to see how these two variables and the true number of shares move together.  Then, do the same for the values of these variables and the corresponding predictions.  It probably isn't super clear, but you can likely see a bit of a trend.

```{r}
p1p2_m <- ranger(data=strain_df_p1, formula = shares~., mtry=8, num.trees=1000, 
                 importance='permutation')

p1p2_df <- data.frame(p1p2_m$variable.importance)
p1p2_df$var <- rownames(p1p2_df)
p1p2_df <- p1p2_df %>% rename(var_imp = p1p2_m.variable.importance)

ggplot(data=p1p2_df, aes(x=var, y=var_imp)) +
  geom_bar(stat='identity', width=0.75) + coord_flip() +
  labs(x="Features", y="Variable Importance") +
  theme_bw()
```

```{r}
p1p2_df %>% arrange(desc(var_imp)) %>% head(2) #most important
p1p2_df %>% arrange(desc(var_imp)) %>% tail(1) #least important

#true relationships
ggplot(data=strain_df_p1) +
  geom_point(aes(x=kw_avg_avg, y=shares)) +
  theme_bw()

ggplot(data=strain_df_p1) +
  geom_point(aes(x=kw_max_avg, y=shares))+
  theme_bw()

#prediction relationships
pred_r <- data.frame(kw_avg_avg = strain_df_p1$kw_avg_avg, 
                     kw_max_avg = strain_df_p1$kw_max_avg,
                     pred_shares = p1p2_m$predictions)

ggplot(data=pred_r) +
  geom_point(aes(x=kw_avg_avg, y=pred_shares)) +
  theme_bw()

ggplot(data=pred_r) +
  geom_point(aes(x=kw_max_avg, y=pred_shares)) +
  theme_bw()
```


## Question 2 - Building a Big Predictive Model (25 pts.)

Using all available predictors, come up with a predictive approach that maximizes the expected predictive accuracy for the **log number of shares** for articles in the unseen test set.  Your model for this problem should use any tools we've covered (or that you know from prerequisite courses) to build the predictive model that **minimizes the expected prediction error on the unseen test set**.  It can use any number of predictors and any functional form you can uncover.  Obviously, you don't have this test set, so you can never really know!  However, we've covered methods for approximating this value (to the best of our non-omniscient ability).

Please explain your thought process as to the choices you make!

For any method that you seriously consider as a candidate for producing the best possible predictive model, collect the following quantities and include them as a row in a table:

  1. An in-sample estimate of the EPE generated using appropriate methods
  2. A true out-of-sample estimate of the EPE generated using appropriate methods
  3. The time (in seconds) required for you to tune the model and run it on the full training data **together**.  This time should include the computational time needed to choose the values of any hyperparameters and run the model of the full training data to get a prediction method for unseen data.  There are many different ways to interpret and accomplish this task, but there is a general trend that you should see when training your models. 

Once you decide on a final model, use the data included in `midterm_test.csv` to produce a set of predictions for the test data.  These predictions should be exported as a .csv file with two columns - `ID` of the observation and the `prediction` produced by your model.  Please be sure to include a code block that demonstrates your method for doing this in your final submission.  Your predictions should be saved as "Midterm1Q1_Predictions.csv" and submitted along with your final solutions.

```{r}
#question - how to solve the collinearity problem??? do not use gam or subset selection
#question - does GAM model have collinearity problem??? yess
#question - how to find the optimal GAM model??? nooo
```

```{r}
#sample training dataset
set.seed(123)
sam_train <- as.numeric(sample(x=nrow(train_df), size=30000, replace=FALSE))
sam_test <- setdiff(seq(1,35000,1), sam_train)
strain_df_p2 <- train_df[sam_train,]
stest_df_p2 <- train_df[sam_test,]

#transform variable shares
strain_df_p2$shares <- log(strain_df_p2$shares)
stest_df_p2$shares <- log(stest_df_p2$shares)

#drop variable url
strain_df_p2 <- strain_df_p2[,-1]
stest_df_p2 <- stest_df_p2[,-1]
```

```{r}
#According to problem 1, we take for granted that 8 is a good choice for the number of variables considered for each bootstrap tree in the random forest model in terms of OOB error and MSE on the heldout data set.
p2_rf_m <- ranger(data=strain_df_p2, formula = shares~., mtry=8, num.trees=1000)
#OOB error
p2_rf_m$prediction.error
#out of sample mse
mse(predict(p2_rf_m, data=stest_df_p2)$predictions, stest_df_p2$shares)
```

```{r}
#According to problem 1 part 2, we can choose a subset of important variables to fit a GAM model.
p1p2_df %>% arrange(desc(var_imp))
p2_gam <- gam(data=strain_df_p2, family=gaussian(), 
              shares ~ s(kw_avg_avg)+s(kw_max_avg)+s(self_reference_avg_sharess))
#in sample mse
mse(predict.gam(p2_gam, strain_df_p2), strain_df_p2$shares)
#out of sample mse
mse(predict.gam(p2_gam, stest_df_p2), stest_df_p2$shares)
```

```{r}
p2_x <- data.matrix(strain_df_p2[,-60])
p2_y <- data.matrix(strain_df_p2[,60])

p2_cv_lassom <- cv.glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=1, nfolds=10,
                          type.measure="mse")
plot(p2_cv_lassom)
print(p2_cv_lassom)
p2_optimal_lassom <- glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=1,
                            lambda=p2_cv_lassom$lambda.min)
#in sample mse
mse(p2_y, predict(p2_optimal_lassom, newx=p2_x))
#out of sample mse
mse(stest_df_p2$shares, predict(p2_optimal_lassom, newx=data.matrix(stest_df_p2[,-60])))
```

```{r}
p2_cv_ridgem <- cv.glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=0, nfolds=10,
                          type.measure="mse")
plot(p2_cv_ridgem)
print(p2_cv_ridgem)
p2_optimal_ridgem <- glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=0,
                            lambda=p2_cv_ridgem$lambda.min)
#in sample mse
mse(p2_y, predict(p2_optimal_ridgem, newx=p2_x))
#out of sample mse
mse(stest_df_p2$shares, predict(p2_optimal_ridgem, newx=data.matrix(stest_df_p2[,-60])))
```

```{r}
knn <- seq(5,1000,100)
knn_mse_out <- c()

for (i in knn) {
  knn_m <- knn.reg(train=p2_x, test=stest_df_p2[,-60], y=p2_y, k=i)
  knn_mse_out <- c(knn_mse_out,mse(knn_m$pred, stest_df_p2$shares))
}

p2_knn_df <- data.frame(knn,knn_mse_out)
ggplot(data=p2_knn_df) +
  geom_point(aes(x=knn, y=knn_mse_out)) +
  geom_line(aes(x=knn, y=knn_mse_out)) +
  theme_bw()
```

```{r}
#conclusion
p2_mse_in_sample <- c(mse(predict.gam(p2_gam, strain_df_p2), strain_df_p2$shares),
                      mse(p2_y, predict(p2_optimal_lassom, newx=p2_x)),
                      mse(p2_y, predict(p2_optimal_ridgem, newx=p2_x)))
p2_mse_out_sample <- c(mse(predict.gam(p2_gam, stest_df_p2), stest_df_p2$shares),
                       mse(stest_df_p2$shares, predict(p2_optimal_lassom, 
                                                       newx=data.matrix(stest_df_p2[,-60]))),
                       mse(stest_df_p2$shares, predict(p2_optimal_ridgem, 
                                                       newx=data.matrix(stest_df_p2[,-60]))))
p2_m <- c('GAM','RIDGE','LASSO')
data.frame(model = p2_m,
           mse_in_sample = p2_mse_in_sample,
           mse_out_sample = p2_mse_out_sample)
```

In addition, the lowest out of sample mse in the knn model is 0.79 when k equals around 100. But the random forest model clearly outperforms all four, with an OOB error of 0.7239764 and out of sample mse of 0.6875225. Hence, the random forest model with the number of variables considered for each bootstrap tree of 8 is our optimal model.

  
## Question 3 - Interpreting Your Big Predictive Model (7 pts.)

Using your predictive model from question 1, explain how the predictions change as a  function of your predictors.  Specifically, try to figure out an interpretable set of conditions under which we would expect that an article is shared a lot and when we would expect it to be shared infrequently.  This is a high dimensional predictor space, so you won't be able to do this exactly, but do your best to come up with some general observations that dictate the predicted shares for an article.

*Hint*: I'm guessing that the method you choose in Q1 will not yield a particularly predictable prediction function.  Just do your best to try to make some general conclusions.  The goal of this question is to get you thinking about the difference between a good predictive model and an interpretable one!

```{r}
p3_m <- ranger(data=strain_df_p2, formula = shares~., mtry=8, num.trees=1000,
               importance='permutation')

p3_df <- data.frame(p3_m$variable.importance)
p3_df$var <- rownames(p3_df)
p3_df <- p3_df %>% rename(var_imp = p3_m.variable.importance)

ggplot(data=p3_df, aes(x=var, y=var_imp)) +
  geom_bar(stat='identity', width=0.75) + coord_flip() +
  labs(x="Features", y="Variable Importance") +
  theme_bw()

p3_df %>% arrange(desc(var_imp))
```

According to the variable importance plot from above based on our optimal random forest model from problem 2 trained on a dataset of 30000 observations with the mtry value of 8, some of the most important predictors that dictate the predicted shares of an article are kw_avg_avg, kw_max_avg, self_reference_avg_shares, kw_min_avg, LDA_02, and timedelta. Articles that are high in those features are likely to be shared more often than those that are not. However, since the random forest model is a good predictive model but not known as a good interpretable model, the important variable plot above may not be very accurate.


## Question 4 - Building a Predictive Model with a Subset of Predictors (25 pts.)
## Question 4 - Building a Predictive Model with a Subset of Predictors (25 pts.)
### Demonstration of our method for question 4 optimal model
```{r}
# There are mainly two parts in the work for question4: variable selection and model selection. In variable selection, we will use LASSO and random forest to have some potential optimal-subset of variables. Then, we will use splines plot from the GAM function to have a general pictures of the relations between each variable and the log(share).Once we narrow down on 1/2 potential subset of variable, we will move on to the model selection part. We some analysis on what we got in the variable selection and previous work, we decide to apply non-linear model KNN and random forest to the subset of variables. With mse from each potential model, we will finalize the best model and make prediction on test data.
```

### Data ini & recode

```{r}
# take random sample data set for training & testing
set.seed(123)
train_df_q4 <- train_df[sample(x=nrow(train_df), size=7500, replace=FALSE),]
test_df_q4 <- train_df[sample(x=nrow(train_df), size=2500, replace=FALSE),]
# log(share) transformation
train_df_q4$shares <- log(train_df_q4$shares)
test_df_q4$shares <- log(test_df_q4$shares)
#drop variable url
train_df_q4 <- train_df_q4[,-1]
test_df_q4 <- test_df_q4[,-1]
# subset for later
test_df_q4_predictor <- data.matrix(train_df_q4[, -60])
test_df_q4_outcome <- data.matrix(train_df_q4$shares)
```

### Variable selection(at most 5 variables)

We will work on variables selections on LASSO and random forest, and using GAM to check the relationship between each variables and log(share).

#### LASSO variable selection

```{r}
# find optimal lambda
c_lasso_cv <- cv.glmnet(x = test_df_q4_predictor, y = test_df_q4_outcome,
    family = "gaussian", alpha = 1, nfolds = 10, type.measure = "mse")
optimalLambda <- c_lasso_cv$lambda.min # the optimal lambda that minimize MSE
lasso_optimalModel <- glmnet(x = test_df_q4_predictor, y = test_df_q4_outcome,
    family = "gaussian", alpha = 1, lambda = optimalLambda)
opt_variableList <- lasso_optimalModel$beta
opt_variableList
```

5 variables LASSO model suggests: global_rate_negative_words/ global_subjectivity/ n_unique_tokens/ LDA_00/ min_positive_polarity

#### Random Forest variables selection

```{r}
# test and see how many variables we should include
oob <- c()
rf_mse <- c()
for (i in 2:5) {
  model <- ranger(data=train_df_q4, formula = shares~., mtry=i, num.trees=1000)
  oob <- c(oob,model$prediction.error)
  testPred <- predict(model, data=test_df_q4)
  rf_mse <- c(rf_mse,mse(test_df_q4$shares, testPred$predictions))
}
mtry<-c(2,3,4,5)
RFepe <- data.frame(mtry,oob,rf_mse)
# mse suggests 5 variables
ggplot(data=RFepe) +
  geom_point(aes(x=mtry, y=rf_mse)) +
  geom_line(aes(x=mtry, y=rf_mse)) +
  theme_bw()
# oob suggest 4 variables
ggplot(data=RFepe) +
  geom_point(aes(x=mtry, y=oob)) +
  geom_line(aes(x=mtry, y=oob)) +
  theme_bw()
# RF model with 4 variables & 5 variables
RF_model_4 <- ranger(data=train_df_q4, formula = shares~., mtry=4, num.trees=1000,importance='permutation')
RF_model_5 <- ranger(data=train_df_q4, formula = shares~., mtry=5, num.trees=1000,importance='permutation')
# find the top-importance variables for each model
# we won't worry about the negative importance here as they're all generally small
varImportance4 <- data.frame(var_importance=RF_model_4$variable.importance)
arrange(varImportance4, desc(var_importance))
varImportance5 <- data.frame(var_importance=RF_model_5$variable.importance)
arrange(varImportance5, desc(var_importance))
```

4 variable RF model suggests: kw_avg_avg/ self_reference_avg_sharess /kw_max_avg / n_unique_tokens / (5th: kw_min_avg) 
5 variable RF model suggests: kw_avg_avg/ self_reference_avg_sharess/ kw_min_avg/ kw_max_avg / self_reference_min_shares

#### Variables selection sub-conclusion
```{r}
# check suggested variable with spline and decide the final variable subset
checkGam<-gam(shares ~ s(kw_avg_avg) + s(self_reference_avg_sharess) + s(kw_min_avg) + s(kw_max_avg)+s(self_reference_min_shares)+s(global_rate_negative_words)+s(global_subjectivity)+s(n_unique_tokens)+s(LDA_00)+s(min_positive_polarity),data=train_df_q4,family = gaussian())
plot(checkGam,pages=1)
```
We might don't want to approach data analysis on this data set with linear approaches, since most splines don't really show much linear relationship with share. we will move on with knn and random forest with both 4-variable and 5-variable data set

### Model selection
#### data recode with selected subset of data
```{r}
train_df_4RFvar<-data.frame(shares=train_df_q4$shares,kw_avg_avg=train_df_q4$kw_avg_avg,self_reference_avg_sharess=train_df_q4$self_reference_avg_sharess,kw_max_avg=train_df_q4$kw_max_avg,n_unique_tokens=train_df_q4$n_unique_tokens)
test_df_4RFvar<-data.frame(shares=test_df_q4$shares,kw_avg_avg=test_df_q4$kw_avg_avg,self_reference_avg_sharess=test_df_q4$self_reference_avg_sharess,kw_max_avg=test_df_q4$kw_max_avg,n_unique_tokens=test_df_q4$n_unique_tokens)
train_df_5RFvar<-data.frame(shares=train_df_q4$shares,kw_avg_avg=train_df_q4$kw_avg_avg,self_reference_avg_sharess=train_df_q4$self_reference_avg_sharess,kw_min_avg=train_df_q4$kw_min_avg,kw_max_avg=train_df_q4$kw_max_avg,self_reference_min_shares=train_df_q4$self_reference_min_shares)
test_df_5RFvar<-data.frame(shares=test_df_q4$shares,kw_avg_avg=test_df_q4$kw_avg_avg,self_reference_avg_sharess=test_df_q4$self_reference_avg_sharess,kw_min_avg=test_df_q4$kw_min_avg,kw_max_avg=test_df_q4$kw_max_avg,self_reference_min_shares=test_df_q4$self_reference_min_shares)
# list for oob and mse
mse<-c()
oob<-c()
modelName<-c()
# predictor and outcome df based on train_df_4RFvar and train_df_5RFvar
# train set
train_df_4RFvar_predictor <- data.matrix(train_df_4RFvar[, -1])
train_df_4RFvar_outcome <- data.matrix(train_df_4RFvar$shares)
train_df_5RFvar_predictor <- data.matrix(train_df_5RFvar[, -1])
train_df_5RFvar_outcome <- data.matrix(train_df_5RFvar$shares)
# test set
test_df_4RFvar_predictor <- data.matrix(test_df_4RFvar[, -1])
test_df_4RFvar_outcome <- data.matrix(test_df_4RFvar$shares)
test_df_5RFvar_predictor <- data.matrix(test_df_5RFvar[, -1])
test_df_5RFvar_outcome <- data.matrix(test_df_5RFvar$shares)
```

#### Random Forest model
```{r}
# random forest
RFmodel_selected_4var<-ranger(data=train_df_4RFvar, formula = shares~., mtry=4, num.trees=1000)
RFmodel_selected_5var<-ranger(data=train_df_5RFvar, formula = shares~., mtry=5, num.trees=1000)
# OOB
OOB_RF4var<-RFmodel_selected_4var$prediction.error
OOB_RF5var<-RFmodel_selected_5var$prediction.error
# out of sample MSE
MSE_RF4var<-mse(predict(RFmodel_selected_4var, data=test_df_4RFvar)$predictions, test_df_4RFvar$shares)
MSE_RF5var<-mse(predict(RFmodel_selected_5var, data=test_df_5RFvar)$predictions, test_df_5RFvar$shares)
oob<-c(oob,OOB_RF4var,OOB_RF5var)
mse<-c(mse,MSE_RF4var,MSE_RF5var)
modelName<-c(modelName,"RF4var","RF5var")
```

#### KNN model
```{r}
# find the optimal k for knn
knn_mTest<-function(train_pred,train_outcome,test_pred,test_outcome,k){
  knn <- k
  knn_mse_outOfSample <- c()
  for (i in knn) {
    knn_m <- knn.reg(train=train_pred, test=test_pred, y=train_outcome, k=i)
    knn_mse_outOfSample <- c(knn_mse_outOfSample,mse(knn_m$pred, test_outcome))
  }
  p2_knn_df <- data.frame(knn,knn_mse_outOfSample)
  ggplot(data=p2_knn_df) +
    geom_point(aes(x=knn, y=knn_mse_outOfSample)) +
    geom_line(aes(x=knn, y=knn_mse_outOfSample)) +
    theme_bw()
}
# kList<-seq(5,1000,100)# optimal k=105
kList<-seq(5,110,1)# k=33 for 5 var model; k=92 for 4 var model
knn_mTest(train_df_4RFvar_predictor,train_df_4RFvar_outcome,test_df_4RFvar_predictor,test_df_4RFvar_outcome,kList)
knn_mTest(train_df_5RFvar_predictor,train_df_5RFvar_outcome,test_df_5RFvar_predictor,test_df_5RFvar_outcome,kList)
# KNN model for 4Var and 5Var
knn_4Var<- knn.reg(train=train_df_4RFvar_predictor, test=test_df_4RFvar_predictor, y=train_df_4RFvar_outcome, k=92)
knn_4Var_mseOut <- mse(knn_4Var$pred, test_df_4RFvar_outcome)
knn_5Var<- knn.reg(train=train_df_5RFvar_predictor, test=test_df_5RFvar_predictor, y=train_df_5RFvar_outcome, k=33)
knn_5Var_mseOut <- mse(knn_5Var$pred, test_df_5RFvar_outcome)
mse<-c(mse,knn_4Var_mseOut,knn_5Var_mseOut)
modelName<-c(modelName,"knn4Var","knn5Var")
```


#### Model selection sub-conclusion
We randomly take three seeds(77/123/95) and compare the mse for the potential-optimal models we have so far. It turns out that Random Forest models genarally perform better than the KNN ones. We finalized on the random forest model with 4 selected variable performs the best across all seeds(though there is no significant difference between RF4variables and RF5variables).
```{r}
msePrint<-data.frame(modelName,mse)
seed77MSE<-msePrint
#seed123MSE<-msePrint
#seed95MSE<-msePrint
```

### Prediction on midterm1_testNoShares.csv
With our work earlier on, we now have our optimal model(RFmodel_selected_4var): Random Forest model on kw_avg_avg/ self_reference_avg_sharess /kw_max_avg / n_unique_tokens
```{r}
bestModel<-RFmodel_selected_4var
test_df_predictor<-data.frame(kw_avg_avg=test_df$kw_avg_avg,self_reference_avg_sharess=test_df$self_reference_avg_sharess,kw_max_avg=test_df$kw_max_avg,n_unique_tokens=test_df$n_unique_tokens)
bestPredShares<-predict(bestModel, data=test_df_predictor)$predictions
outPutFile<-data.frame(ID=test_df$ID,shares_Pred=bestPredShares)
# save the .csv
write.csv(outPutFile,"Midterm1Q3_Predictions.csv", row.names = FALSE)
```

## Question 5 - Interpreting the Subset Model (8 pts.)
```{r}
# plot the variable importance
bestM<-ranger(data=train_df_4RFvar, formula = shares~., mtry=4, num.trees=1000,importance='permutation')
p5_df <- data.frame(bestM$variable.importance)
p5_df$var <- rownames(p5_df)
p5_df <- p5_df %>% rename(var_imp = bestM.variable.importance)
ggplot(data=p5_df, aes(x=var, y=var_imp)) +
  geom_bar(stat='identity', width=0.75) + coord_flip() +
  labs(x="Features", y="Variable Importance") +
  theme_bw()
p5_df %>% arrange(desc(var_imp))
# variables: 
# kw_avg_avg/ self_reference_avg_sharess /kw_max_avg / n_unique_tokens
# bestModel
```

From question 4, we choose random forest model for good prediction performance, while we sacrifice the interpretability of the model. With our best model on our selected variables, we could only tell the importance of each selected variables shown above. We could see that the Avg. keyword (max. shares)[kw_max_avg] and Avg. keyword (avg. shares)[kw_avg_avg] takes a huge part of the variable importance, while the model also suugests some importance on Avg. shares of referenced articles in Mashable [self_reference_avg_sharess] and Rate of unique words in the content [n_unique_tokens]. The result is quite similar with what we got in the full model: all of our four varibels in Q4 are also in the top 8 importance of the full model; 3 out of 4 are the most importance among all variables in full model. 

Since random forest doesn't return any coefficient as the linear models do, we don't really know for sure whether these variables are positively OR negatively correlate to the outcome[shares]. With our best educational guess, we might take a conjecture that highest number of shares articles will be those which: 1) keywords with good amount of shares(both top number of shares and average number of shares) in the past; 2) reference on high-shares articles; 3) using good amount of high rate unique words. The lowest number of shares articles will be those don't do well on our conjecture.

Actionable rules: 
1) have a good idea about popular "keywords" on Mashable and use those keywords as frequent as possible.If not sure what are those "keywords", use diverse vocabulary so that you will hit some by chance.
2) include reference to the popular articles on Mashable. If not sure about what's the *most popular ones, maybe refer to as many articles as possible such that you will somehow hit one.
3) use unique vocabulary with huge diversity.

why we not expect the rule set makes sense:
There are many confounding variables that we are unable to include in our model. For example, the recommend-article-to-reader algorithm might have bias on some types of articles and thus recommend those articles more frequently. In this way, those articles with high exposure to the public will naturally have larger audiences group and higher amount of shares.

## Question 6 - Building a Predictive Model with a Single Predictor (15 pts.)

Now, take the idea of creating a small actionable predictive model to the extreme and find the single predictor model that minimizes expected prediction error for log of shares on the unseen test data set.  Discuss your process and how you decided on a final model.  Create a plot that shows your chosen variable against the log number of shares for the training data.  Create a similar plot for a heldout test data set.  Then, overlay a curve that shows the predicted value of log shares as a function of your feature on both plots.

The difference between this question and the previous two questions is that there are a finite number of models to test!  After choosing a method that is most appropriate, just iterate over all possible features and compare two measures of EPE - an in-sample EPE method and actual performance on an out-of-sample data set.

In your final deliverable, explain your process of choosing a model that is appropriate for modeling univariate relationships and demonstrate the process that you used to choose which predictor was the best.  Be sure to engage with different EPE measures.

Once you decide on a final model, use the data included in `midterm_test.csv` to produce a set of predictions for the test data.  These predictions should be exported as a .csv file with two columns - `ID` of the observation and the `prediction` produced by your model.  Please be sure to include a code block that demonstrates your method for doing this in your final submission.  Your predictions should be saved as "Midterm1Q5_Predictions.csv" and submitted along with your final solutions.

Initial sampling and manipulation
```{r}
#sample training dataset
set.seed(456)
sam6_train <- as.numeric(sample(x=nrow(train_df), size=30000, replace=FALSE))
sam6_test <- setdiff(seq(1,35000,1), sam6_train)
strain_df_p6 <- train_df[sam6_train,]
stest_df_p6 <- train_df[sam6_test,]
rm(train_df)

#transform variable shares
strain_df_p6$shares <- log(strain_df_p6$shares)
stest_df_p6$shares <- log(stest_df_p6$shares)

#drop variable url
strain_df_p6 <- strain_df_p6[,-1]
stest_df_p6 <- stest_df_p6[,-1]
```

We start by getting some sense on what might be a good choice of the uni-variable. Since OLS is dominated by ridge and lasso, we will not use OLS. Since GAM with one "s(variable)" term is the same with spline model, we will only use the gam function. We will use a for-loop to iterate all 59 variables on the rest of possible models we learned--KNN, ridge, lasso, polynomial expansion, spline, LOESS, RF with mtry = 1, and compute and compare in-sample and out-of-sample EPE to pick out best model and best variable.

KNN
```{r}
#out-of-sample
knn_p6_mse_out <- c()
#k = 1 since only 1 variable
#test for all 59 variables
for (j in 1:59) {
  knn_p6_m <- knn.reg(train=as.data.frame(strain_df_p6[,j]), test=as.data.frame(stest_df_p6[,j]), y=as.data.frame(strain_df_p6[,60]), k=1)
  knn_p6_mse_out <- c(knn_p6_mse_out,mse(knn_p6_m$pred, stest_df_p6$shares))
}
p6_knn_out_df <- data.frame(var = colnames(strain_df_p6[,-60]), knn_p6_mse_out)
p6_knn_out_df %>% arrange(knn_p6_mse_out) #weekday_is_wednesday; data_channel_is_world is 13th

#in-sample using KCV
knn_reg_kfold <- function(x, y, k, folds, seed){
  set.seed(seed)
  #sample
  sam6_KNN_kfold_train <- as.numeric(sample(nrow(as.data.frame(x)), size=ceiling(nrow(as.data.frame(x))/folds), replace=FALSE))
  sam6_KNN_kfold_test <- setdiff(seq(1,30000,1), sam6_KNN_kfold_train)
  #model for prediction error
  p6_p <- c()
  for (i in 1:folds){
    x_train <- x[sam6_KNN_kfold_train]
    x_test <- x[sam6_KNN_kfold_test]
    y_train <- y[sam6_KNN_kfold_train]
    y_test <- y[sam6_KNN_kfold_test]
    p6_KNN_kfoldm <- knn.reg(train = as.data.frame(x_train), test = as.data.frame(x_test), y = as.data.frame(y_train), k = k)
    p6_p[i] <- mse(p6_KNN_kfoldm$pred, as.numeric(unlist(y_test)))
  }
  return(p6_p[1]/folds)
}
p6_kfold_5 <- c()
p6_kfold_10 <- c()
for (j in 1:59) {
  p6_kfold_5[j] <- knn_reg_kfold(strain_df_p6[,j], strain_df_p6[,60], 1, 5, 456)
  p6_kfold_10[j] <- knn_reg_kfold(strain_df_p6[,j], strain_df_p6[,60], 1, 10, 456)
}
p6_knn_in_df <- data.frame(var = colnames(strain_df_p6[,-60]), p6_kfold_5, p6_kfold_10)
p6_knn_in_df %>% arrange(p6_kfold_5) #data_channel_is_world, 5fold cv; weekday_is_wednesday is 3rd
p6_knn_in_df %>% arrange(p6_kfold_10) #data_channel_is_world, 10fold cv; weekday_is_wednesday is 6th
```
For KNN, we found that "weekday_is_wednesday" is a good uni-predictor for out-of-sample EPE, and "data_channel_is_world" is a good uni-predictor for in-sample EPE both for 5 and 10 fold CV. Since "weekday_is_wednesday" is also a relatively good predictor for in-sample error, we will go with this predictor for KNN model. 
```{r}
colnames(strain_df_p6) #weekday_is_wednesday is when 33rd variable
p6_result <- data.frame(model = "KNN", var = p6_knn_out_df[33,1], mse_out = p6_knn_out_df[33,2], mse_in = p6_knn_in_df[33,2])
p6_result
```
We recorded the EPE of KNN model using weekday_is_wednesday above. We can see that the in-sample EPE is surprisingly good, but since its oos EPE doesn't look ideal, we will wait and see other models. For easier comparison, we only recorded 5fold CV to be out in sample mse, since it's only slightly higher than 10fold.

Ridge
```{r}
p6_x <- data.matrix(strain_df_p6[,-60]) #pool of 59 potential variables
p6_y <- data.matrix(strain_df_p6[,60])

#scientific lambda
p6_cv_ridge <- cv.glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=1, nfolds=10,
                          type.measure="mse")
p6_ridge <- glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=0, nfolds=10,
                          lambda=p6_cv_ridge$lambda.min)
p6_cv_ridge$lambda.min
#graph, ideally one significant variable left when lambda is large
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
  cf1 <- append(cf1, rownames(p6_ridge$beta)[i])
  cf2 <- append(cf2, unname(p6_ridge$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
  geom_bar(stat="identity", width=0.75) + coord_flip() +
  labs(x="Features", y="Coefficient Estimate") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="red", size = 12),
        axis.title.y = element_text(face="bold", colour="red", size = 12))

#big lambda
p6_ridge <- glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=0, nfolds=10,
                          lambda=1)
#graph, ideally one significant variable left when lambda is large
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
  cf1 <- append(cf1, rownames(p6_ridge$beta)[i])
  cf2 <- append(cf2, unname(p6_ridge$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
  geom_bar(stat="identity", width=0.75) + coord_flip() +
  labs(x="Features", y="Coefficient Estimate") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="red", size = 12),
        axis.title.y = element_text(face="bold", colour="red", size = 12))

#very big lambda
p6_ridge <- glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=0, nfolds=10,
                          lambda=10)
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
  cf1 <- append(cf1, rownames(p6_ridge$beta)[i])
  cf2 <- append(cf2, unname(p6_ridge$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
  geom_bar(stat="identity", width=0.75) + coord_flip() +
  labs(x="Features", y="Coefficient Estimate") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="red", size = 12),
        axis.title.y = element_text(face="bold", colour="red", size = 12))

#crazy big lambda
p6_ridge <- glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=0, nfolds=10,
                          lambda=100)
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
  cf1 <- append(cf1, rownames(p6_ridge$beta)[i])
  cf2 <- append(cf2, unname(p6_ridge$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
  geom_bar(stat="identity", width=0.75) + coord_flip() +
  labs(x="Features", y="Coefficient Estimate") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="red", size = 12),
        axis.title.y = element_text(face="bold", colour="red", size = 12))
```
As we can see from the graph above, while we already made lambda very large, we cannot find a single variable that is significantly more important than others. This might be due to ridge's inability to shrink coefficients exactly to zero. So we move onto LASSO next without calculating EPE of ridge.

LASSO
```{r}
#scientific lambda
p6_cv_lasso <- cv.glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=1, nfolds=10,
                          type.measure="mse")
p6_lasso <- glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=1,
                            lambda=p6_cv_lasso$lambda.min)
p6_cv_lasso$lambda.min
#graph, ideally one important variable left
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
  cf1 <- append(cf1, rownames(p6_lasso$beta)[i])
  cf2 <- append(cf2, unname(p6_lasso$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
  geom_bar(stat="identity", width=0.75) + coord_flip() +
  labs(x="Features", y="Coefficient Estimate") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="red", size = 12),
        axis.title.y = element_text(face="bold", colour="red", size = 12))

#big lambda
p6_lasso <- glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=1,
                            lambda=0.002)
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
  cf1 <- append(cf1, rownames(p6_lasso$beta)[i])
  cf2 <- append(cf2, unname(p6_lasso$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
  geom_bar(stat="identity", width=0.75) + coord_flip() +
  labs(x="Features", y="Coefficient Estimate") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="red", size = 12),
        axis.title.y = element_text(face="bold", colour="red", size = 12))

#very big lambda
p6_lasso <- glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=1,
                            lambda=0.01)
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
  cf1 <- append(cf1, rownames(p6_lasso$beta)[i])
  cf2 <- append(cf2, unname(p6_lasso$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
  geom_bar(stat="identity", width=0.75) + coord_flip() +
  labs(x="Features", y="Coefficient Estimate") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="red", size = 12),
        axis.title.y = element_text(face="bold", colour="red", size = 12))

#crazy big lambda
p6_ideal_lasso <- glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=1,
                            lambda=0.087)
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
  cf1 <- append(cf1, rownames(p6_ideal_lasso$beta)[i])
  cf2 <- append(cf2, unname(p6_ideal_lasso$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
  geom_bar(stat="identity", width=0.75) + coord_flip() +
  labs(x="Features", y="Coefficient Estimate") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="red", size = 12),
        axis.title.y = element_text(face="bold", colour="red", size = 12))
```
We see from crazy big lambda that, while we finally shrinks down to only one variable, its coefficient is very close to zero. So we don't expect this LASSO model with univariable "kw_avg_avg" is going to have good predictive capability. We move onto polynomial expansion without calculating EPE of LASSO.

Polynomial Expansion
We continue to use strain_df_p6 and stest_df_p6
```{r}
#We choose LOOCV as there is less worry about bias and variance than other approaches and it's asymptotically unbiased for the in-sample EPE.

#select variable based on in sample EPE
lm_pred_metrics <- function(fit_model) {
  LOOCV <- mean((fit_model$residuals / (1-hatvalues(fit_model)))^2)
  return (LOOCV)
}
#exclude binary variables and variables with small number of possible values since degree of polynomial cannot exceed number of possible values
index <- c(1:3, 7:12, 19:30, 39:59)
p6_poly_mse <- c()
p6_poly_deg <- c()
for (i in index){
  p6_poly_1 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],1))
  p6_poly_2 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],2))
  p6_poly_3 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],3))
  p6_poly_4 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],4))
  p6_poly_5 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],5))
  p6_poly_6 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],6))
  p6_poly_7 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],7))
  p6_poly_8 <- lm(strain_df_p6$shares~poly(strain_df_p6[,i],8))
  p6_poly_df <- cbind(lm_pred_metrics(p6_poly_1), lm_pred_metrics(p6_poly_2),
                        lm_pred_metrics(p6_poly_3), lm_pred_metrics(p6_poly_4),
                        lm_pred_metrics(p6_poly_5), lm_pred_metrics(p6_poly_6),
                        lm_pred_metrics(p6_poly_7), lm_pred_metrics(p6_poly_8))
  p6_poly_mse <- c(p6_poly_mse, min(p6_poly_df))
  p6_poly_deg <- c(p6_poly_deg, which.min(p6_poly_df))
}
p6_poly <- data.frame(mse = p6_poly_mse, degree = p6_poly_deg)
which.min(p6_poly$mse) #18th obs. in p6_poly df
min(p6_poly$mse) #lowest mse is 0.8092535, degree is 7.
index[which.min(p6_poly$mse)] #27th variable in strain_df_p6, which is kw_avg_avg
```
From above, we see that, if we were to use a polynomial model, we would use variable "kw_avg_avg" and set function degree to be 7.
```{r}
#in sample EPE is:
p6_poly_optimal <- lm(data = strain_df_p6, shares~poly(kw_avg_avg,7))
p6_poly_in <- lm_pred_metrics(p6_poly_optimal)
p6_poly_in
#out of sample EPE is:
p6_poly_pred <- p6_poly_optimal %>% predict(stest_df_p6)
p6_poly_out <- mse(p6_poly_pred, stest_df_p6$shares)
p6_poly_out
#compile into final result
p6_poly_result <- data.frame(model = "Polynomial_deg_7", var = "kw_avg_avg", mse_out = p6_poly_out, mse_in = p6_poly_in)
p6_result <- rbind(p6_result, p6_poly_result)
p6_result
```

Spline
We continue to use strain_df_p6 and stest_df_p6
```{r}
strain_df_p6 <- as.data.frame(strain_df_p6)
stest_df_p6 <- as.data.frame(stest_df_p6)
#in sample
#non-binary variables
index_nb_gam <- c(1:4, 6:12, 19:30, 39:59)
nonbinary_s <- c()
for (i in index_nb_gam){
  p6_nb_spline_m <- gam(shares ~ s(strain_df_p6[,i]),data=strain_df_p6,family = gaussian())
  nonbinary_s<-c(nonbinary_s, p6_nb_spline_m$gcv.ubre.dev)
}
min(nonbinary_s) #0.805739
index_nb_gam[which.min(nonbinary_s)] #27th variable is kw_avg_avg (seems to be better candidate)
#binary variables
index_b_gam <- c(5, 13:18, 31:38)
binary_s <- c()
for (i in index_b_gam){
  p6_b_spline_m <- gam(shares ~ strain_df_p6[,i],data=strain_df_p6,family = gaussian())
  binary_s<-c(binary_s, p6_b_spline_m$gcv.ubre.dev)
}
min(binary_s) #0.8459762
#since non-binary performs better based on in sample EPE, we use kw_avg_avg for spline
p6_spline_optimal <- gam(shares ~ s(kw_avg_avg),data=strain_df_p6,family = gaussian())
p6_spline_in <- p6_spline_optimal$gcv.ubre.dev
p6_spline_in
#out of sample
p6_spline_out<-mse(stest_df_p6$shares,predict.gam(p6_spline_optimal,stest_df_p6))
p6_spline_out
#compile into final result
p6_spline_result <- data.frame(model = "Spline", var = "kw_avg_avg", mse_out = p6_spline_out, mse_in = p6_spline_in)
p6_result <- rbind(p6_result, p6_spline_result)
p6_result
```

Now we proceed to LOESS

## Question 7 - Do your models perform well on out-of-sample data? (10 pts.)

For each model, I'll compute a root mean squared prediction error using your submitted predictions and the true log number of shares for the articles in the hidden test set.  Across all models turned in, points will be allocated as follows:

  1. For Q1 and Q3, let $\text{RMSE}_{\text{Min}}$ be the minimum prediction error over the submitted models.  Let $\text{RMSE}_i$ be your predictive error.  Your prediction points will be computed as:
  
  $$4 \times \frac{\text{RMSE}_{\text{Min}}}{\text{RMSE}_{i}}$$
  
  2. For Q5 it will be:
  
  $$2 \times \frac{\text{RMSE}_{\text{Min}}}{\text{RMSE}_{i}}$$

The goal here is to add a low-stakes incentive and fun "competition" to try to optimize your predictive models.  **I expect that the performance across groups/students will be quite close, so this won't greatly affect your final grade.**

