
---
title: "Midterm Data Exercise #1 - Continuous Outcomes"
author: "Kevin McAlister"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    df_print: kable
    theme: leonids
    highlight: github
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
---

```{r, include=FALSE}
library(ggplot2)
library(data.table)
library(ranger)
library(Metrics)
library(tidyverse)
library(glmnet)
library(FNN)
library(mgcv)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

```{r}
#setwd('/Users/billyge/Desktop/Emory Fall 2022/QTM 385-4/problem sets/ps github/midterm1')
setwd('/Users/annie/Desktop/QTM385/QTM350_Github/QTM350 Github/QTM385/midterm1')
train_df <- read.csv('midterm1_train.csv')
test_df <- read.csv('midterm1_testNoShares.csv')
```

Collaborators: Billy Ge, Latifa Tan, Annie Luo

This is the first midterm data analysis exercise for QTM 385 - Introduction to Statistical Learning.  This exercise will use a single data set to ask a few questions related to creating predictive models for a continuous outcome.  This data set is larger and messier than those that have been given for the problem set exercises.  It is intended to provide you with a "real-world" scenario you might see if attempting to build a predictive model for a question you care about.  This assignment is worth 12.5% of your final grade and your final solutions should be submitted by 11:59 PM on October 17th.  Your final submission should include a .Rmd/.md/.ipynb file, a rendered .pdf/.html version of your notebook, and three .csv files including your predictions for an unseen test set.

Unlike the problem sets, there will be little guidance as to what methods to use and how to interpret results.  The idea is that you can treat this like a real analysis exercise - applying different models, seeing what works and doesn't work, presenting the best possible model but being transparent about the downsides of you choice.  For each of the three tasks, you should consider a variety of potential methods and choose a single one as your "best" model.

A review of fitting methods that we've covered in this course (thus far):

  1. Linear regression via OLS
  2. Polynomial regression
  3. K-nearest neighbors regression
  4. Shrinkage Methods (Ridge and LASSO)
  5. Splines for univariate relationships
  6. Local linear regression (LOESS)
  7. Generalized Additive Models (GAMs)
  8. Bagged Regression Trees and Random Forests 
  9. (For the adventurous) Boosted Regression Trees
  
A review of concepts that should be central in your answers:

  1. Training Error vs. Test Error
  2. Train/Validation/Test Splits
  3. Expected Prediction Error
  4. Cross-validation
  5. Predictor Selection
  6. Linear vs. Nonlinear Relationships
  7. Non-additive Errors
  8. Bagging
  9. (For the adventurous) Boosting
  
***

## Data Set Overview

This assignment revolves around a data set of 39,644 articles published by the website [mashable.com](https://mashable.com/) in 2013 and 2014.  Mashable is an international multi-platform media and entertainment company that can be seen as a news aggregator; the site publishes its own articles about current events and topics.  In many ways, Mashable was (and still is, but it's under different ownership now) a content aggregator.  The goal of Mashable was to provide an aggregate of important daily "news" and to generate ad-revenue via advertising clicks.  Therefore, the profitability of the website was directly tied to its ability to attract users to the site.  The most common way users were attracted was by shared articles passing through various social media outlets.  More shares leads to more ad revenue!

The data set includes article urls, the number of shares on social media via the site's provided sharing mechanisms, and a collection of 59 covariates related to the length and content of the articles.  It is important to note that all 59 of these covariates could potentially be measured *prior to publication*.  The reason for this can be seen in the original paper that collected and used this data set - ["A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News" by Kelwin Fernandes, Pedro Vinagre, and Paulo Cortez](https://repositorium.sdum.uminho.pt/bitstream/1822/39169/1/main.pdf).  In this paper, the authors want to demonstrate that predictive models can be used to create actionable suggestions to increase the number of shares that an article receives.  In doing so, they first build a predictive model that is expected to perform well on unseen data (articles that have not yet been written) and use this to create a rules set that provides expected improvements for new articles.  The proposed IDSS is operationalized over keywords - summary words that are used to categorize an article.  However, it is easy to see how a system of this sort could be used to create content suggestions for the writers, themselves, to write the "clickbaitiest" articles possible.

The idea of interpretable and actionable rules sets is central to the study of fairness and bias in machine learning and actionable prediction models.  Though most of the models we've discussed in class are of the interpretable variety (KNN regression being the exception) - we can viably see how a predictor increases or decreases the resulting prediction - the resulting prediction surfaces can be very complex.  These methods allow "transparent" decision making processes and ensure that we're not making decision based on fragile or unethical logical links (redlining for mortgages, for example).  While this data set does not present any avenues for ethically dubious prediction methods (I don't think, at least), you may find associations that are clearly non-causal.  You'll discuss these as part of one of the problems.

Many of the covariates presented as processed natural language processing (NLP) metrics that are intended to capture the topic, positive/negative sentiment (polarity), and subjectivity of the article.  I'll briefly summarize these concepts here:

  1. Topics are presented using LDA dimensions that sum to 1.  Latent Dirichlet allocation is a common method of finding common "themes" and word associations across a large corpus of text documents.  The authors ran an LDA analysis and settled on 5 "topics" for the entire data set.  Each document is assumed to be a mixture of these 5 topics and can be described by a weight that corresponds to how much each document contains each topic.  A document with weight 1 on a single topic means that it is wholly decribed by that one topic while weight zero means that there is no part of the document that speaks to the topic.  Since this is a mixture, the weights sum to one.  There is no guarantee that the LDA topics are coherent, but you can find similarities between articles that are close to each topic.  Should you use one of the LDA dimensions in your models (especially your small models), you may want to undertake the task of figuring out what each of them means.  **A warning: since the LDA weights for each article sum to 1, placing all 5 into a model that doesn't select away collinear features will run into issues!**
  
  2. Polarity is a measure of the positiveness or negativeness of a document.  This is typically computed using a dictionary of positive and negative words and then using associational methods for determining whether words that are not recognized are positive or negative.  For this data set, negative polarity means that the document is largely negative while positive polarity means that the document is largely positive.  Related measures look for the rate of positive and negative words, title polarity, and conditional polarity for only positive or negative words.
  
  3. Subjectivity is a measure of how "fact-based" an article is.  Values of 0 mean that the document is fully fact-based while values of 1 mean that the document is wholly opinion based.  The machinery of the subjectivity measure largely mimics that of the polarity measure starting with a dictionary of fact vs. opinion based words and using associations to cover new words.
  
The rest of the covariates relate to specific features of the articles - keyword usage, data channels, etc.  A short description of each covariate can be seen in the `OnlineNewsPopularity.names` file included with this assignment.  Further descriptions of the data set can be found in the original paper.  The paper is relatively short, so I highly recommend reading it to get a sense of what the authors are doing and how the features are used to produce this automated decision making system.
  
A final thing to watch out for are two sets of unordered categorical predictors - day of publication and data channel.  All of these categorical predictors are already "dummied out", so you don't need to do anything special to make them usable.  However, be cautious if building a linear model!  The day of the week dummies are also accompanied by `is_weekend` which is just a linear combination of the other predictors.  The data channel predictors are binary variables that tell which large topic each article falls under - one of lifestyle, entertainment, business, social media, technology, world news/issues, and viral content.  Viral content was dropped from the predictor set.  As best I can tell, any article that has a zero on all other data channels falls into the viral content category.  You can include this variable in your models if you'd like by adding a column of appropriate indicators.

***

## Overall Goal

For this assignment, you'll take on the role of a data scientist attempting to understand and **predict** what articles are most likely to to garner the most shares.  You will have access to a training set of 35,000 observations each corresponding to a specific article.  Each observation has 1) the article url, 2) the number of shares, and 3) 59 covariates related to the length and content of the article.  Your goal is to build predictive models that will predict the number of shares for an article on data that was not used to train the model.

Your grade on this assignment will be a combination of:

  1. Explaining your model building process
  2. Assessing the out-of-sample predictive abilities for your chosen models
  3. Interpreting your predictive models (plots and similar exercises)
  4. Generating predictions using a test set of features **where you will not know the true outcome**
  
You do not need to try all prediction methods for each question.  Rather, I would like you to justify why you aren't considering certain methods.  For example, you can justify not using OLS by saying that we know that Ridge is guaranteed to produce a lower mean squared prediction error than OLS in almost any situation.  This is sufficient reasoning!

We've discussed multiple ways of approximating the unseeable expected prediction error quantity.  The first set of approaches leverage the training data **only** to compute an estimate of the EPE - these methods include cross-validation and out-of-bag approaches.  The second method leverages an actual held out test data set that can be used to truly assess the EPE - the data is large enough to hold out a subset!  Given the size of this data set, I highly recommend that you use a combination of the two to make any model choice decisions.

Most of the methods we've discussed in class will move quite quickly on data of this size.  KNN regression, bagged regression trees, random forests, and boosted trees will take a little longer.  If you run into problems with the scale of the data, feel free to take a random subset of the training data.  You won't be penalized for this reduction in data size, but your models are unlikely to score as well as the large $N$ models in terms of actual out-of-sample predictive ability.  You may also reduce the size of the data set to **tune** any hyperparameters for any specific model.  This may reduce the computational time needed to complete the assignment.  All said, I'm looking for students to attempt to produce good predictive models for the outcome but I am well aware of the computational limitations that you all will experience.  Just try your best!

I have a hidden test set of 4,644 articles that were randomly selected from the full data set that will be used to assess the predictive accuracy of your chosen approaches.  You will provide three .csv files that include your predictions for the unseen test data set using the three models you choose in this exercise.  The test data set, included as `midterm1_testNoShares`, has all of the features of the original data set along with an additional `ID` column.  Your .csv files should only include two columns: the `ID` of the observation and the `prediction` made using your chosen method.      

***

## Question 1 - Implementing a Random Forest Model (10 pts.)

Let's walk through the process of implementing and interpreting a random forest model in this first question.

### Part 1

Begin by taking a random sample of 7,500 training observations and use this as your training set.  Similarly, take a random sample of 2,500 training observations and use this as a heldout test set.  

Using your reduced training set, implement a series of random forest using all predictors that creates predictions for the **log number of shares**.  For each random forest, you should generate 1,000 bootstrap trees.  To try to "tune" the random forest, vary the number of variables to consider in each bootstrap tree (`mtry` in `ranger`).  At a minimum, consider `mtry = c(2,4,6,8,10,20,30,40,50,59)`.

*Note*: The bagged tree with all 59 predictors may take a few minutes to run.

For each value of `mtry`, record the OOB error estimate and compute the MSE for your held out test data set.  Create a plot that shows these two curves as a function of the number of variables considered in each tree.

Which number of features considered for each bootstrap tree yields the lowest OOB error?  What about the lowest error on your heldout test set?

What does this say about the bias/variance tradeoff for this specific data set?  Does out-of-sample predictive accuracy favor higher variance among trees or lower bias in the estimates?

```{r}
#question - is it okay to further downsize the training set??? yess
#question - drop url??? yess
```

```{r}
#sample training dataset
set.seed(123)
strain_df_p1 <- train_df[sample(x=nrow(train_df), size=7500, replace=FALSE),]
stest_df_p1 <- train_df[sample(x=nrow(train_df), size=2500, replace=FALSE),]

#transform variable shares
strain_df_p1$shares <- log(strain_df_p1$shares)
stest_df_p1$shares <- log(stest_df_p1$shares)

#drop variable url
strain_df_p1 <- strain_df_p1[,-1]
stest_df_p1 <- stest_df_p1[,-1]
```

```{r}
mtry <- c(2,4,6,8,10,20,30,40,50,59)
oob <- c()
rf_mse <- c()

for (i in mtry) {
  p1p1_m <- ranger(data=strain_df_p1, formula = shares~., mtry=i, num.trees=1000)
  
  oob <- c(oob,p1p1_m$prediction.error)
  p1p1_pred <- predict(p1p1_m, data=stest_df_p1)
  rf_mse <- c(rf_mse,mse(stest_df_p1$shares, p1p1_pred$predictions))
}
```

```{r}
p1p1_df <- data.frame(mtry,oob,rf_mse)

ggplot(data=p1p1_df) +
  geom_point(aes(x=mtry, y=rf_mse)) +
  geom_line(aes(x=mtry, y=rf_mse)) +
  theme_bw()

ggplot(data=p1p1_df) +
  geom_point(aes(x=mtry, y=oob)) +
  geom_line(aes(x=mtry, y=oob)) +
  theme_bw()
```

The number of features considered for each bootstrap tree that yields the lowest OOB error is 8, whereas the number of features that yields the lowest error on the heldout test set is 20. However, in terms of mse on the heldout test set, there's no significant decrease beyond when the number of features goes above 8. Therefore, we conclude that 8 is a good choice.

In addition, the specific dataset implies that we have more decrease in variance among trees than increase in estimates bias when mtry is smaller than 8, whereas the increase in estimates bias is equal to the decrease in tree variance when mtry goes beyond 8. Therefore, the out of sample mse drops most significantly until mtry hits 8, which is our optimal choice for mtry.


### Part 2

Using your chosen value for the number of variables to consider in your random forest, compute the permutation importance of the features for your subset of the training data.  Create a graph that demonstrates which variables are most important for the regression tree and which ones are least important.

Explore the relationship between the two most important predictors and the log number of shares.  Can you parse out the relationship between these variables and the predictions?

*Hint*: Try to see how these two variables and the true number of shares move together.  Then, do the same for the values of these variables and the corresponding predictions.  It probably isn't super clear, but you can likely see a bit of a trend.

```{r}
p1p2_m <- ranger(data=strain_df_p1, formula = shares~., mtry=8, num.trees=1000, 
                 importance='permutation')

p1p2_df <- data.frame(p1p2_m$variable.importance)
p1p2_df$var <- rownames(p1p2_df)
p1p2_df <- p1p2_df %>% rename(var_imp = p1p2_m.variable.importance)

ggplot(data=p1p2_df, aes(x=var, y=var_imp)) +
  geom_bar(stat='identity', width=0.75) + coord_flip() +
  labs(x="Features", y="Variable Importance") +
  theme_bw()
```

```{r}
p1p2_df %>% arrange(desc(var_imp)) %>% head(2) #most important
p1p2_df %>% arrange(desc(var_imp)) %>% tail(1) #least important

#true relationships
ggplot(data=strain_df_p1) +
  geom_point(aes(x=kw_avg_avg, y=shares)) +
  theme_bw()

ggplot(data=strain_df_p1) +
  geom_point(aes(x=kw_max_avg, y=shares))+
  theme_bw()

#prediction relationships
pred_r <- data.frame(kw_avg_avg = strain_df_p1$kw_avg_avg, 
                     kw_max_avg = strain_df_p1$kw_max_avg,
                     pred_shares = p1p2_m$predictions)

ggplot(data=pred_r) +
  geom_point(aes(x=kw_avg_avg, y=pred_shares)) +
  theme_bw()

ggplot(data=pred_r) +
  geom_point(aes(x=kw_max_avg, y=pred_shares)) +
  theme_bw()
```


## Question 2 - Building a Big Predictive Model (25 pts.)

Using all available predictors, come up with a predictive approach that maximizes the expected predictive accuracy for the **log number of shares** for articles in the unseen test set.  Your model for this problem should use any tools we've covered (or that you know from prerequisite courses) to build the predictive model that **minimizes the expected prediction error on the unseen test set**.  It can use any number of predictors and any functional form you can uncover.  Obviously, you don't have this test set, so you can never really know!  However, we've covered methods for approximating this value (to the best of our non-omniscient ability).

Please explain your thought process as to the choices you make!

For any method that you seriously consider as a candidate for producing the best possible predictive model, collect the following quantities and include them as a row in a table:

  1. An in-sample estimate of the EPE generated using appropriate methods
  2. A true out-of-sample estimate of the EPE generated using appropriate methods
  3. The time (in seconds) required for you to tune the model and run it on the full training data **together**.  This time should include the computational time needed to choose the values of any hyperparameters and run the model of the full training data to get a prediction method for unseen data.  There are many different ways to interpret and accomplish this task, but there is a general trend that you should see when training your models. 

Once you decide on a final model, use the data included in `midterm_test.csv` to produce a set of predictions for the test data.  These predictions should be exported as a .csv file with two columns - `ID` of the observation and the `prediction` produced by your model.  Please be sure to include a code block that demonstrates your method for doing this in your final submission.  Your predictions should be saved as "Midterm1Q1_Predictions.csv" and submitted along with your final solutions.

```{r}
#question - how to solve the collinearity problem??? do not use gam or subset selection
#question - does GAM model have collinearity problem??? yess
#question - how to find the optimal GAM model??? nooo
```

```{r}
#sample training dataset
set.seed(123)
sam_train <- as.numeric(sample(x=nrow(train_df), size=30000, replace=FALSE))
sam_test <- setdiff(seq(1,35000,1), sam_train)
strain_df_p2 <- train_df[sam_train,]
stest_df_p2 <- train_df[sam_test,]

#transform variable shares
strain_df_p2$shares <- log(strain_df_p2$shares)
stest_df_p2$shares <- log(stest_df_p2$shares)

#drop variable url
strain_df_p2 <- strain_df_p2[,-1]
stest_df_p2 <- stest_df_p2[,-1]
```

```{r}
#According to problem 1, we take for granted that 8 is a good choice for the number of variables considered for each bootstrap tree in the random forest model in terms of OOB error and MSE on the heldout data set.
p2_rf_m <- ranger(data=strain_df_p2, formula = shares~., mtry=8, num.trees=1000)
#OOB error
p2_rf_m$prediction.error
#out of sample mse
mse(predict(p2_rf_m, data=stest_df_p2)$predictions, stest_df_p2$shares)
```

```{r}
#According to problem 1 part 2, we can choose a subset of important variables to fit a GAM model.
p1p2_df %>% arrange(desc(var_imp))
p2_gam <- gam(data=strain_df_p2, family=gaussian(), 
              shares ~ s(kw_avg_avg)+s(kw_max_avg)+s(self_reference_avg_sharess))
#in sample mse
mse(predict.gam(p2_gam, strain_df_p2), strain_df_p2$shares)
#out of sample mse
mse(predict.gam(p2_gam, stest_df_p2), stest_df_p2$shares)
```

```{r}
p2_x <- data.matrix(strain_df_p2[,-60])
p2_y <- data.matrix(strain_df_p2[,60])

p2_cv_lassom <- cv.glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=1, nfolds=10,
                          type.measure="mse")
plot(p2_cv_lassom)
print(p2_cv_lassom)
p2_optimal_lassom <- glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=1,
                            lambda=p2_cv_lassom$lambda.min)
#in sample mse
mse(p2_y, predict(p2_optimal_lassom, newx=p2_x))
#out of sample mse
mse(stest_df_p2$shares, predict(p2_optimal_lassom, newx=data.matrix(stest_df_p2[,-60])))
```

```{r}
p2_cv_ridgem <- cv.glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=0, nfolds=10,
                          type.measure="mse")
plot(p2_cv_ridgem)
print(p2_cv_ridgem)
p2_optimal_ridgem <- glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=0,
                            lambda=p2_cv_ridgem$lambda.min)
#in sample mse
mse(p2_y, predict(p2_optimal_ridgem, newx=p2_x))
#out of sample mse
mse(stest_df_p2$shares, predict(p2_optimal_ridgem, newx=data.matrix(stest_df_p2[,-60])))
```

```{r}
knn <- seq(5,1000,100)
knn_mse_out <- c()

for (i in knn) {
  knn_m <- knn.reg(train=p2_x, test=stest_df_p2[,-60], y=p2_y, k=i)
  knn_mse_out <- c(knn_mse_out,mse(knn_m$pred, stest_df_p2$shares))
}

p2_knn_df <- data.frame(knn,knn_mse_out)
ggplot(data=p2_knn_df) +
  geom_point(aes(x=knn, y=knn_mse_out)) +
  geom_line(aes(x=knn, y=knn_mse_out)) +
  theme_bw()
```

```{r}
#conclusion
p2_mse_in_sample <- c(mse(predict.gam(p2_gam, strain_df_p2), strain_df_p2$shares),
                      mse(p2_y, predict(p2_optimal_lassom, newx=p2_x)),
                      mse(p2_y, predict(p2_optimal_ridgem, newx=p2_x)))
p2_mse_out_sample <- c(mse(predict.gam(p2_gam, stest_df_p2), stest_df_p2$shares),
                       mse(stest_df_p2$shares, predict(p2_optimal_lassom, 
                                                       newx=data.matrix(stest_df_p2[,-60]))),
                       mse(stest_df_p2$shares, predict(p2_optimal_ridgem, 
                                                       newx=data.matrix(stest_df_p2[,-60]))))
p2_m <- c('GAM','RIDGE','LASSO')
data.frame(model = p2_m,
           mse_in_sample = p2_mse_in_sample,
           mse_out_sample = p2_mse_out_sample)
```

In addition, the lowest out of sample mse in the knn model is 0.79 when k equals around 100. But the random forest model clearly outperforms all four, with an OOB error of 0.7239764 and out of sample mse of 0.6875225. Hence, the random forest model with the number of variables considered for each bootstrap tree of 8 is our optimal model.

  
## Question 3 - Interpreting Your Big Predictive Model (7 pts.)

Using your predictive model from question 1, explain how the predictions change as a  function of your predictors.  Specifically, try to figure out an interpretable set of conditions under which we would expect that an article is shared a lot and when we would expect it to be shared infrequently.  This is a high dimensional predictor space, so you won't be able to do this exactly, but do your best to come up with some general observations that dictate the predicted shares for an article.

*Hint*: I'm guessing that the method you choose in Q1 will not yield a particularly predictable prediction function.  Just do your best to try to make some general conclusions.  The goal of this question is to get you thinking about the difference between a good predictive model and an interpretable one!

```{r}
p3_m <- ranger(data=strain_df_p2, formula = shares~., mtry=8, num.trees=1000,
               importance='permutation')

p3_df <- data.frame(p3_m$variable.importance)
p3_df$var <- rownames(p3_df)
p3_df <- p3_df %>% rename(var_imp = p3_m.variable.importance)

ggplot(data=p3_df, aes(x=var, y=var_imp)) +
  geom_bar(stat='identity', width=0.75) + coord_flip() +
  labs(x="Features", y="Variable Importance") +
  theme_bw()

p3_df %>% arrange(desc(var_imp))
```

According to the variable importance plot from above based on our optimal random forest model from problem 2 trained on a dataset of 30000 observations with the mtry value of 8, some of the most important predictors that dictate the predicted shares of an article are kw_avg_avg, kw_max_avg, self_reference_avg_shares, kw_min_avg, LDA_02, and timedelta. Articles that are high in those features are likely to be shared more often than those that are not. However, since the random forest model is a good predictive model but not known as a good interpretable model, the important variable plot above may not be very accurate.


## Question 4 - Building a Predictive Model with a Subset of Predictors (25 pts.)

Though most of the model fitting methods we've discussed in class are interpretable in the sense that we can directly see how each predictor contributes to the overall prediction, it is difficult to parse through all of the predictors to derive a coherent set of rules that dictates the predictions.  The original purpose of this data set was to show that an automated content analysis system (called the Intelligent Decision Support System by the authors) could be used to improve the "shareability" of articles - translating predictions to actionable directions for maximizing profit.  While not as robust as causal analyses, predictive models do provide a method for understanding **associations** in high dimensional data and can be used to think about potential changes that lead to different predictions.  You'll discuss the caveats of this method in the next question.

Using the training data, build a predictive model for the log number of shares on an article that minimizes expected prediction error on an unseen data set **using no more than 5 predictors**.  Your goal here should be to both select a good set of predictors and model as much of the variation within the outcome using the small set of predictors - nonlinearities, interactions, and all.  Compared to the big prediction model, it should be easier to find these quirks and improve the model to better model them.

Please explain your thought process for the choices you make!

This process is a little less *standard* than the one for your big predictive model.  Rather than showing me the results of all the approaches you check, just summarize what things you tried to accomplish two tasks:

  1. Selecting the 5 variables
  2. Predicting the outcome using the five variables
  
At the end, please explicitly note the five variables that you chose and the method you used to produce your best predictive model.  Be sure to leverage both in-sample EPE estimates and estimates of EPE produced by a true holdout data set.

Once you decide on a final model, use the data included in `midterm_test.csv` to produce a set of predictions for the test data.  These predictions should be exported as a .csv file with two columns - `ID` of the observation and the `prediction` produced by your model.  Please be sure to include a code block that demonstrates your method for doing this in your final submission.  Your predictions should be saved as "Midterm1Q3_Predictions.csv" and submitted along with your final solutions.
  
## Question 5 - Interpreting the Subset Model (8 pts.)

Using your predictive model from Question 3, explain how the prediction surface changes as a function of your predictors.  Since the set of predictors is much more manageable, discuss what types of articles you would predict to have the highest number of shares and what types would have the lowest number of shares.  Does the prediction surface over this small subset of predictors look the same as it did in the full model?  What changes?

Using your model, come up with a set of actionable rules that you would pass to the Mashable writers to try to maximize the number of shares and discuss why this rule set makes sense in the context of the problem.  Then, discuss reasons why we might not expect the rules set to result in the expected increase in shares in the sense of "a change in $x$ results in a change in $y$".  Specifically think about how this method differs from a carefully designed causal analysis (think about confounding!).

## Question 6 - Building a Predictive Model with a Single Predictor (15 pts.)

Now, take the idea of creating a small actionable predictive model to the extreme and find the single predictor model that minimizes expected prediction error for log of shares on the unseen test data set.  Discuss your process and how you decided on a final model.  Create a plot that shows your chosen variable against the log number of shares for the training data.  Create a similar plot for a heldout test data set.  Then, overlay a curve that shows the predicted value of log shares as a function of your feature on both plots.

The difference between this question and the previous two questions is that there are a finite number of models to test!  After choosing a method that is most appropriate, just iterate over all possible features and compare two measures of EPE - an in-sample EPE method and actual performance on an out-of-sample data set.

In your final deliverable, explain your process of choosing a model that is appropriate for modeling univariate relationships and demonstrate the process that you used to choose which predictor was the best.  Be sure to engage with different EPE measures.

Once you decide on a final model, use the data included in `midterm_test.csv` to produce a set of predictions for the test data.  These predictions should be exported as a .csv file with two columns - `ID` of the observation and the `prediction` produced by your model.  Please be sure to include a code block that demonstrates your method for doing this in your final submission.  Your predictions should be saved as "Midterm1Q5_Predictions.csv" and submitted along with your final solutions.

Initial sampling and manipulation
```{r}
#sample training dataset
set.seed(456)
sam6_train <- as.numeric(sample(x=nrow(train_df), size=30000, replace=FALSE))
sam6_test <- setdiff(seq(1,35000,1), sam6_train)
strain_df_p6 <- train_df[sam6_train,]
stest_df_p6 <- train_df[sam6_test,]

#transform variable shares
strain_df_p6$shares <- log(strain_df_p6$shares)
stest_df_p6$shares <- log(stest_df_p6$shares)

#drop variable url
strain_df_p6 <- strain_df_p6[,-1]
stest_df_p6 <- stest_df_p6[,-1]
```

We start by getting some sense on what might be a good choice of the uni-variable. From the full model, we see that, conditional on mtry being 8, the variables that seem most important are "kw_avg_avg" and "kw_max_avg." However, from the plot of shares vs "kw_avg_avg" and vs "kw_max_avg," we saw a big blob of data points that seem hard to be captured by OLS or polynomial expansion of a single variable. Also, since OLS is dominated by ridge and lasso, we will not use OLS or polynomial expansion. We also won't use GAM since GAM with one "s(variable)" term is just univariate spline. Consequently, we will use a for-loop to iterate all 59 variables on all possible models we learned--KNN, ridge, lasso, spline, LOESS, RF with mtry = 1, and compute and compare in-sample and out-of-sample EPE to pick out best model and best variable.

KNN
```{r}
#out-of-sample
knn_p6_mse_out <- c()
#k = 1 since only 1 variable
#test for all 59 variables
for (j in 1:59) {
  knn_p6_m <- knn.reg(train=as.data.frame(strain_df_p6[,j]), test=as.data.frame(stest_df_p6[,j]), y=as.data.frame(strain_df_p6[,60]), k=1)
  knn_p6_mse_out <- c(knn_p6_mse_out,mse(knn_p6_m$pred, stest_df_p6$shares))
}
p6_knn_out_df <- data.frame(var = colnames(strain_df_p6[,-60]), knn_p6_mse_out)
p6_knn_out_df %>% arrange(knn_p6_mse_out) #weekday_is_wednesday; data_channel_is_world is 13th

#in-sample using KCV
knn_reg_kfold <- function(x, y, k, folds, seed){
  set.seed(seed)
  #sample
  sam6_KNN_kfold_train <- as.numeric(sample(nrow(as.data.frame(x)), size=ceiling(nrow(as.data.frame(x))/folds), replace=FALSE))
  sam6_KNN_kfold_test <- setdiff(seq(1,30000,1), sam6_KNN_kfold_train)
  #model for prediction error
  p6_p <- c()
  for (i in 1:folds){
    x_train <- x[sam6_KNN_kfold_train]
    x_test <- x[sam6_KNN_kfold_test]
    y_train <- y[sam6_KNN_kfold_train]
    y_test <- y[sam6_KNN_kfold_test]
    p6_KNN_kfoldm <- knn.reg(train = as.data.frame(x_train), test = as.data.frame(x_test), y = as.data.frame(y_train), k = k)
    p6_p[i] <- mse(p6_KNN_kfoldm$pred, as.numeric(unlist(y_test)))
  }
  return(p6_p[1]/folds)
}
p6_kfold_5 <- c()
p6_kfold_10 <- c()
for (j in 1:59) {
  p6_kfold_5[j] <- knn_reg_kfold(strain_df_p6[,j], strain_df_p6[,60], 1, 5, 456)
  p6_kfold_10[j] <- knn_reg_kfold(strain_df_p6[,j], strain_df_p6[,60], 1, 10, 456)
}
p6_knn_in_df <- data.frame(var = colnames(strain_df_p6[,-60]), p6_kfold_5, p6_kfold_10)
p6_knn_in_df %>% arrange(p6_kfold_5) #data_channel_is_world, 5fold cv; weekday_is_wednesday is 3rd
p6_knn_in_df %>% arrange(p6_kfold_10) #data_channel_is_world, 10fold cv; weekday_is_wednesday is 6th
```

Ridge and LASSO
```{r}
p6_x <- data.matrix(strain_df_p6[,-60]) #pool of 59 potential variables
p6_y <- data.matrix(strain_df_p6[,60])

#ridge
p6_ridge_in <- c()
p6_ridge_out <- c()
test <- cv.glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=0, nfolds=10,
                          type.measure="mse")
test2 <- glmnet(x=p6_x, y=p6_y, family="gaussian", alpha=0, nfolds=10,
                          lambda=test$lambda)

cf1 <- c()
cf2 <- c()
for (i in 1:37) {
  cf1 <- append(cf1, rownames(test2$beta)[i])
  cf2 <- append(cf2, unname(test2$beta)[i])
}
cf <- data.frame(cf1, cf2)

ggplot(data=cf, aes(x=cf1, y=cf2)) +
  geom_bar(stat="identity", width=0.75) + coord_flip() +
  labs(x="Features", y="Coefficient Estimate") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(face="bold", colour="red", size = 12),
        axis.title.y = element_text(face="bold", colour="red", size = 12))

for (i in 1:59){
  p6_cv_ridgem <- cv.glmnet(x=p6_x[,i], y=p6_y, family="gaussian", alpha=0, nfolds=10,
                          type.measure="mse")
  p6_optimal_ridgem <- glmnet(x=p6_x[,i], y=p6_y, family="gaussian", alpha=0,
                            lambda=p6_cv_ridgem$lambda.min)
  #in sample mse
  p6_ridge_in <- data.frame(p6_ridge_in, mse(p6_y, predict(p6_optimal_ridgem, newx=p6_x[,i])))
  #out of sample mse
  p6_ridge_out <- data.frame(p6_ridge_out, mse(stest_df_p6$shares, predict(p6_optimal_ridgem, newx=data.matrix(stest_df_p6[,i]))))
}

#LASSO
p2_cv_lassom <- cv.glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=1, nfolds=10,
                          type.measure="mse")
plot(p2_cv_lassom)
print(p2_cv_lassom)
p2_optimal_lassom <- glmnet(x=p2_x, y=p2_y, family="gaussian", alpha=1,
                            lambda=p2_cv_lassom$lambda.min)
#in sample mse
mse(p2_y, predict(p2_optimal_lassom, newx=p2_x))
#out of sample mse
mse(stest_df_p2$shares, predict(p2_optimal_lassom, newx=data.matrix(stest_df_p2[,-60])))
```

## Question 7 - Do your models perform well on out-of-sample data? (10 pts.)

For each model, I'll compute a root mean squared prediction error using your submitted predictions and the true log number of shares for the articles in the hidden test set.  Across all models turned in, points will be allocated as follows:

  1. For Q1 and Q3, let $\text{RMSE}_{\text{Min}}$ be the minimum prediction error over the submitted models.  Let $\text{RMSE}_i$ be your predictive error.  Your prediction points will be computed as:
  
  $$4 \times \frac{\text{RMSE}_{\text{Min}}}{\text{RMSE}_{i}}$$
  
  2. For Q5 it will be:
  
  $$2 \times \frac{\text{RMSE}_{\text{Min}}}{\text{RMSE}_{i}}$$

The goal here is to add a low-stakes incentive and fun "competition" to try to optimize your predictive models.  **I expect that the performance across groups/students will be quite close, so this won't greatly affect your final grade.**

