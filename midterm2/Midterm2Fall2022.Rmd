---
title: "Midterm Data Exercise #2 - Classification"
author: "Kevin McAlister"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  prettydoc::html_pretty:
    df_print: kable
    theme: leonids
    highlight: github
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
urlcolor: blue
---

```{r, include=FALSE}
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
library(mgcv)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

------------------------------------------------------------------------
```{r}
# data read-in
train_x <- read.csv('MNISTTrainXV2.csv')
train_y <- read.csv('MNISTTrainY.csv')
test_x <- read.csv('MNISTTestXRand.csv')
test_y <- read.csv("MNISTTestYRand.csv")
validation_x <- read.csv('MNISTValidationX.csv')
validation_y <- read.csv('MNISTValidationY.csv')
# plot function
plot_digit <- function(x, bw = FALSE,...){
  if(sqrt(length(x)) != round(sqrt(length(x)))){
    stop(print("Not a square image! Something is wrong here."))
  }
  n <- sqrt(length(x))
  if(bw == TRUE){
    x <- as.numeric(x > 50)*256
  }
  par(pty = "s")
  image(matrix(as.matrix(x), nrow = n)[,n:1], col = gray(12:1 / 12), ...)
}
#Example
plot_digit(x = train_x[1,], bw = FALSE, main = "True Class = 0") #x vector length = square of integer
plot_digit(x = train_x[1,], bw = TRUE, main = "True Class = 0")
```

Collaborators: Billy Ge, Latifa Tan, Annie Luo


## Question 1 (15 pts.)
### Part 1 Solution
```{r}
train_y[50,] #0
plot_digit(x = train_x[50,], bw = FALSE, main = "True Class = 0")
train_y[1500,] #0
plot_digit(x = train_x[1500,], bw = FALSE, main = "True Class = 0")
train_y[2800,] #1
plot_digit(x = train_x[2800,], bw = FALSE, main = "True Class = 1")
train_y[6500,] #2
plot_digit(x = train_x[6500,], bw = FALSE, main = "True Class = 2")
train_y[10000,] #3
plot_digit(x = train_x[10000,], bw = FALSE, main = "True Class = 3")
train_y[13780,] #5
plot_digit(x = train_x[13780,], bw = FALSE, main = "True Class = 5")
train_y[17000,] #6
plot_digit(x = train_x[17000,], bw = FALSE, main = "True Class = 6")
train_y[20000,] #7
plot_digit(x = train_x[20000,], bw = FALSE, main = "True Class = 7")
train_y[23730,] #9
plot_digit(x = train_x[23730,], bw = FALSE, main = "True Class = 9")
```
Yes the images match the labels.

### Part 2 Solution
```{r}
mean_train <- matrix(nrow = 10, ncol = 144)
x_split <- seq(1,27500,2500)
for (i in 1:144){
  for (j in 1:10){
    mean_train[j,i] <- mean(train_x[x_split[j]:(x_split[j+1]-1),i])
  }
}
for (j in 1:10){
  plot_digit(x = mean_train[j,], bw = FALSE)
}
```
解释还没写


### Part 3 Solution
解释还没写


## Question 2 (15 pts.)
## Question 2 Data
```{r}
p2_trainX <- train_x[1:5000,]
p2_trainY <- train_y[1:5000,]
p2_validX <- validation_x[1:3000,]
p2_validY <- validation_y[1:3000,]
```

### Part 1 Solution
```{r}
#logistic regression model
p2_train <- data.frame(p2_trainX, p2_trainY)
p2p1_logistic <- glm(data=p2_train, 
                     formula = p2_trainY ~ .,
                     family = binomial(link='logit'),
                     control = list(maxit = 100))
#in-sample EPE
folds <- create_folds(seq(1,nrow(p2_train)), k = 10, type = "basic")
mis_rate <- c()
for(i in 1:length(folds)){
  countMs = 0
  logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
  logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
  #classify predictions
  for (j in 1:length(logistic_cv_pred)){
    if (logistic_cv_pred[j] > 0.5) {
      logistic_cv_pred[j] <- 1
    }
    else {
      logistic_cv_pred[j] <- 0
    }
  }
  #Count of Misclassifications
  for (k in 1:length(logistic_cv_pred)) {
    if (logistic_cv_pred[k] != p2_train[-folds[[i]],]$p2_trainY[k]){
      countMs = countMs + 1
    }
  }
  mis_rate[i] <- countMs / length(logistic_cv_pred)
}
p2p1_epe<-mean(mis_rate)
p2p1_epe #0.0036
#oos
countMs = 0
logistic_valid <- predict(p2p1_logistic, newdata = p2_validX, type = "response")
#classify predictions
for (j in 1:length(logistic_valid)){
  if (logistic_valid[j] > 0.5) {
    logistic_valid[j] <- 1
  }
  else {
    logistic_valid[j] <- 0
  }
}
#Count of Misclassifications
for (k in 1:length(logistic_valid)) {
  if (logistic_valid[k] != p2_validY[k]){
    print(k) #misclassified: 271, 1057, 2203, 2259, 2311, 2555
    countMs = countMs + 1
  }
}
p2p1_oos <- countMs / length(logistic_valid)
p2p1_oos #0.002
zeroOne_result <- data.frame("In Sample" = p2p1_epe, "OOS" = p2p1_oos)
```
The in-sample EPE and oos PE are both extremely small. Our model has great predictive capabilities.
intuitive解释没写(Logistic regression predicts probabilities between 0 and 1, which are exactly the two classes we're trying to predict here. So logistic regressions is a perfect choice here.)

```{r}
countMs #there are 6 misclassifications: 271, 1057, 2203, 2259, 2311, 2555
validation_y[271,] #0
plot_digit(x = validation_x[271,], bw = FALSE, main = "True Class = 0")
validation_y[1057,] #0
plot_digit(x = validation_x[1057,], bw = FALSE, main = "True Class = 0")
validation_y[2203,] #1
plot_digit(x = validation_x[2203,], bw = FALSE, main = "True Class = 1")
validation_y[2311,] #1
plot_digit(x = validation_x[2311,], bw = FALSE, main = "True Class = 1")
```
The misclassification does make sense, because the ones have a lot of white area around the middle vertical line (when x is 0.4-0.6), and zeros have a lot of gray area around the middle vertical line.

### Part 2 Solution
```{r}
#LASSO
p2p2_trainX <- data.matrix(p2_trainX)
p2p2_trainY <- data.matrix(p2_trainY)
#scientific lambda
p2_cv_lasso <- cv.glmnet(x=p2p2_trainX, y=p2p2_trainY, family="binomial",
                     control = list(maxit = 100), alpha=1, nfolds=10,
                          type.measure="mse")
#our chosen value of lambda is
p2_cv_lasso$lambda.min
p2_lasso <- glmnet(x=p2p2_trainX, y=p2p2_trainY, family="binomial", alpha=1,
                            lambda=p2_cv_lasso$lambda.min)
p2_lasso$beta #coefficients
#correlations
p2_corr <- c()
for (i in 1:length(p2_lasso$beta)){
  if (p2_lasso$beta[i] > 0) {
    p2_corr[i] = 1
  }
  else if (p2_lasso$beta[i] < 0) {
    p2_corr[i] = -1
  }
  else {
    p2_corr[i] = 0
  }
}
cf1 <- colnames(p2_trainX)
names(p2_corr) <- cf1
matrix(nrow = 12, ncol = 12, p2_corr, byrow = TRUE)
```
As shown in the above matrix, where 1 represents a positive correlation, 0 non-important correlation, and -1 a negative correlation, we can see that the center has a positive correlation, meaning that the center being filled with pencil mark signals the class of one. There's also a 1 in [1,8], showing a pencil mark on the middle top signals the class of one. And there are -1 around the center, meaning marks there would decrease the probability of the number being classified as one, and therefore signals the class of zero.

## Question 3 (25 pts.)
## Question 3 Data
```{r}
p3_trainX <- train_x[c(10001:12500, 22501:25000),]
p3_trainY <- train_y[c(10001:12500, 22501:25000),]
p3_validX <- validation_x[c(6001:7500, 13501:15000),]
p3_validY <- validation_y[c(6001:7500, 13501:15000),]
```

### Part 1 Solution
```{r}
#recode for logistic regression
length(p3_trainY)
p3p1_trainY <- matrix(nrow = 5000, ncol = 1)
p3p1_trainY[1:2500,] = 0 #4 -> 0
p3p1_trainY[2501:5000,] = 1 #9 -> 1
length(p3_validY)
p3p1_validY <- matrix(nrow = 3000, ncol = 1)
p3p1_validY[1:1500,] = 0 #4 -> 0
p3p1_validY[1501:3000,] = 1 #9 -> 1
#logistic regression model
p3_logTrain <- data.frame(p3_trainX, p3p1_trainY)
p3p1_logistic <- glm(data=p3_logTrain, 
                     formula = p3p1_trainY ~ .,
                     family = binomial(link='logit'),
                     control = list(maxit = 100))
#in-sample EPE
folds <- create_folds(seq(1,nrow(p3_logTrain)), k = 10, type = "basic")
mis_rate <- c()
for(i in 1:length(folds)){
  countMs = 0
  logistic_cv <- glm(p3p1_trainY ~ ., data=p3_logTrain[folds[[i]],], trace=FALSE)
  logistic_cv_pred <- predict(logistic_cv, newdata = p3_logTrain[-folds[[i]],], type = "response")
  #classify predictions
  for (j in 1:length(logistic_cv_pred)){
    if (logistic_cv_pred[j] > 0.5) {
      logistic_cv_pred[j] <- 1
    }
    else {
      logistic_cv_pred[j] <- 0
    }
  }
  #Count of Misclassifications
  for (k in 1:length(logistic_cv_pred)) {
    if (logistic_cv_pred[k] != p3_logTrain[-folds[[i]],]$p3p1_trainY[k]){
      countMs = countMs + 1
    }
  }
  mis_rate[i] <- countMs / length(logistic_cv_pred)
}
p3p1_epe<-mean(mis_rate)
p3p1_epe #approx. 0.0508; numerically unstable
#oos
countMs = 0
logistic_valid <- predict(p3p1_logistic, newdata = p3_validX, type = "response")
#classify predictions
for (j in 1:length(logistic_valid)){
  if (logistic_valid[j] > 0.5) {
    logistic_valid[j] <- 1
  }
  else {
    logistic_valid[j] <- 0
  }
}
#Count of Misclassifications
for (k in 1:length(logistic_valid)) {
  if (logistic_valid[k] != p3p1_validY[k]){
    countMs = countMs + 1
  }
}
p3p1_oos <- countMs / length(logistic_valid)
p3p1_oos #0.035
fourNine_result <- data.frame("Logistic" = p3p1_oos)
```
The four-nine classification's in-sample EPE is 0.0508, while oos EPE is 0.035. The EPE is ten times larger than zero-one classification. The results differ mainly because four and nine naturally look much similar than zero and one do.

```{r}
#LASSO
p3_lTrainX <- data.matrix(p3_trainX)
p3_lTrainY <- data.matrix(p3_trainY)
#scientific lambda
p3_cv_lasso <- cv.glmnet(x=p3_lTrainX, y=p3_lTrainY, family="binomial",
                     control = list(maxit = 100), alpha=1, nfolds=10,
                          type.measure="mse")
#our chosen value of lambda is
p3_cv_lasso$lambda.min
p3_lasso <- glmnet(x=p3_lTrainX, y=p3_lTrainY, family="binomial", alpha=1,
                            lambda=p3_cv_lasso$lambda.min)
p3_lasso$beta #coefficients
#correlations
p3_corr <- c()
for (i in 1:length(p3_lasso$beta)){
  if (p3_lasso$beta[i] > 0) {
    p3_corr[i] = 1
  }
  else if (p3_lasso$beta[i] < 0) {
    p3_corr[i] = -1
  }
  else {
    p3_corr[i] = 0
  }
}
cf1 <- colnames(p3_trainX)
names(p3_corr) <- cf1
matrix(nrow = 12, ncol = 12, p3_corr, byrow = TRUE)
```
One big difference is that four-nine has much more important pixels for classification than zero-one. This conforms to our intuition that, because it's harder to tell four and nine apart, we need more features to predict the class. The upper center is concentrated with 1, signaling four. The lower center is full of -1, signaling nine.

### Part 2 Solution
```{r}
p3_train <- data.frame(p3_trainX, p3_trainY)
p3_valid <- data.frame(p3_validX, p3_validY)
# QDA 
FN_QDA_classifier <- qda(p3_trainY ~ ., data=p3_train)
FN_QDA <- predict(FN_QDA_classifier,p3_valid)$class
FN_QDA_oos <- mean(FN_QDA != p3_valid$p3_validY)
FN_QDA_oos #0.06533333
fourNine_result$QDA <- FN_QDA_oos
# Naive Bayes classifier: KDE naive bayes
#we use KDE since data was high dimensional and has a lot of obs.
FN_naiveBayes_classifier<-naive_bayes(as.character(p3_train$p3_trainY) ~ ., data=p3_train, usekernel=TRUE)
FN_naiveBayes <- predict(FN_naiveBayes_classifier, p3_valid, type= "class")
FN_naiveBayes_oos <- mean(FN_naiveBayes != p3_valid$p3_validY)
FN_naiveBayes_oos #0.16
fourNine_result$NaiveBayes <- FN_naiveBayes_oos
fourNine_result
```
We can see that logistic regression has the best performance, while NB has the worst. It makes sense that NaiveBayes does not perform very well, since when one pixel has pencil marks, it's likely that the pixels around it are marked too, which violates the independence assumption required by NB. QDA relies on the multivariate normal density (MND) assumption where, for each class, the predictors are drawn from a multivariate normal density function. In other words, QDA assumes that the marginal distribution of predictors are normal. With this assumption, QDA models $\log( \frac{Pr(Y=k | X=x)}{Pr(Y=K | X=x)})$ in a quadratic way in the sense that it allows interaction of pairs of predictors. It makes sense that it outperforms NB due to high correlation between pixels.  Logistic regression maps probabilities from 0 to 1. 解释没写完(assumpt of LR & why LR is the best)

### Part 3 (10 pts.)

Using the full suite of classification approaches discussed in class, find a classification approach that **minimizes** the expected misclassification rate of 4s and 9s on a true out of sample data set.

Your answer should discuss the possible approaches to this problem and explain how you made your final choice. Discuss how you chose any tuning parameter values.

You do not need to run all possible classification methods to get full credit for this question! There are some methods that we can rule out without ever running them. When doing this, provide grounded reasoning related to the strengths and weaknesses of different approaches.

For your chosen method, explain **why** it outperforms all other approaches. Think carefully about strengths and weaknesses.

Finally, present at least 4 examples of misclassified images. Could we ever expect a classification algorithm to get those images correct?

### Part 3 Solution
We have used Logistic Regression, QDA, and NB w/ KDE for continuous predictors. As explained above, we will not use NB without KDE due to high dimensionality. Since there are only two classes--four and nine, we won't be using Multinomial logistic regression. We also won't be using KNN due to the curse of high dimentionality. We also won't be treating predictors as discrete for NB, since the variables are values on a gray scale which are continuous. For the same reason, we also won't be using Bayes' Theorem for discrete features.
We will first continue our prediction with the lasso model we built before. Then, we will build new models using LDA, generalized additive LR, linearSVM, polySVM, radialSVM, and Random Forest (bagged trees, probability trees, boosted trees). We attempted the GAM for Logistic Regression, but since it's too computational expensive to try tensor ti() to capture interactions for all combinations, we decided to not continue using GAM for LR. We believe we didn't loose much info for not trying this method, since we already tried generalized methods of LDA and QDA (which captures interactions between different combinations of two variables).
```{r}
#lasso
p3_lvalidX <- as.matrix(p3_validX)
FN_lasso <- predict(p3_lasso, p3_lvalidX, type= "class")
FN_lasso_oos <- mean(FN_lasso != p3_validY)
FN_lasso_oos #0.03233333
fourNine_result$lasso <- FN_lasso_oos

#LDA
FN_LDA_classifier <- lda(p3_trainY ~ ., data=p3_train)
FN_LDA <- predict(FN_LDA_classifier,p3_validX)$class
FN_LDA_oos <- mean(FN_LDA != p3_validY)
fourNine_result$LDA <- FN_LDA_oos 
#we can see that LDA performs slightly better than QDA, so we believe that LDA is appropriate in the Four-Nine problem, where QDA has a little less precision. This is not surprising because we expected very unclean cuts as Four and Nine look very similar.

#Generalized Additive LR
#write all Xvariables in spline
#form <- as.formula(paste("p3p1_trainY~",paste("s(",names(p3_trainX),")",collapse="+")))
#FN_GAM <- gam(formula = form, data = p3_logTrain, family = binomial(link = 'logit'))

#SVM data prep
p3_trainYfactored <- factor(p3_trainY)
p3_trainFactored <- data.frame(p3_trainX, p3_trainYfactored)
p3_validYfactored <- factor(p3_validY)
p3_validFactored <- data.frame(p3_validX, p3_validYfactored)

#linearSVM
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values. We set our cost in an exponential way so that the cost we try won't be too high that the model overfits, and there are more cost values between 0 and 1 that we would try. This is because we expect the optimal cost value to fall between 0 and 1 for good generalizability, and we want to test carefully which cost value exactly would be the optimal value through CV. This tuning method applies to linearSVM, polySVM, and radialSVM.
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
  #Train the SVM
  svm1 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "vanilladot", 
                C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
  #Extract the 5fold CV est and store
  temp_p3 <- kernlab::cross(svm1)
  if (temp_p3<optimal_epe){
    optimal_epe<-temp_p3
    optimal_cost<-cost_vals[i]
  }
}
# finalized model
FN_linearSVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", 
                               kernel = "vanilladot", C = optimal_cost, cross = 5, prob.model = TRUE, 
                               kpar = list())
FN_linearSVM <- predict(FN_linearSVM_classfier, newdata = p3_validFactored , type = "response")
FN_linearSVM_oos<- mean(FN_linearSVM != p3_validYfactored)
fourNine_result$linearSVM <- FN_linearSVM_oos

#polySVM
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for (j in 2:3){ #conventionally 2nd or 3rd degree polynomial would be enough
  for(i in 1:length(cost_vals)){
    #Train the SVM
    svm2 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "polydot", 
                 kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
    #Extract the 5fold CV est and store
    temp_p3 <- kernlab::cross(svm2)
    if (temp_p3<optimal_epe){
      optimal_epe<-temp_p3
      optimal_cost<-cost_vals[i]
      optimal_degree<-j
    }
  }
}
# finalized model
FN_polySVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", 
                             kernel = "polydot", kpar = list(degree = optimal_degree), C = optimal_cost,
                             cross = 5, prob.model = TRUE)
FN_polySVM <- predict(FN_polySVM_classfier, newdata = p3_validFactored , type = "response")
FN_polySVM_oos<- mean(FN_polySVM != p3_validYfactored)
fourNine_result$polySVM <- FN_polySVM_oos

#radialSVM
# to find optimal cost
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
  #Train the SVM
  svm3 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "rbfdot", 
                C = cost_vals[i], cross = 5, prob.model = TRUE)
  #Extract the 5fold CV est and store
  temp_p3 <- kernlab::cross(svm3)
  if (temp_p3<optimal_epe){
    optimal_epe<-temp_p3
    optimal_cost<-cost_vals[i]
  }
}
# finalized model
FN_radialSVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", 
                            kernel = "rbfdot", C = optimal_cost, cross = 5, prob.model = TRUE)
FN_radialSVM <- predict(FN_radialSVM_classfier, newdata = p3_validFactored , type = "response")
FN_radialSVM_oos<- mean(FN_radialSVM != p3_validYfactored)
fourNine_result$radialSVM <- FN_radialSVM_oos

#tree methods
```

### Part 4 (5 pts.)

Use the hidden test set to generate a prediction for each included image. Your predictions should be stored in a matrix that has the image key as the first column and the integer value of the class in the second column. Save this matrix as `Q3Predictions.csv` and include it with your final submission.

Points for this question will be given with respect to classification accuracy. Let $E_i$ be the proportion of observations in the test set (that are actually 4s and 9s) that are correctly classified. Let $E_{max}$ be the maximum proportion of correctly classified observations across the class. Then, your final point total for this part will be:

$$5 \times \frac{E_i}{E_{max}}$$

## Question 4 (25 pts.)

Now, let's work with **three classes** - 3s, 5s, and 8s. Start by subsetting your training and validation sets to only include 3s, 5s and 8s.

### Part 1 (25 pts.)

Hidden among the 7500 3s, 5s, and 8s in the training set are 20 12 $\times$ 12 pixel images of sandals, sneakers, and ankle boots. These images are taken from another commonly used ML benchmarking data set called "Fashion MNIST". [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) is an alternative data set used for classification method benchmarking that has uses pictures of shirts, bags, shoes, etc. rather than images of handwritten digits. Each of the 20 images are shown in the figures below:

![](Shoes1.jpeg)

![](Shoes2.jpeg)

Using everything you've learned about classification methods in this class (and some intuition), come up with a method that finds these images in the training set.  At a minimum, create a plot of the most suspect "digits" that shows your best guesses as to the 20 anomalies.

Your points for this problem will be wholly allocated as a function of your method.  There is **no perfect answer** for this problem.  As long as you put in some effort to find these images, you will get most (if not all) points for this part.  As such, please be sure to clearly explain your approach to this task.  I am eager to see what you all come up with!  This is a really abstract problem, so just try your best.

Note that this is an example of **anomaly detection** - a subset of machine learning algorithms that intend to tell us when something in our data set is weird (or an outlier, if you like to use that kind of terminology).  This is a classification-*esque* problem as it would be easy to solve this if we had some labelled shoe images.  However, this training set doesn't include any appropriately labelled instances.

Notes:

  1. Part of what makes this task so challenging for this particular set of three digits is that 3s, 5s, and 8s look a lot alike!  This would be really easy for 0s and 1s because our classifiers are so well tuned for these digits.  This means that an approach that finds images close to the decision boundary for this classification problem will yield a mixture of really poorly written digits **and** the shoes.  This is a good starting point for creating your own approach, though!  You'll have to get creative to try to find things that really don't look like the other images in the data set.
  
  2. An initial thought may be to use instances from the fashion MNIST data set to train a "shoe/not shoe" classifier.  This will not work out-of-the-box because I have processed all of the original 28 x 28 images down to 12 x 12 images.  You are more than welcome to do this processing yourself if this is the route that you choose (though, I suggest not going this route as it kinda defeats the purpose of this problem).
  
  3. An approach that I highly recommend checking out is called a "one-class SVM".  Implementations of this approach exist in `ksvm` and `sklearn` in Python.  This approach differs from the SVM we covered in class as it uses a parameter `nu` rather than the cost.  Read the documentation on this method and apply it to drastically reduce the number of candidate images!  However, it's not guaranteed to return **all** of the shoe images.  You'll need to do a little work to get the rest.
  
  4. At a certain point, you may just want to use good old human checking to try to sort between bad digit images and shoe images.  Unless you're willing to commit a lot of time to labelling, just going through 7500 images one by one will not work.  However, looking at 30ish wouldn't take too long.  Use classification algorithms to find a few good candidates and sort through those to find your first few "shoe" images.
  
  5.  Once you've found a few shoes, think about creating a classifier.  Trees and SVMs are likely your best best for fitting a decision boundary with only a few observations in one specific class (e.g. just a few shoes).  One thing that will help here is to train a second classifier using a few "prototype" images - 3s, 5s, and 8s that are good examples of the digits and not the chicken-scratch that makes this problem so hard.
  
  6. Without using the knowledge of where the shoe images are, I was able to find 15 of the 20 images.  I, personally, had a hard time re-finding the images of the stiletto sandals because they look a lot like 5s!  
  
  7. I'm really eager to see your approaches here - this is a weird problem that requires a lot of thought.  But, I think that this is a good demonstration of how you might use the approaches we discussed in class to address atypical classification problems.



## Question 5 (20 pts.)

Finally, let's work with the full MNIST data set. This is a 10 class classification problem that has been studied extensively. With your work on this problem, you will join the club of data scientists who have taken a crack at one of machine learning's most infamous classification tasks!

### Part 1 (10 pts.)

Use any tools in our classification arsenal to try to minimize the expected misclassification rate for out of sample handwritten digits.

In your answer, you should benchmark any algorithms that are suited for the problem. For any classification methods that you know will not work well (by virtue of the structure of the data), justify why it's not worth the time to check it. Be sure to explain your method for tuning any hyperparameters.

For any models you run, produce a table that shows the misclassification rate on the validation set. Similarly, record the compute time needed to arrive at your final model (including any hyperparameter tuning) and include this in the same table. See [this StackOverflow thread](https://stackoverflow.com/questions/6262203/measuring-function-execution-time-in-r) for an elementary way to do this in R.

Which model performs the best on the 10 class problem? Why do you think this model performs the best? Compare your approach to other possible approaches when answering this question.

Is the tradeoff in accuracy vs. compute time worth the gain in realistic scenarios? Think about how these algorithms might scale as $N$ and $P$ get larger. Specifically, discuss the problem of hyperparameter tuning and how this might contribute to difficulty in implementing your chosen approach.

For one-off classification problems, the computational time may not matter. However, there is significant research into the area of **online classification algorithms** - algorithms in which new predictions can be made quickly using existing data and the classification algorithm can be updated using new training data as it arrives. See [this Wikipedia page](https://en.wikipedia.org/wiki/Online_machine_learning) for more information on this topic.

**WARNING!**  SVMs will be very competitive on this problem.  However, the compute time needed to train them will be very high.  As always, I expect that you will tune the cost parameter for any SVM.  So, even more compute time!  To mitigate this, you are encouraged to subset the data for SVMs (and trees if you're really running into problems).  I think that a reasonable size is 500 per class.  You can also reduce the number of cost tuning parameters that you check, though I highly recommend trying cost values that are both small, medium (around 1), and large.  Trees are much more scalable, so I think you'll be able to use the full data for those.  

### Part 2 (5 pts.)

For your final model, compute a **confusion matrix** for the validation set that shows how often each digit is classified in each class. Which incorrect classifications happen most frequently?

For the most commonly confused digit pairs, plot examples that are misclassified. What aspect of these images led to their misclassification?

The MNIST data as presented to you has been simplified in some ways. There are also other data cleaning tasks that can improve predictive accuracy. What are some steps that could be taken in the data cleaning stage that would help your chosen method improve its misclassification rate?

### Part 3 (5 pts.)

Use the hidden test set to generate a prediction for each included image. Your predictions should be stored in a matrix that has the image key as the first column and the integer value of the class in the second column. Save this matrix as `Q5Predictions.csv` and include it with your final submission.

Points for this question will be given with respect to classification accuracy. Let $E_i$ be the proportion of observations in the test set misclassified. Let $E_{min}$ be the minimum proportion of misclassified observations across the class. Then, you final point total for this part will be:

$$5 \times \frac{E_{max}}{E_i}$$
