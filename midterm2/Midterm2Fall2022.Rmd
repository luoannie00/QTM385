---
title: "Midterm Data Exercise #2 - Classification"
author: "Kevin McAlister"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  prettydoc::html_pretty:
    df_print: kable
    theme: leonids
    highlight: github
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
urlcolor: blue
---

```{r, include=FALSE}
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab) #KSVM
library(MASS) #QDA
library(nnet) #MultiNom
library(FNN)
library(splitTools)
library(ranger)
library(glmnet) #Ridge&LASSO
library(naivebayes)
library(mgcv) #GAM
library(gbm)
library(xgboost)
library(dplyr)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

------------------------------------------------------------------------
```{r}
# data read-in
train_x <- read.csv('MNISTTrainXV2.csv')
train_y <- read.csv('MNISTTrainY.csv')
test_x <- read.csv('MNISTTestXRand.csv')
test_y <- read.csv("MNISTTestYRand.csv")
validation_x <- read.csv('MNISTValidationX.csv')
validation_y <- read.csv('MNISTValidationY.csv')
# plot function from instruction
plot_digit <- function(x, bw = FALSE,...) {
  if(sqrt(length(x)) != round(sqrt(length(x)))) {
    stop(print("Not a square image! Something is wrong here."))
  }
  n <- sqrt(length(x))
  if(bw == TRUE) {
    x <- as.numeric(x > 50)*256
  }
  par(pty = "s")
  image(matrix(as.matrix(x), nrow = n)[,n:1], col = gray(12:1 / 12), ...)
}
#Example
plot_digit(x = train_x[1,], bw = FALSE, main = "True Class = 0") #x vector length = square of integer
# bw= TRUE: round up pixel to black or white
plot_digit(x = train_x[1,], bw = TRUE, main = "True Class = 0")
```

Collaborators: Billy Ge, Latifa Tan, Annie Luo


## Question 1 (15 pts.)
### Part 1 Solution
```{r}
train_y[50,] #image label 0
plot_digit(x = train_x[50,], bw = FALSE, main = "True Class = 0")
train_y[1500,] #0
plot_digit(x = train_x[1500,], bw = FALSE, main = "True Class = 0")
train_y[2800,] #1
plot_digit(x = train_x[2800,], bw = FALSE, main = "True Class = 1")
train_y[6500,] #2
plot_digit(x = train_x[6500,], bw = FALSE, main = "True Class = 2")
train_y[10000,] #3
plot_digit(x = train_x[10000,], bw = FALSE, main = "True Class = 3")
train_y[13780,] #5
plot_digit(x = train_x[13780,], bw = FALSE, main = "True Class = 5")
train_y[17000,] #6
plot_digit(x = train_x[17000,], bw = FALSE, main = "True Class = 6")
train_y[20000,] #7
plot_digit(x = train_x[20000,], bw = FALSE, main = "True Class = 7")
train_y[23730,] #9
plot_digit(x = train_x[23730,], bw = FALSE, main = "True Class = 9")
```
Yes the images match the labels.

### Part 2 Solution
```{r}
mean_train <- matrix(nrow = 10, ncol = 144)
x_split <- seq(1,27500,2500)
for (i in 1:144){
  for (j in 1:10){
    mean_train[j,i] <- mean(train_x[x_split[j]:(x_split[j+1]-1),i])
  }
}
for (j in 1:10){
  plot_digit(x = mean_train[j,], bw = FALSE)
}
```
Class for label 4,5,6,8, and 9 seem to show the most within class variation over images; class for label 1 and 0 seem to show the least within class variation over images.


### Part 3 Solution
We use euclidean distance between each obs. to means of each classes as our measure of similarity. Since there are certain classes that have large variation within the class (4,5,6,8,9), the classes with large variances are going to be difficult to tell apart. The zero-one pair will be easy since those classes have low variances.


## Question 2 (15 pts.)

## Question 2 Data
```{r}
#subset data to only include 0s and 1s
p2_trainX <- train_x[1:5000,]
p2_trainY <- train_y[1:5000,]
p2_validX <- validation_x[1:3000,]
p2_validY <- validation_y[1:3000,]
```

### Part 1 Solution
```{r}
#logistic regression model
p2_train <- data.frame(p2_trainX, p2_trainY)
p2p1_logistic <- glm(data=p2_train, 
                     formula = p2_trainY ~ .,
                     family = binomial(link='logit'),
                     control = list(maxit = 100))
#in-sample EPE
folds <- create_folds(seq(1,nrow(p2_train)), k = 10, type = "basic")
mis_rate <- c()
for(i in 1:length(folds)){
  countMs = 0
  logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
  logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
  #classify predictions
  for (j in 1:length(logistic_cv_pred)){
    if (logistic_cv_pred[j] > 0.5) {
      logistic_cv_pred[j] <- 1
    }
    else {
      logistic_cv_pred[j] <- 0
    }
  }
  #Count of Misclassifications
  for (k in 1:length(logistic_cv_pred)) {
    if (logistic_cv_pred[k] != p2_train[-folds[[i]],]$p2_trainY[k]){
      countMs = countMs + 1
    }
  }
  mis_rate[i] <- countMs / length(logistic_cv_pred)
}
p2p1_epe<-mean(mis_rate)
p2p1_epe #0.0036
#oos
countMs = 0
logistic_valid <- predict(p2p1_logistic, newdata = p2_validX, type = "response")
#classify predictions
for (j in 1:length(logistic_valid)){
  if (logistic_valid[j] > 0.5) {
    logistic_valid[j] <- 1
  }
  else {
    logistic_valid[j] <- 0
  }
}
#Count of Misclassifications
for (k in 1:length(logistic_valid)) {
  if (logistic_valid[k] != p2_validY[k]){
    #print(k) misclassified: 271, 1057, 2203, 2259, 2311, 2555
    countMs = countMs + 1
  }
}
p2p1_oos <- countMs / length(logistic_valid)
p2p1_oos #0.002
zeroOne_result <- data.frame("In Sample" = p2p1_epe, "OOS" = p2p1_oos)
zeroOne_result
```
The in-sample EPE and oos PE are both extremely small. Our model has great predictive capabilities.
It's easy to find separators between 0 and 1, which makes LR shine, because LR draws simple linear boundaries and thus has high precision. Logistic regression calculates the probability of digit 1 given all the pixel values of each image. Since 0 and 1 are very different images visually, the logistic regression is capable of classifying them easily.

```{r}
countMs #there are 6 misclassifications: 271, 1057, 2203, 2259, 2311, 2555
validation_y[271,] #0
plot_digit(x = validation_x[271,], bw = FALSE, main = "True Class = 0")
validation_y[1057,] #0
plot_digit(x = validation_x[1057,], bw = FALSE, main = "True Class = 0")
validation_y[2203,] #1
plot_digit(x = validation_x[2203,], bw = FALSE, main = "True Class = 1")
validation_y[2311,] #1
plot_digit(x = validation_x[2311,], bw = FALSE, main = "True Class = 1")
```
The misclassification does make sense, because the ones have a lot of white area around the middle vertical line (when x is 0.4-0.6), and zeros have a lot of gray area around the middle vertical line.

### Part 2 Solution
```{r}
#LASSO
p2p2_trainX <- data.matrix(p2_trainX)
p2p2_trainY <- data.matrix(p2_trainY)
#scientific lambda
p2_cv_lasso <- cv.glmnet(x=p2p2_trainX, y=p2p2_trainY, family="binomial",
                     control = list(maxit = 100), alpha=1, nfolds=10,
                          type.measure="mse")
#our chosen value of lambda is
p2_cv_lasso$lambda.min
p2_lasso <- glmnet(x=p2p2_trainX, y=p2p2_trainY, family="binomial", alpha=1,
                            lambda=p2_cv_lasso$lambda.min)
p2_lasso$beta #coefficients
#correlations
p2_corr <- c()
for (i in 1:length(p2_lasso$beta)){
  if (p2_lasso$beta[i] > 0) {
    p2_corr[i] = 1
  }
  else if (p2_lasso$beta[i] < 0) {
    p2_corr[i] = -1
  }
  else {
    p2_corr[i] = 0
  }
}
cf1 <- colnames(p2_trainX)
names(p2_corr) <- cf1
matrix(nrow = 12, ncol = 12, p2_corr, byrow = TRUE)
```
As shown in the above matrix, where 1 represents a positive correlation, 0 non-important correlation, and -1 a negative correlation, we can see that the center has a positive correlation, meaning that the center being filled with pencil mark signals the class of one. There's also a 1 in [1,8], showing a pencil mark on the middle top signals the class of one. And there are -1 around the center, meaning marks there would decrease the probability of the number being classified as one, and therefore signals the class of zero.

## Question 3 (25 pts.)
## Question 3 Data
```{r}
p3_trainX <- train_x[c(10001:12500, 22501:25000),]
p3_trainY <- train_y[c(10001:12500, 22501:25000),]
p3_validX <- validation_x[c(6001:7500, 13501:15000),]
p3_validY <- validation_y[c(6001:7500, 13501:15000),]
```

### Part 1 Solution
```{r}
#recode for logistic regression
length(p3_trainY)
p3p1_trainY <- matrix(nrow = 5000, ncol = 1)
p3p1_trainY[1:2500,] = 0 #4 -> 0
p3p1_trainY[2501:5000,] = 1 #9 -> 1
length(p3_validY)
p3p1_validY <- matrix(nrow = 3000, ncol = 1)
p3p1_validY[1:1500,] = 0 #4 -> 0
p3p1_validY[1501:3000,] = 1 #9 -> 1
#logistic regression model
p3_logTrain <- data.frame(p3_trainX, p3p1_trainY)
p3p1_logistic <- glm(data=p3_logTrain, 
                     formula = p3p1_trainY ~ .,
                     family = binomial(link='logit'),
                     control = list(maxit = 100))
#in-sample EPE
folds <- create_folds(seq(1,nrow(p3_logTrain)), k = 10, type = "basic")
mis_rate <- c()
for(i in 1:length(folds)){
  countMs = 0
  logistic_cv <- glm(p3p1_trainY ~ ., data=p3_logTrain[folds[[i]],], trace=FALSE)
  logistic_cv_pred <- predict(logistic_cv, newdata = p3_logTrain[-folds[[i]],], type = "response")
  #classify predictions
  for (j in 1:length(logistic_cv_pred)){
    if (logistic_cv_pred[j] > 0.5) {
      logistic_cv_pred[j] <- 1
    }
    else {
      logistic_cv_pred[j] <- 0
    }
  }
  #Count of Misclassifications
  for (k in 1:length(logistic_cv_pred)) {
    if (logistic_cv_pred[k] != p3_logTrain[-folds[[i]],]$p3p1_trainY[k]){
      countMs = countMs + 1
    }
  }
  mis_rate[i] <- countMs / length(logistic_cv_pred)
}
p3p1_epe<-mean(mis_rate)
p3p1_epe #approx. 0.0508; numerically unstable
#oos
countMs = 0
logistic_valid <- predict(p3p1_logistic, newdata = p3_validX, type = "response")
#classify predictions
for (j in 1:length(logistic_valid)){
  if (logistic_valid[j] > 0.5) {
    logistic_valid[j] <- 1
  }
  else {
    logistic_valid[j] <- 0
  }
}
#Count of Misclassifications
for (k in 1:length(logistic_valid)) {
  if (logistic_valid[k] != p3p1_validY[k]){
    countMs = countMs + 1
  }
}
p3p1_oos <- countMs / length(logistic_valid)
p3p1_oos #0.035
fourNine_result <- data.frame("In sample logistic"=p3p1_epe, 'OOS logistic'=p3p1_oos)
fourNine_result
```
The four-nine classification's in-sample EPE is 0.0508, while oos EPE is 0.035. The EPE is ten times larger than zero-one classification. The results differ mainly because four and nine naturally look much similar than zero and one do.

```{r}
#LASSO
p3_lTrainX <- data.matrix(p3_trainX)
p3_lTrainY <- data.matrix(p3_trainY)
#scientific lambda
p3_cv_lasso <- cv.glmnet(x=p3_lTrainX, y=p3_lTrainY, family="binomial",
                     control = list(maxit = 100), alpha=1, nfolds=10,
                          type.measure="mse")
#our chosen value of lambda is
p3_cv_lasso$lambda.min
p3_lasso <- glmnet(x=p3_lTrainX, y=p3_lTrainY, family="binomial", alpha=1,
                            lambda=p3_cv_lasso$lambda.min)
p3_lasso$beta #coefficients
#correlations
p3_corr <- c()
for (i in 1:length(p3_lasso$beta)){
  if (p3_lasso$beta[i] > 0) {
    p3_corr[i] = 1
  }
  else if (p3_lasso$beta[i] < 0) {
    p3_corr[i] = -1
  }
  else {
    p3_corr[i] = 0
  }
}
cf1 <- colnames(p3_trainX)
names(p3_corr) <- cf1
matrix(nrow = 12, ncol = 12, p3_corr, byrow = TRUE)
```
One big difference is that four-nine has much more important pixels for classification than zero-one. This conforms to our intuition that, because it's harder to tell four and nine apart, we need more features to predict the class. The upper center is concentrated with 1 in a semi- circular shape, signaling the top half of nine. The lower center(row 8) is largely concentrated with -1, signaling the horizontal line in the bottom of four.

### Part 2 Solution
```{r}
p3_train <- data.frame(p3_trainX, p3_trainY)
p3_valid <- data.frame(p3_validX, p3_validY)
# QDA 
FN_QDA_classifier <- qda(p3_trainY ~ ., data=p3_train)
FN_QDA <- predict(FN_QDA_classifier,p3_valid)$class
FN_QDA_oos <- mean(FN_QDA != p3_valid$p3_validY)
FN_QDA_oos #0.06533333
fourNine_result$QDA <- FN_QDA_oos
# Naive Bayes classifier: KDE naive bayes
#we use KDE since data was high dimensional and has a lot of obs.
FN_naiveBayes_classifier<-naive_bayes(as.character(p3_train$p3_trainY) ~ ., data=p3_train, usekernel=TRUE)
FN_naiveBayes <- predict(FN_naiveBayes_classifier, p3_valid, type= "class")
FN_naiveBayes_oos <- mean(FN_naiveBayes != p3_valid$p3_validY)
FN_naiveBayes_oos #0.16
fourNine_result$NaiveBayes <- FN_naiveBayes_oos
fourNine_result
```
We can see that logistic regression has the best performance, while NB has the worst. It makes sense that NaiveBayes does not perform very well, since when one pixel has pencil marks, it's likely that the pixels around it are marked too, which violates the independence assumption required by NB. QDA relies on the multivariate normal density (MND) assumption where, for each class, the predictors are drawn from a multivariate normal density function. In other words, QDA assumes that the marginal distribution of predictors are normal. With this assumption, QDA models $\log( \frac{Pr(Y=k | X=x)}{Pr(Y=K | X=x)})$ in a quadratic way in the sense that it allows interaction of pairs of predictors. It makes sense that it outperforms NB due to high correlation between pixels. However, our data does not conform well to QDA's assumed MND distribution of features, because most of our pixels are either 250 (pencil-marked) or 0 (not marked), rather than distributed on a continuous scale. LR, on the other hand, assumes Bernoulli distribution of the outcome variable, which our data conforms to since our possible outcomes zero and one is represented via a 2-class categorical variable.


### Part 3 Solution
We have used Logistic Regression, QDA, and NB w/ KDE for continuous predictors. As explained above, we will not use NB without KDE due to high dimensionality. Since there are only two classes--four and nine, we won't be using Multinomial logistic regression. We also won't be using KNN due to the curse of high dimentionality. We've seen in the matrix created from the lasso problem that there seems to be a lot of pixels that are quite important in the prediction, so we won't use DANN either since we don't expect a huge reduction of dimensions. We also won't be treating predictors as discrete for NB, since the variables are values on a gray scale which are continuous. For the same reason, we also won't be using Bayes' Theorem for discrete features.
We will first continue our prediction with the lasso model we built before. Then, we will build new models using LDA, generalized additive LR, linearSVM, polySVM, radialSVM, and tree methods (Bagging, Random Forest, Probability Trees, Boosted Trees). We attempted the GAM for Logistic Regression, but since it's too computational expensive to try tensor ti() to capture interactions for all combinations, we decided to not continue using GAM for LR. We believe we didn't loose much info for not trying this method, since we already tried generalized methods of LDA and QDA (which captures interactions between different combinations of two variables).
```{r}
#lasso
p3_lvalidX <- as.matrix(p3_validX)
FN_lasso <- predict(p3_lasso, p3_lvalidX, type= "class")
FN_lasso_oos <- mean(FN_lasso != p3_validY)
FN_lasso_oos #0.03233333
fourNine_result$lasso <- FN_lasso_oos
```

```{r}
#LDA
FN_LDA_classifier <- lda(p3_trainY ~ ., data=p3_train)
FN_LDA <- predict(FN_LDA_classifier,p3_validX)$class
FN_LDA_oos <- mean(FN_LDA != p3_validY)
fourNine_result$LDA <- FN_LDA_oos 
#we can see that LDA performs slightly better than QDA, so we believe that LDA is appropriate in the Four-Nine problem, where QDA has a little less precision. This is not surprising because we expected very unclean cuts as Four and Nine look very similar. Hence, instead of the unclean cut provided by QDA, a simpler linear decision boundary in high dimension space might have a better performance, as suggested by LDA.
```

```{r}
#SVM data prep
p3_trainYfactored <- factor(p3_trainY)
p3_trainFactored <- data.frame(p3_trainX, p3_trainYfactored)
p3_validYfactored <- factor(p3_validY)
p3_validFactored <- data.frame(p3_validX, p3_validYfactored)

#linearSVM
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values. 
#We set our cost in an exponential way so that the cost we try won't be too high that the model overfits, and there are more cost values between 0 and 1 that we would try. This is because we expect the optimal cost value to fall between 0 and 1 for good generalizability, and we want to test carefully which cost value exactly would be the optimal value through CV. This tuning method applies to linearSVM, polySVM, and radialSVM.
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
  #Train the SVM
  svm1 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "vanilladot", 
                C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
  #Extract the 5fold CV est and store
  temp_p3 <- kernlab::cross(svm1)
  if (temp_p3<optimal_epe){
    optimal_epe<-temp_p3
    optimal_cost<-cost_vals[i]
  }
}
# finalized model
FN_linearSVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", 
                               kernel = "vanilladot", C = optimal_cost, cross = 5, prob.model = TRUE, 
                               kpar = list())
FN_linearSVM <- predict(FN_linearSVM_classfier, newdata = p3_validFactored , type = "response")
FN_linearSVM_oos<- mean(FN_linearSVM != p3_validYfactored)
fourNine_result$linearSVM <- FN_linearSVM_oos

#polySVM
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for (j in 2:3){ #conventionally 2nd or 3rd degree polynomial would be enough
  for(i in 1:length(cost_vals)){
    #Train the SVM
    svm2 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "polydot", 
                 kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
    #Extract the 5fold CV est and store
    temp_p3 <- kernlab::cross(svm2)
    if (temp_p3<optimal_epe){
      optimal_epe<-temp_p3
      optimal_cost<-cost_vals[i]
      optimal_degree<-j
    }
  }
}
# finalized model
FN_polySVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", 
                             kernel = "polydot", kpar = list(degree = optimal_degree), C = optimal_cost,
                             cross = 5, prob.model = TRUE)
FN_polySVM <- predict(FN_polySVM_classfier, newdata = p3_validFactored , type = "response")
FN_polySVM_oos<- mean(FN_polySVM != p3_validYfactored)
fourNine_result$polySVM <- FN_polySVM_oos

#radialSVM
# to find optimal cost
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
  #Train the SVM
  svm3 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "rbfdot", 
                C = cost_vals[i], cross = 5, prob.model = TRUE)
  #Extract the 5fold CV est and store
  temp_p3 <- kernlab::cross(svm3)
  if (temp_p3<optimal_epe){
    optimal_epe<-temp_p3
    optimal_cost<-cost_vals[i]
  }
}
# finalized model
FN_radialSVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", 
                            kernel = "rbfdot", C = optimal_cost, cross = 5, prob.model = TRUE)
FN_radialSVM <- predict(FN_radialSVM_classfier, newdata = p3_validFactored , type = "response")
FN_radialSVM_oos<- mean(FN_radialSVM != p3_validYfactored)
fourNine_result$radialSVM <- FN_radialSVM_oos
```

```{r}
#tree methods(bagging, random forest, probability trees, boosted trees)
#data: y-var will also be factored. so we use same data we used in SVM.

#Bagging
#Since random forest increases variance between trees, it strictly dominates bagging trees. Hence we start off with random forest.

#Random Forest
optimal_varNum<-0
optimal_treeNum<-0
optimal_oob<-100000000
varNum <- c(2:8, 20, 60, 100) #too many var would be like bagging. but considering the high dimentionality of our x_train dataset, we decided to try a couple below 100.
for (i in 1:length(varNum)){
  rfMod<-ranger(p3_trainYfactored ~ . , data = p3_trainFactored, num.trees = 1000, 
                mtry = varNum[i], classification = TRUE)
  temp_oob<-rfMod$prediction.error
  if (temp_oob < optimal_oob){
    optimal_oob<-temp_oob
    optimal_varNum<-i
  }
}
FN_RF_classifier<-ranger(p3_trainYfactored ~ . , data = p3_trainFactored, 
                         num.trees = 1000, mtry = varNum[optimal_varNum], 
                         classification = TRUE)
FN_RF <- predict(FN_RF_classifier, data = p3_validFactored)$predictions
FN_RF_oos<- mean(FN_RF != p3_validFactored$p3_validYfactored)
fourNine_result$RF <- FN_RF_oos
```


```{r}
#Probability Trees
optimal_varNum<-0
optimal_treeNum<-0
optimal_oob<-100000000
varNum <- c(2:8, 20, 60, 100)
for (i in 1:length(varNum)){
  probTMod<-ranger(p3_trainYfactored ~ . , data = p3_trainFactored, num.trees = 1000,
                   probability = TRUE, mtry = varNum[i], classification = TRUE)
  temp_oob<-probTMod$prediction.error
  if (temp_oob < optimal_oob){
    optimal_oob<-temp_oob
    optimal_varNum<-i
  }
}
# finalized model
FN_probT_classifier<-ranger(p3_trainYfactored ~ . , data = p3_trainFactored, 
                            num.trees = 1000, probability = TRUE, 
                            mtry = varNum[optimal_varNum], classification = TRUE)
FN_probT <- predict(FN_probT_classifier, data = p3_validFactored)$predictions
FN_probTree <- c()
for (i in 1:3000){
  if (FN_probT[i,1] > FN_probT[i,2]){
    FN_probTree[i] = 4
  }
  else{
    FN_probTree[i] = 9
  }
}
FN_probTree <- factor(FN_probTree)
FN_probT_oos<- mean(FN_probTree != p3_validFactored$p3_validYfactored)
fourNine_result$ProbabilityTrees <- FN_probT_oos
```


```{r}
#Boosted Trees
#Data prep
p3_trainY_GBM <- p3p1_trainY
p3_train_GBM <- data.frame(p3_trainX, p3_trainY_GBM)
p3_validY_GBM <- p3p1_validY
p3_valid_GBM <- data.frame(p3_validX, p3_validY_GBM)

#ADA
optimal_treeNum<-0
optimal_err<-100000000
treeNum <- seq(500, 3000, 500)
for (i in 1:length(treeNum)){
  ADAm <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM), 
              distribution = "bernoulli", n.trees = treeNum[i], cv.folds = 5)
  #we use validation data to calculate temp_err to prevent overfit
  temp_err <- mean(round(predict(ADAm, newdata = p3_valid_GBM, n.trees = 60, type = "response")) 
                   != p3_valid_GBM$p3_validY_GBM)
  if (temp_err < optimal_err){
      optimal_err<-temp_err
      optimal_treeNum<-i
  }
}
FN_ADA_classifier <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM), 
                         distribution = "bernoulli", n.trees = treeNum[optimal_treeNum], cv.folds = 5)
FN_ADA_oos <- mean(round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60, 
                                 type = "response")) != p3_valid_GBM$p3_validY_GBM)
fourNine_result$ADA <- FN_ADA_oos

#XGB
xgb_trainY <- p3p1_trainY #coded in {0,1}
xgb_train <- data.frame(p3_trainX, xgb_trainY)
xgb_train$xgb_trainY <- NULL
xgb_validY <- p3p1_validY #coded in {0,1}
xgb_valid <- data.frame(p3_validX, p3p1_validY)
xgb_valid$p3p1_validY <- NULL
depth <- seq(2,20,2)
learn_rate <- seq(0.1,1,0.1)
treeNum <- seq(50, 15000, 3000)
optimal_depth<-0
optimal_learn_rate<-0
optimal_treeNum<-0
optimal_err<-100000000
#waited 8 hours. don't run this!!
for (i in 1:length(depth)){
  for (j in 1:length(learn_rate)){
    for (k in 1:length(treeNum)){
      xgbMod <- xgboost(data = as.matrix(xgb_train), label = xgb_trainY, max.depth = depth[i], 
                        eta = learn_rate[j], nrounds = treeNum[k], 
                        objective = "binary:logistic", verbose = 0)
      #we use validation data to calculate temp_err to prevent overfit
      temp_err <- mean(round(predict(xgbMod, newdata = as.matrix(xgb_valid))) != xgb_validY)
      if (temp_err < optimal_err){
        optimal_err<-temp_err
        optimal_treeNum<-k
        optimal_learn_rate<-j
        optimal_depth<-i
      }
    }
  }
}
depth[optimal_depth] #8
learn_rate[optimal_learn_rate] #0.2
treeNum[optimal_treeNum] #3050
#finalized model
FN_xgb_classifier <- xgboost(data = as.matrix(xgb_train), label = xgb_trainY, 
                             max.depth = depth[optimal_depth], eta = learn_rate[optimal_learn_rate], 
                             nrounds = treeNum[optimal_treeNum], 
                        objective = "binary:logistic", verbose = 0)
FN_xgb_oos <- mean(round(predict(xgbMod, newdata = as.matrix(xgb_valid))) != xgb_validY)
fourNine_result$xgb <- FN_xgb_oos
```

```{r}
fourNine_result
fourNine_result[which.min(fourNine_result)]
```
We can see that polySVM performed the best. radialSVM performed worse probably becuase we did not tune the width of the kernal hyperparameter since we didn't think it's computationally beneficial. SVM performed better than trees since tree methods still have a weak assumption of the starting search point. In our case, our tree methods have found a local minimum, but may not have found the global minimum, making trees perform worse than the truly assumption-free SVMs. For qda, lda, NB, lasso, and LR, they all performed worse because their assumptions about distributions, about features or outcomes, or about shape of decision boundaries were not properly fulfilled by our dataset. Therefore, they performed worse than our assumption-free SVM.

```{r}
for (k in 1:length(FN_polySVM)) {
  if (FN_polySVM[k] != p3_validYfactored[k]){
    print(k) #misclassified: 371, 1240, 2025, 2734
  }
}
p3_validYfactored[371] #4
plot_digit(x = p3_validX[371,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[1240] #4
plot_digit(x = p3_validX[1240,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[2025] #9
plot_digit(x = p3_validX[2025,], bw = FALSE, main = "True Class = 9")
p3_validYfactored[2734] #9
plot_digit(x = p3_validX[2734,], bw = FALSE, main = "True Class = 9")
```
It makes sense that the above 4 images are misclassified--each digit does look like it belongs to the opposite class. However, I think images 1 and 4 may have a chance of being classified correctly, if those two images are oriented more towards the center line and thus less slanted.


### Part 4 Solution
```{r}
test_y$label <- predict(FN_polySVM_classfier, newdata=test_x)
write.csv(as.matrix(test_y), file='Q3Predictions.csv', row.names=FALSE)
```


## Question 4 (25 pts.)
## Question 4 Data
```{r}
p4_trainX <- train_x[c(7501:10000, 12501:15000, 20001:22500),]
p4_trainY <- train_y[c(7501:10000, 12501:15000, 20001:22500),]
p4_validX <- validation_x[c(4501:6000, 7501:9000, 12001:13500),]
p4_validY <- validation_y[c(4501:6000, 7501:9000, 12001:13500),]
```

### Part 1 Solution
From the observation of shoe pictures, we found that a commonality is that the 12th column has dark marks. The 3s, 5s, and 8s, however, are not likely to have the whole or most of 12th column filled. So we picked three points at the 12th column after transforming all observations to 12x12 matrices and tuned a grayscale value to find as many shoes as possible.
```{r}
shoes <- c()
for (i in 1:7500){
  p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
  if (p4_trainMatrix[6,12] > 5 && p4_trainMatrix[4,12] > 5 && p4_trainMatrix[8,12] > 5) {
    shoes <- append(shoes, i)
  }
}
for (i in 1:length(shoes)){
  plot_digit(x = p4_trainX[shoes[i],], bw = FALSE)
}
```
We then matched those pictures with the ones given and identified the remaining 2 shoes to be found. For one shoe, both its upper-right corner and lower-left corner seem to be marked, which should be rare for digits.
```{r}
obs <- c()
for (i in 1:7500){
  p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
  if (p4_trainMatrix[1,12] > 100 && p4_trainMatrix[12,1] > 100) {
    obs <- append(obs, i)
  }
}
for (i in 1:length(obs)){
  plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
}
obs
shoes <- append(1244, shoes)
```
Now we only have one shoe left to find. We observe that its middle-left and two points on the right are marked. We reasoned that those three points may be more likely to be filled for digits too, so we start by setting a high grey-scale value.
```{r}
obs <- c()
for (i in 1:7500){
  p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
  if (p4_trainMatrix[6,1] > 50 && p4_trainMatrix[4,12] > 50 && p4_trainMatrix[7,12] > 50) {
    obs <- append(obs, i)
  }
}
for (i in 1:length(obs)){
  plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
}
obs
shoes <- append(2190, shoes)
for (i in 1:length(shoes)){
  plot_digit(x = p4_trainX[shoes[i],], bw = FALSE)
}
```

## Question 5 (20 pts.)

Finally, let's work with the full MNIST data set. This is a 10 class classification problem that has been studied extensively. With your work on this problem, you will join the club of data scientists who have taken a crack at one of machine learning's most infamous classification tasks!

### Part 1 (10 pts.)

Use any tools in our classification arsenal to try to minimize the expected misclassification rate for out of sample handwritten digits.

In your answer, you should benchmark any algorithms that are suited for the problem. For any classification methods that you know will not work well (by virtue of the structure of the data), justify why it's not worth the time to check it. Be sure to explain your method for tuning any hyperparameters.

For any models you run, produce a table that shows the misclassification rate on the validation set. Similarly, record the compute time needed to arrive at your final model (including any hyperparameter tuning) and include this in the same table. See [this StackOverflow thread](https://stackoverflow.com/questions/6262203/measuring-function-execution-time-in-r) for an elementary way to do this in R.

Which model performs the best on the 10 class problem? Why do you think this model performs the best? Compare your approach to other possible approaches when answering this question.

Is the tradeoff in accuracy vs. compute time worth the gain in realistic scenarios? Think about how these algorithms might scale as $N$ and $P$ get larger. Specifically, discuss the problem of hyperparameter tuning and how this might contribute to difficulty in implementing your chosen approach.

For one-off classification problems, the computational time may not matter. However, there is significant research into the area of **online classification algorithms** - algorithms in which new predictions can be made quickly using existing data and the classification algorithm can be updated using new training data as it arrives. See [this Wikipedia page](https://en.wikipedia.org/wiki/Online_machine_learning) for more information on this topic.

**WARNING!**  SVMs will be very competitive on this problem.  However, the compute time needed to train them will be very high.  As always, I expect that you will tune the cost parameter for any SVM.  So, even more compute time!  To mitigate this, you are encouraged to subset the data for SVMs (and trees if you're really running into problems).  I think that a reasonable size is 500 per class.  You can also reduce the number of cost tuning parameters that you check, though I highly recommend trying cost values that are both small, medium (around 1), and large.  Trees are much more scalable, so I think you'll be able to use the full data for those.  

### Part 1 Solution
Since there are 10 classes, logistic regression (which is suited for 2 classes) doesn't really work. Instead, we will start with multinomial logistic regression. Then we will try LASSO. For the same reason for the fourNine problem, we are not going to use GAM, Bayes for discrete features, NB with discrete features. And for the same reason of high dimentionality, we won't be using NB w/o KDE or KNN.
We will be using MLR, LASSO, LDA, QDA, NB w/ continuous variable w/ KDE, 2 SVM methods (we are not using linear SVM for the same reason as fourNine problem), and tree methods. We won't be using bagging since RF is likely to outperfom bagging. Also, to save computation time, we don't tune num.trees since from our previous experiences, the change in num.trees usually don't change oos EPE too much.
```{r MLR}
start.time <- Sys.time()
#Factored Data
train_factored <- train_x
train_factored$train_yFactored <- factor(train_y$label)
valid_factored <- validation_x
valid_factored$valid_yFactored <- factor(validation_y$label)

#MLR
multiLogistic_classifier <- nnet::multinom(train_yFactored ~., data = train_factored, 
                                     trace = FALSE, MaxNWts = 80000)
multiLog_pred<-predict(multiLogistic_classifier, newdata =valid_factored)
multiLog_oos<- mean(multiLog_pred != valid_factored$test_yFactored)
result <- data.frame("Multinomial LR" = multiLog_oos)


#lasso
lasso_TrainX <- data.matrix(train_x)
lasso_TrainY <- data.matrix(train_y)
lasso_validX <- as.matrix(validation_x)
cv_lasso <- cv.glmnet(x=lasso_TrainX, y=lasso_TrainY, family="multinomial",
                     control = list(maxit = 100), alpha=1, nfolds=5,
                          type.measure="mse")
lasso_classifier <- glmnet(x=lasso_TrainX, y=lasso_TrainY, family="multinomial", alpha=1,
                            lambda=cv_lasso$lambda.min)
lasso <- predict(lasso_classifier, lasso_validX, type= "class")
lasso_oos <- mean(lasso != validation_y)
result$lasso <- lasso_oos


#data prep
train <- data.frame(train_x, "train_y" = train_y$label)
#LDA
LDA_classifier <- lda(train_y ~ ., data=train)
LDA <- predict(LDA_classifier,validation_x)$class
LDA_oos <- mean(LDA != validation_y$label)
result$LDA <- LDA_oos 
#QDA
QDA_classifier <- qda(train_y ~ ., data=train)
QDA <- predict(QDA_classifier,validation_x)$class
QDA_oos <- mean(QDA != validation_y$label)
result$QDA <- QDA_oos
# Naive Bayes classifier: KDE naive bayes
#we use KDE since data was high dimensional and has a lot of obs.
naiveBayes_classifier<-naive_bayes(as.character(train$train_y) ~ ., data=train, usekernel=TRUE)
naiveBayes <- predict(naiveBayes_classifier, validation_x, type= "class")
naiveBayes_oos <- mean(naiveBayes != validation_y$label)
result$NaiveBayes <- naiveBayes_oos



#SVM data prep
#subsetted data
set.seed(456)
sam_train_factored <- train_factored %>% group_by(train_yFactored) %>% slice_sample(n=500)
#sam_train_factored$train_yFactored
#valid_factored
#valid_factored$test_yFactored

#polySVM
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
cost_vals <- exp(seq(-3,3,length = 30))
#We set our cost in an exponential way so that the cost we try won't be too high that the model overfits, and there are more cost values between 0 and 1 that we would try. This is because we expect the optimal cost value to fall between 0 and 1 for good generalizability, and we want to test carefully which cost value exactly would be the optimal value through CV. This tuning method applies to polySVM and radialSVM.
for (j in 2:3){ #conventionally 2nd or 3rd degree polynomial would be enough
  for(i in 1:length(cost_vals)){
    #Train the SVM
    svm2 <- ksvm(train_yFactored ~ . , data = sam_train_factored, type = "C-svc", kernel = "polydot", 
                 kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
    #Extract the 5fold CV est and store
    temp <- kernlab::cross(svm2)
    if (temp<optimal_epe){
      optimal_epe<-temp
      optimal_cost<-cost_vals[i]
      optimal_degree<-j
    }
  }
}
# finalized model
polySVM_classfier <- ksvm(train_yFactored ~ . , data = sam_train_factored, type = "C-svc", 
                             kernel = "polydot", kpar = list(degree = optimal_degree), C = optimal_cost,
                             cross = 5, prob.model = TRUE)
polySVM <- predict(polySVM_classfier, newdata = valid_factored, type = "response")
polySVM_oos<- mean(polySVM != valid_factored$valid_yFactored)
result$polySVM <- polySVM_oos

#radialSVM
# to find optimal cost
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
  #Train the SVM
  svm3 <- ksvm(train_yFactored ~ . , data = sam_train_factored, type = "C-svc", kernel = "rbfdot", 
                C = cost_vals[i], cross = 5, prob.model = TRUE)
  #Extract the 5fold CV est and store
  temp <- kernlab::cross(svm3)
  if (temp<optimal_epe){
    optimal_epe<-temp
    optimal_cost<-cost_vals[i]
  }
}
# finalized model
radialSVM_classfier <- ksvm(train_yFactored ~ . , data = sam_train_factored, type = "C-svc", 
                            kernel = "rbfdot", C = optimal_cost, cross = 5, prob.model = TRUE)
radialSVM <- predict(radialSVM_classfier, newdata = valid_factored, type = "response")
radialSVM_oos<- mean(radialSVM != valid_factored$valid_yFactored)
result$radialSVM <- radialSVM_oos



#tree methods(bagging, random forest, probability trees, boosted trees)
#data: y-var will also be factored.
#train_factored
#train_factored$train_yFactored
#valid_factored
#valid_factored$test_yFactored

#Random Forest
optimal_varNum<-0
optimal_treeNum<-0
optimal_oob<-100000000
varNum <- c(2,8,20) #too many var would be like bagging. but considering the high dimentionality of our x_train dataset, we decided to try a couple below 20.
for (i in 1:length(varNum)){
  rfMod<-ranger(train_yFactored ~ . , data = train_factored, num.trees = 1000, 
                mtry = varNum[i], classification = TRUE)
  temp_oob<-rfMod$prediction.error
  if (temp_oob < optimal_oob){
    optimal_oob<-temp_oob
    optimal_varNum<-i
  }
}
RF_classifier<-ranger(train_yFactored ~ . , data = train_factored, num.trees = 1000, 
                      mtry = varNum[optimal_varNum], classification = TRUE)
RF <- predict(RF_classifier, data = valid_factored)$predictions
RF_oos<- mean(RF != valid_factored$valid_yFactored)
result$RF <- RF_oos

#Probability Trees
optimal_varNum<-0
optimal_treeNum<-0
optimal_oob<-100000000
varNum <- c(2,8,20)
for (i in 1:length(varNum)){
  probTMod<-ranger(train_yFactored ~ . , data = train_factored, num.trees = 1000, 
                  probability = TRUE, mtry = varNum[i], classification = TRUE)
  temp_oob<-probTMod$prediction.error
  if (temp_oob < optimal_oob){
    optimal_oob<-temp_oob
    optimal_varNum<-i
  }
}
# finalized model
probT_classifier<-ranger(train_yFactored ~ . , data = train_factored, num.trees = 1000,
                          probability = TRUE, mtry = varNum[optimal_varNum], classification = TRUE)
probT <- predict(probT_classifier, data = valid_factored)$predictions
probTree <- c()
for (i in 1:15000){
  probTree[i] <- which.max(probT[i,]) -1
}
probTree <- factor(probTree)
probT_oos<- mean(probTree != valid_factored$valid_yFactored)
result$ProbabilityTrees <- probT_oos
end.time <- Sys.time()

#computation time
time.taken <- end.time - start.time
result$time.taken <- time.taken
```
As we can see, polySVM has the lowest oos EPE, as expected due to reasons explained in the fourNine problem. So we decided to go with polySVM as our final model

```{r p5finalModel}
polySVM_classfier
```

### Part 2 (5 pts.)

For your final model, compute a **confusion matrix** for the validation set that shows how often each digit is classified in each class. Which incorrect classifications happen most frequently?

For the most commonly confused digit pairs, plot examples that are misclassified. What aspect of these images led to their misclassification?

The MNIST data as presented to you has been simplified in some ways. There are also other data cleaning tasks that can improve predictive accuracy. What are some steps that could be taken in the data cleaning stage that would help your chosen method improve its misclassification rate?

### Part 3 Solution
```{r}
test_y$label <- predict(polySVM_classfier, newdata=test_x, type = "response")
write.csv(as.matrix(test_y), file='Q5Predictions.csv', row.names=FALSE)
```
