library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# data read-in
train_x <- read.csv('MNISTTrainXV2.csv')
train_y <- read.csv('MNISTTrainY.csv')
test_x <- read.csv('MNISTTestXRand.csv')
test_y <- read.csv("MNISTTestYRand.csv")
validation_x <- read.csv('MNISTValidationX.csv')
validation_y <- read.csv('MNISTValidationY.csv')
# plot function
plot_digit <- function(x, bw = FALSE,...){
if(sqrt(length(x)) != round(sqrt(length(x)))){
stop(print("Not a square image! Something is wrong here."))
}
n <- sqrt(length(x))
if(bw == TRUE){
x <- as.numeric(x > 50)*256
}
par(pty = "s")
image(matrix(as.matrix(x), nrow = n)[,n:1], col = gray(12:1 / 12), ...)
}
#Example
plot_digit(x = train_x[1,], bw = FALSE, main = "True Class = 0") #x vector length = square of integer
plot_digit(x = train_x[1,], bw = TRUE, main = "True Class = 0")
train_y[50,] #0
plot_digit(x = train_x[50,], bw = FALSE, main = "True Class = 0")
train_y[1500,] #0
plot_digit(x = train_x[1500,], bw = FALSE, main = "True Class = 0")
train_y[2800,] #1
plot_digit(x = train_x[2800,], bw = FALSE, main = "True Class = 1")
train_y[6500,] #2
plot_digit(x = train_x[6500,], bw = FALSE, main = "True Class = 2")
train_y[10000,] #3
plot_digit(x = train_x[10000,], bw = FALSE, main = "True Class = 3")
train_y[13780,] #5
plot_digit(x = train_x[13780,], bw = FALSE, main = "True Class = 5")
train_y[17000,] #6
plot_digit(x = train_x[17000,], bw = FALSE, main = "True Class = 6")
train_y[20000,] #7
plot_digit(x = train_x[20000,], bw = FALSE, main = "True Class = 7")
train_y[23730,] #9
plot_digit(x = train_x[23730,], bw = FALSE, main = "True Class = 9")
mean_train <- matrix(nrow = 10, ncol = 144)
x_split <- seq(1,27500,2500)
for (i in 1:144){
for (j in 1:10)
mean_train[j,i] <- mean(train_x[x_split[j]:(x_split[j+1]-1),i])
}
View(mean_train)
for (j in 1:10){
plot_digit(x = mean_train[j,], bw = FALSE)
}
p2_trainX <- train_x[1:5000,]
View(p2_trainX)
p2_trainY <- train_y[1:5000,]
p2_trainY
tail(p2_trainY)
p2_validX <- validation_x[1:3000,]
p2_validY <- validation_y[1:3000,]
tail(p2_validY)
validation_y[3001,]
p2_trainY <- train_y[1:5000]
p2_trainY <- train_y[1:5000,]
p2_train <- data.frame(p2_trainX, p2_trainY)
View(p2_train)
p2p1_logistic <- glm(data=p2_train,
formula = p2_trainY ~ .,
family = binomial(link='logit'))
predict(p2p1_logistic, type='response', newdata=p2_train)
# 1) training misclassification rate
logistic_pred<-predict(p2p1_logistic, newdata = p2_train, type = "response")
tail(logistic_pred)
for (i in 1:nrow(p2_trainX)) {
if (logistic_pred[i] > 0.5) {
logistic_pred[i] <- 1
}
else {
logistic_pred[i] <- 0
}
}
logistic_pred
tail(logistic_pred)
logistic_pred[2500]
logistic_pred[2501]
#predicted classification
logistic_pred<-predict(p2p1_logistic, newdata = p2_train, type = "class")
#predicted classification
logistic_pred<-predict(p2p1_logistic, newdata = p2_train, type = "link")
head(logistic_pred)
tail(logistic_pred)
#predicted classification
logistic_pred<-predict(p2p1_logistic, newdata = p2_train, type = "response")
#logistic regression model
p2_train <- data.frame(p2_trainX, p2_trainY)
p2p1_logistic <- glm(data=p2_train,
formula = p2_trainY ~ .,
family = binomial(link='logit'))
#in-sample EPE
folds <- create_folds(seq(1,nrow(p2_train)), k = 10, type = "basic")
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
#logistic regression model
p2_train <- data.frame(p2_trainX, p2_trainY)
p2p1_logistic <- glm(data=p2_train,
formula = p2_trainY ~ .,
family = binomial(link='logit'))
#in-sample EPE
folds <- create_folds(seq(1,nrow(p2_train)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
#predicted classification
if (logistic_cv_pred[i] > 0.5) {
logistic_cv_pred[i] <- 1
}
else {
logistic_cv_pred[i] <- 0
}
#Proportion Misclassified
kf_misclass[i] <- mean(logistic_cv_pred != p2_train[-folds[[i]],]$p2_trainY)
}
p2p1_epe<-mean(kf_misclass)
p2p1_epe
kf_misclass
length(folds)
folds
length(logistic_cv_pred)
#in-sample EPE
folds <- create_folds(seq(1,nrow(p2_train)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
#predicted classification
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
#Count of Misclassifications
if (logistic_cv_pred[j] != p2_train[-folds[[i]],]$p2_trainY[j])
}
p2_train[-folds[[1]],]$p2_trainY[500]
#in-sample EPE
folds <- create_folds(seq(1,nrow(p2_train)), k = 10, type = "basic")
kf_misclass <- c()
countMs = 0
for(i in 1:length(folds)){
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
#predicted classification
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
#Count of Misclassifications
if (logistic_cv_pred[j] != p2_train[-folds[[i]],]$p2_trainY[j]){
++countMs
}
}
}
for(i in 1:length(folds)){
for(i in 1:length(folds)){
for(i in 1:length(folds)){
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
#predicted classification
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_cv_pred)) {
if (logistic_cv_pred[j] != p2_train[-folds[[i]],]$p2_trainY[j]){
countMs[i] = countMs[i] + 1
}
}
}
for(i in 1:length(folds)){
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
#predicted classification
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_cv_pred)) {
if (logistic_cv_pred[j] != p2_train[-folds[[i]],]$p2_trainY[j]){
countMs[i] = countMs[i] + 1
}
}
}
for(i in 1:length(folds)){
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
#classify predictions
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_cv_pred)) {
if (logistic_cv_pred[k] != p2_train[-folds[[i]],]$p2_trainY[k]){
countMs[i] = countMs[i] + 1
}
}
}
countMs
p2_trainX <- train_x[1:5000,]
p2_trainY <- train_y[1:5000,]
p2_validX <- validation_x[1:3000,]
p2_validY <- validation_y[1:3000,]
#logistic regression model
p2_train <- data.frame(p2_trainX, p2_trainY)
p2p1_logistic <- glm(data=p2_train,
formula = p2_trainY ~ .,
family = binomial(link='logit'))
#in-sample EPE
folds <- create_folds(seq(1,nrow(p2_train)), k = 10, type = "basic")
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
#logistic regression model
p2_train <- data.frame(p2_trainX, p2_trainY)
p2p1_logistic <- glm(data=p2_train,
formula = p2_trainY ~ .,
family = binomial(link='logit'))
#in-sample EPE
folds <- create_folds(seq(1,nrow(p2_train)), k = 10, type = "basic")
kf_misclass <- c()
countMs <- c()
for(i in 1:length(folds)){
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
#classify predictions
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_cv_pred)) {
if (logistic_cv_pred[k] != p2_train[-folds[[i]],]$p2_trainY[k]){
countMs[i] = countMs[i] + 1
}
}
}
p2p1_epe<-mean(kf_misclass)
p2p1_epe
countMs
for(i in 1:length(folds)){
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
#classify predictions
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_cv_pred)) {
if (logistic_cv_pred[k] != p2_train[-folds[[i]],]$p2_trainY[k]){
countMs[i] = countMs[i] + 1
}
else {
count[i] = 0
}
}
}
for(i in 1:length(folds)){
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
#classify predictions
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_cv_pred)) {
if (logistic_cv_pred[k] != p2_train[-folds[[i]],]$p2_trainY[k]){
countMs[i] = countMs[i] + 1
}
else {
countMs[i] = 0
}
}
}
countMs
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[1]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[1]],], type = "response")
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
}
logistic_cv_pred
p2_train[-folds[[i]],]$p2_trainY
p2_train[-folds[[1]],]$p2_trainY[266]
p2_train[-folds[[1]],]$p2_trainY[265]
p2_train[-folds[[1]],]$p2_trainY[267]
logistic_cv_pred[266]
mis_rate <- c()
for(i in 1:length(folds)){
countMs = 0
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
#classify predictions
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_cv_pred)) {
if (logistic_cv_pred[k] != p2_train[-folds[[i]],]$p2_trainY[k]){
countMs = countMs + 1
}
}
mis_rate[i] <- countMs / length(logistic_cv_pred)
}
mis_rate
p2p1_epe<-mean(mis_rate)
p2p1_epe
mis_rate <- c()
countMs = 0
logistic_valid <- predict(p2p1_logistic, newdata = p2_validX, type = "response")
#classify predictions
for (j in 1:length(logistic_valid)){
if (logistic_valid[j] > 0.5) {
logistic_valid[j] <- 1
}
else {
logistic_valid[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_valid)) {
if (logistic_valid[k] != p2_validY[k]){
countMs = countMs + 1
}
}
mis_rate[i] <- countMs / length(logistic_valid)
p2p1_oos<-mean(mis_rate)
p2p1_oos
mis_rate <- countMs / length(logistic_valid)
p2p1_oos<-mean(mis_rate)
p2p1_oos
logistic_valid
length(logistic_valid)
logistic_valid[1500]
logistic_valid[1501]
length(mis_rate)
p2p1_oos <- countMs / length(logistic_valid)
p2p1_oos #0.002
#oos
countMs = 0
logistic_valid <- predict(p2p1_logistic, newdata = p2_validX, type = "response")
#classify predictions
for (j in 1:length(logistic_valid)){
if (logistic_valid[j] > 0.5) {
logistic_valid[j] <- 1
}
else {
logistic_valid[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_valid)) {
if (logistic_valid[k] != p2_validY[k]){
countMs = countMs + 1
}
}
p2p1_oos <- countMs / length(logistic_valid)
p2p1_oos #0.002
countMs
#Count of Misclassifications
for (k in 1:length(logistic_valid)) {
if (logistic_valid[k] != p2_validY[k]){
print(k)
countMs = countMs + 1
}
}
validation_y[271,] #0
plot_digit(x = validation_x[271,], bw = FALSE, main = "True Class = 0")
validation_y[2203,] #0
validation_y[2311,] #0
validation_y[1057,] #0
countMs #there are 6 misclassifications: 271, 1057, 2203, 2259, 2311, 2555
validation_y[271,] #0
plot_digit(x = validation_x[271,], bw = FALSE, main = "True Class = 0")
validation_y[1057,] #0
plot_digit(x = validation_x[1057,], bw = FALSE, main = "True Class = 0")
validation_y[2203,] #1
plot_digit(x = validation_x[2203,], bw = FALSE, main = "True Class = 1")
validation_y[2311,] #1
plot_digit(x = validation_x[2311,], bw = FALSE, main = "True Class = 1")
#logistic regression model
p2_train <- data.frame(p2_trainX, p2_trainY)
p2p1_logistic <- glm(data=p2_train,
formula = p2_trainY ~ .,
family = binomial(link='logit'),
control = list(maxit = 100))
#in-sample EPE
folds <- create_folds(seq(1,nrow(p2_train)), k = 10, type = "basic")
mis_rate <- c()
for(i in 1:length(folds)){
countMs = 0
logistic_cv <- glm(p2_trainY ~ ., data=p2_train[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p2_train[-folds[[i]],], type = "response")
#classify predictions
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_cv_pred)) {
if (logistic_cv_pred[k] != p2_train[-folds[[i]],]$p2_trainY[k]){
countMs = countMs + 1
}
}
mis_rate[i] <- countMs / length(logistic_cv_pred)
}
p2p1_epe<-mean(mis_rate)
p2p1_epe #0.0034
#oos
countMs = 0
logistic_valid <- predict(p2p1_logistic, newdata = p2_validX, type = "response")
#classify predictions
for (j in 1:length(logistic_valid)){
if (logistic_valid[j] > 0.5) {
logistic_valid[j] <- 1
}
else {
logistic_valid[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_valid)) {
if (logistic_valid[k] != p2_validY[k]){
print(k) #misclassified: 271, 1057, 2203, 2259, 2311, 2555
countMs = countMs + 1
}
}
p2p1_oos <- countMs / length(logistic_valid)
p2p1_oos #0.002
