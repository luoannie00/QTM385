}
# finalized model
FN_linearSVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc",
kernel = "vanilladot", C = optimal_cost, cross = 5, prob.model = TRUE,
kpar = list())
FN_linearSVM <- predict(FN_linearSVM_classfier, newdata = p3_validFactored , type = "response")
FN_linearSVM_oos<- mean(FN_linearSVM != p3_validYfactored)
fourNine_result$linearSVM <- FN_linearSVM_oos
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for (j in 2:3){ #conventionally 2nd or 3rd degree polynomial would be enough
for(i in 1:length(cost_vals)){
#Train the SVM
svm2 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "polydot",
kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p3 <- kernlab::cross(svm2)
if (temp_p3<optimal_epe){
optimal_epe<-temp_p3
optimal_cost<-cost_vals[i]
optimal_degree<-j
}
}
}
#polySVM
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for (j in 2:3){ #conventionally 2nd or 3rd degree polynomial would be enough
for(i in 1:length(cost_vals)){
#Train the SVM
svm2 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "polydot",
kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p3 <- kernlab::cross(svm2)
if (temp_p3<optimal_epe){
optimal_epe<-temp_p3
optimal_cost<-cost_vals[i]
optimal_degree<-j
}
}
}
# finalized model
FN_polySVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc",
kernel = "polydot", kpar = list(degree = optimal_degree), C = optimal_cost,
cross = 5, prob.model = TRUE)
FN_polySVM <- predict(FN_polySVM_classfier, newdata = p3_validFactored , type = "response")
FN_polySVM_oos<- mean(FN_polySVM != p3_validYfactored)
fourNine_result$polySVM <- FN_polySVM_oos
#radialSVM
# to find optimal cost
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
#Train the SVM
svm3 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "rbfdot",
C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p3 <- kernlab::cross(svm3)
if (temp_p3<optimal_epe){
optimal_epe<-temp_p3
optimal_cost<-cost_vals[i]
}
}
# finalized model
FN_radialSVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc",
kernel = "rbfdot", C = optimal_cost, cross = 5, prob.model = TRUE)
FN_radialSVM <- predict(FN_radialSVM_classfier, newdata = p3_validFactored , type = "response")
FN_radialSVM_oos<- mean(FN_radialSVM != p3_validYfactored)
fourNine_result$radialSVM <- FN_radialSVM_oos
#Bagging
FN_bagging_classifier <- ranger(p3_trainYfactored ~ . , data = p3_trainFactored, mtry =
ncol(p3_trainFactored) - 1, num.trees = 1000)
FN_bagging <- predict(FN_bagging_classifier, data = p3_validFactored)$predictions
FN_bagging_oos<- mean(FN_bagging != p3_validFactored$p3_validYfactored)
fourNine_result$Bagging <- FN_bagging_oos
FN_RF_classifier<-ranger(p3_trainYfactored ~ . , data = p3_trainFactored, num.trees = 1000,
importance = "permutation", mtry = 22, classification = TRUE)
FN_RF <- predict(FN_RF_classifier, data = p3_validFactored)$predictions
FN_RF_oos<- mean(FN_RF != p3_validFactored$p3_validYfactored)
fourNine_result$RF <- FN_RF_oos
optimal_varNum<-0
optimal_treeNum<-0
optimal_oob<-100000000
varNum <- seq(2,60,20) #too many var would be like bagging. but considering the high dimentionality of our x_train dataset, we decided to try a couple below 60.
treeNum <- c(1000,2000) #we wanted to tune treeNum too but don't want to spend too much time
for (i in 1:length(varNum)){
for (j in 1:2){
probTMod<-ranger(p3_trainYfactored ~ . , data = p3_trainFactored, num.trees = treeNum[j],
importance = "permutation", probability = TRUE, mtry = varNum[i], classification = TRUE)
temp_oob<-probTMod$prediction.error
if (temp_oob < optimal_oob){
optimal_oob<-temp_oob
optimal_varNum<-i
optimal_treeNum<-j
}
}
}
# finalized model
FN_probT_classifier<-ranger(p3_trainYfactored ~ . , data = p3_trainFactored,
num.trees = treeNum[optimal_treeNum], importance = "permutation",
probability = TRUE, mtry = varNum[optimal_varNum], classification = TRUE)
FN_probT <- predict(FN_probT_classifier, data = p3_validFactored)$predictions
FN_probTree <- c()
for (i in 1:3000){
if (FN_probT[i,1] > FN_probT[i,2]){
FN_probTree[i] = 4
}
else{
FN_probTree[i] = 9
}
}
FN_probTree <- factor(FN_probTree)
FN_probT_oos<- mean(FN_probTree != p3_validFactored$p3_validYfactored)
fourNine_result$ProbabilityTrees <- FN_probT_oos
#Data prep
p3_trainY_GBM <- p3p1_trainY
p3_train_GBM <- data.frame(p3_trainX, p3_trainY_GBM)
p3_validY_GBM <- p3p1_validY
p3_valid_GBM <- data.frame(p3_validX, p3_validY_GBM)
optimal_treeNum<-0
optimal_err<-100000000
treeNum <- seq(500, 3000, 500)
for (i in 1:length(treeNum)){
ADAm <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM),
distribution = "bernoulli", n.trees = treeNum[i], cv.folds = 5)
temp_err <- mean(round(predict(ADAm, data = p3_train_GBM, n.trees = 60, type = "response"))
!= p3_train_GBM$p3_trainY_GBM)
if (temp_err < optimal_err){
optimal_err<-temp_err
optimal_treeNum<-i
}
}
FN_ADA_classifier <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM),
distribution = "bernoulli", n.trees = treeNum[optimal_treeNum], cv.folds = 5)
FN_ADA_oos <- mean(round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response")) != p3_valid_GBM$p3_validY_GBM)
fourNine_result$ADA <- FN_ADA_oos
fourNine_result
#XGB
xgb_trainY <- p3_train
#XGB
xgb_train <- p3_train
xgb_valid <- p3_valid
xgb_valid$p3_validY <- NULL
xgb_trainX <- p3_trainX
xgboost1 <- xgboost(data = as.matrix(xgb_trainX), label = xgb_train$p3_trainY, max.depth = 3, eta = 1, nrounds = 10000, objective = "binary:logistic", verbose = 0)
xgboost1 <- xgboost(data = as.matrix(xgb_train), label = xgb_train$p3_trainY, max.depth = 3, eta = 1, nrounds = 10000, objective = "binary:logistic", verbose = 0)
xgb_train <- p3_train
xgb_train$p3_trainY <- NULL
xgb_trainY <- p3_trainY
xgboost1 <- xgboost(data = as.matrix(xgb_train), label = xgb_trainY, max.depth = 3, eta = 1, nrounds = 10000, objective = "binary:logistic", verbose = 0)
xgboost1 <- xgboost(data = as.matrix(xgb_trainX), label = xgb_trainY, max.depth = 3, eta = 1, nrounds = 10000, objective = "binary:logistic", verbose = 0)
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
library(mgcv)
library(gbm)
library(xgboost)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
xgboost1 <- xgboost(data = as.matrix(xgb_trainX), label = xgb_trainY, max.depth = 3, eta = 1, nrounds = 10000, objective = "binary:logistic", verbose = 0)
xgboost1 <- xgboost(data = as.matrix(xgb_train), label = xgb_trainY, max.depth = 3, eta = 1, nrounds = 10000, objective = "binary:logistic", verbose = 0)
xgb_trainY <- p3p1_trainY
xgb_trainY <- p3p1_trainY #coded in {0,1}
xgb_train <- data.frame(p3_trainX, xgb_trainY)
xgb_train$xgb_trainY <- NULL
xgb_validY <- p3p1_validY #coded in {0,1}
xgb_trainY <- p3p1_trainY #coded in {0,1}
xgb_train <- data.frame(p3_trainX, xgb_trainY)
xgb_train$xgb_trainY <- NULL
xgb_validY <- p3p1_validY #coded in {0,1}
xgb_valid <- data.frame(p3_validX, p3p1_validY)
xgb_valid$p3p1_validY <- NULL
xgboost1 <- xgboost(data = as.matrix(xgb_train), label = xgb_trainY, max.depth = 3, eta = 1, nrounds = 10000, objective = "binary:logistic", verbose = 0)
1- mean(round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response")) != p3_valid_GBM$p3_validY_GBM)
1- mean(round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response")) = p3_valid_GBM$p3_validY_GBM)
1- mean(round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response")) == p3_valid_GBM$p3_validY_GBM)
fourNine_result
xgb_err <- 1 - mean(round(predict(xgboost1, newdata = as.matrix(xgb_valid))) == xgb_validY)
xgb_err
depth <- seq(2,20,2)
learn_rate <- seq(0.1,1,0.1)
optimal_treeNum<-0
optimal_err<-100000000
treeNum <- seq(500, 3000, 500)
for (i in 1:length(treeNum)){
ADAm <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM),
distribution = "bernoulli", n.trees = treeNum[i], cv.folds = 5)
#we use validation data to calculate temp_err to prevent overfit
temp_err <- mean(round(predict(ADAm, data = p3_valid_GBM, n.trees = 60, type = "response"))
!= p3_train_GBM$p3_trainY_GBM)
if (temp_err < optimal_err){
optimal_err<-temp_err
optimal_treeNum<-i
}
}
FN_ADA_classifier <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM),
distribution = "bernoulli", n.trees = treeNum[optimal_treeNum], cv.folds = 5)
FN_ADA_oos <- mean(round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response")) != p3_valid_GBM$p3_validY_GBM)
fourNine_result$ADA <- FN_ADA_oos
fourNine_result
optimal_err
optimal_treeNum
treeNum[optimal_treeNum]
optimal_treeNum<-0
optimal_err<-100000000
treeNum <- seq(500, 3000, 500)
for (i in 1:length(treeNum)){
ADAm <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM),
distribution = "bernoulli", n.trees = treeNum[i], cv.folds = 5)
#we use validation data to calculate temp_err to prevent overfit
temp_err <- mean(round(predict(ADAm, newdata = p3_valid_GBM, n.trees = 60, type = "response"))
!= p3_valid_GBM$p3_validY_GBM)
if (temp_err < optimal_err){
optimal_err<-temp_err
optimal_treeNum<-i
}
}
FN_ADA_classifier <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM),
distribution = "bernoulli", n.trees = treeNum[optimal_treeNum], cv.folds = 5)
FN_ADA_oos <- mean(round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response")) != p3_valid_GBM$p3_validY_GBM)
fourNine_result$ADA <- FN_ADA_oos
fourNine_result
optimal_err
xgb_err
treeNum <- seq(50, 15000, 5000)
treeNum <- seq(50, 15000, 2000)
treeNum <- seq(50, 15000, 3000)
xgb_trainY <- p3p1_trainY #coded in {0,1}
xgb_train <- data.frame(p3_trainX, xgb_trainY)
xgb_train$xgb_trainY <- NULL
xgb_validY <- p3p1_validY #coded in {0,1}
xgb_valid <- data.frame(p3_validX, p3p1_validY)
xgb_valid$p3p1_validY <- NULL
depth <- seq(2,20,2)
learn_rate <- seq(0.1,1,0.1)
treeNum <- seq(50, 15000, 3000)
optimal_depth<-0
optimal_learn_rate<-0
optimal_treeNum<-0
optimal_err<-100000000
xgb_trainY <- p3p1_trainY #coded in {0,1}
xgb_train <- data.frame(p3_trainX, xgb_trainY)
xgb_train$xgb_trainY <- NULL
xgb_validY <- p3p1_validY #coded in {0,1}
xgb_valid <- data.frame(p3_validX, p3p1_validY)
xgb_valid$p3p1_validY <- NULL
depth <- seq(2,20,2)
learn_rate <- seq(0.1,1,0.1)
treeNum <- seq(50, 15000, 3000)
optimal_depth<-0
optimal_learn_rate<-0
optimal_treeNum<-0
optimal_err<-100000000
for (i in 1:length(depth)){
for (j in 1:length(learn_rate)){
for (k in 1:length(treeNum)){
xgbMod <- xgboost(data = as.matrix(xgb_train), label = xgb_trainY, max.depth = depth[i],
eta = learn_rate[j], nrounds = treeNum[k],
objective = "binary:logistic", verbose = 0)
#we use validation data to calculate temp_err to prevent overfit
temp_err <- mean(round(predict(xgbMod, newdata = as.matrix(xgb_valid))) != xgb_validY)
if (temp_err < optimal_err){
optimal_err<-temp_err
optimal_treeNum<-k
optimal_learn_rate<-j
optimal_depth<-i
}
}
}
}
depth[optimal_depth]
learn_rate[optimal_learn_rate]
treeNum[optimal_treeNum]
#finalized model
FN_xgb_classifier <- xgboost(data = as.matrix(xgb_train), label = xgb_trainY,
max.depth = depth[optimal_depth], eta = learn_rate[optimal_learn_rate],
nrounds = treeNum[optimal_treeNum],
objective = "binary:logistic", verbose = 0)
FN_xgb_oos <- mean(round(predict(xgbMod, newdata = as.matrix(xgb_valid))) != xgb_validY)
FN_xgb_oos
fourNine_result$xgb <- FN_xgb_oos
fourNine_result
temp_err
rf_MTRY <- varNum[optimal_varNum]
#Model Stacking (more than 2 classes would need H2)
#See all available methods with listWrappers()
#Split data into x_train and y_train
stack_trainY <- p3_trainY
stack_train <- p3_train
stack_train$p3_trainY <- NULL
stack_trainY <- p3p1_validY #recode to 0,1
stack_valid <- p3_valid
stack_valid$p3_validY <- NULL
#Let's run a model stack with 5 models: random forest, svm, logistic regression,
#qda, and mean (we just take the average)
#Set some tuning parameters
SL.ksvm.better <- function(...){
SL.ksvm(..., C = cost)
}
SL.ranger.better <- function(...){
SL.ranger(..., num.trees = 1000, mtry = rf_MTRY)
}
#Get 5 fold CV estimate of Risk from superlearner
stack <- SuperLearner(Y = stack_trainY, X = stack_train, family = binomial(), cvControl = list(V = 10),
SL.library = c("SL.ranger.better", "SL.ksvm.better","SL.glm","SL.qda","SL.mean"))
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
library(mgcv)
library(gbm)
library(xgboost)
library(SuperLearner)
install.packages('SuperLearner')
library(SuperLearner)
#Get 5 fold CV estimate of Risk from superlearner
stack <- SuperLearner(Y = stack_trainY, X = stack_train, family = binomial(), cvControl = list(V = 10),
SL.library = c("SL.ranger.better", "SL.ksvm.better","SL.glm","SL.qda","SL.mean"))
stack_trainX <- p3_trainX
#Get 5 fold CV estimate of Risk from superlearner
stack <- SuperLearner(Y = stack_trainY, X = stack_trainX, family = binomial(), cvControl = list(V = 10),
SL.library = c("SL.ranger.better", "SL.ksvm.better","SL.glm","SL.qda","SL.mean"))
fourNine_result
stack_trainY <- p3_trainYfactored
stack_train <- p3_trainFactored
stack_train$p3_trainY <- NULL
#Let's run a model stack with 5 models: random forest, svm, logistic regression,
#qda, and mean (we just take the average)
#Set some tuning parameters
SL.ksvm.better <- function(...){
SL.ksvm(... = , type = "C-svc",
kernel = "rbfdot", C = cost, cross = 5, prob.model = TRUE)
}
SL.ranger.better <- function(...){
SL.ranger(... = ,
num.trees = 1000, importance = "permutation",
mtry = rf_MTRY, classification = TRUE)
}
#Get 5 fold CV estimate of Risk from superlearner
stack <- SuperLearner(Y = stack_trainY, X = stack_train, family = binomial(), cvControl = list(V = 10),
SL.library = c("SL.ranger.better", "SL.ksvm.better","SL.glm","SL.qda","SL.mean"))
stack_trainY <- p3_trainY
stack_train <- p3_train
stack_train$p3_trainY <- NULL
#Get 5 fold CV estimate of Risk from superlearner
stack <- SuperLearner(Y = stack_trainY, X = stack_train, family = binomial(), cvControl = list(V = 10),
SL.library = c("SL.ranger.better", "SL.ksvm.better","SL.glm","SL.qda","SL.mean"))
fourNine_result
min(fourNine_result)
fourNine_result[which.min(fourNine_result)]
for (k in 1:length(FN_polySVM)) {
if (FN_polySVM[k] != p3_validYfactored[k]){
print(k) #misclassified:
}
}
p3_validYfactored[371,] #1
p3_validYfactored[371] #1
plot_digit(x = validation_x[371,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[1240] #4
p3_validYfactored[2025] #4
p3_validYfactored[2734] #4
plot_digit(x = validation_x[1240,], bw = FALSE, main = "True Class = 4")
plot_digit(x = validation_x[2025,], bw = FALSE, main = "True Class = 4")
plot_digit(x = validation_x[2734,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[371] #4
plot_digit(x = validation_x[371,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[1240] #4
plot_digit(x = validation_x[1240,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[2025] #9
plot_digit(x = validation_x[2025,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[2734] #9
plot_digit(x = validation_x[2734,], bw = FALSE, main = "True Class = 4")
plot_digit(x = p3_validX[371,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[371] #4
plot_digit(x = p3_validX[371,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[1240] #4
plot_digit(x = p3_validX[1240,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[2025] #9
plot_digit(x = p3_validX[2025,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[2734] #9
plot_digit(x = p3_validX[2734,], bw = FALSE, main = "True Class = 4")
plot_digit(x = p3_validX[370,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[371] #4
plot_digit(x = p3_validX[371,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[1240] #4
plot_digit(x = p3_validX[1240,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[2025] #9
plot_digit(x = p3_validX[2025,], bw = FALSE, main = "True Class = 4")
p3_validYfactored[2734] #9
plot_digit(x = p3_validX[2734,], bw = FALSE, main = "True Class = 4")
p3_results <- data.frame("digit" = test_y,
"p3_pred" = predict(FN_polySVM_classfier, data=test_x)$predictions)
p3_results <- data.frame("digit" = test_y,
"p3_pred" = predict(FN_polySVM_classfier, newdata=test_x)$predictions)
test <- predict(FN_polySVM_classfier, newdata=test_x)
p3_results <- data.frame("digit" = test_y,
"p3_pred" = as.numeric(predict(FN_polySVM_classfier, newdata=test_x)))
View(test_y)
test_y$label <- as.numeric(predict(FN_polySVM_classfier, newdata=test_x))
View(test_y)
test_y$label <- predict(FN_polySVM_classfier, newdata=test_x)
write.csv(as.matrix(test_y), file='Q3Predictions.csv', row.names=FALSE)
#Factored Data
train_yFactored <- factor(train_y)
train_factored <- data.frame(train_x, train_yFactored)
train_yFactored
View(train_y)
View(train_yFactored)
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
library(mgcv)
library(gbm)
library(xgboost)
library(SuperLearner)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
#Factored Data
train_yFactored <- factor(train_y)
View(train_yFactored)
train_yFactored
#Factored Data
train_yFactored <- factor(train_y, levels = 0:9)
train_yFactored
#Factored Data
train_yFactored <- factor(train_y$label)
train_yFactored
View(train_yFactored)
View(p3_trainY)
View(p3_trainYfactored)
train_factored <- data.frame(train_x, train_yFactored)
valid_yFactored <- factor(validation_y$label)
valid_yFactored <- factor(validation_y$label)
valid_factored <- data.frame(validation_x, valid_yFactored)
#MLR
multiLogistic_classifier<- multinom(train_yFactored ~ ., data=train_factored, trace=FALSE)
#MLR
multiLogistic_classifier<- multinom(train_yFactored ~ ., data=train_factored, trace=FALSE)
train_yFactored <- factor(train_y$label)
train_factored <- data.frame(train_x, train_yFactored)
valid_yFactored <- factor(validation_y$label)
valid_factored <- data.frame(validation_x, valid_yFactored)
#MLR
multiLogistic_classifier<- multinom(train_yFactored ~ ., data=train_factored, trace=FALSE)
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
library(mgcv)
library(gbm)
library(xgboost)
library(SuperLearner)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
#MLR
multiLogistic_classifier <- multinom(train_yFactored ~ ., data=train_factored, trace=FALSE)
View(train_factored)
#MLR
multiLogistic_classifier <- multinom(train_yFactored ~ ., train_factored, trace=FALSE)
train <- data.frame(train_x, train_y)
#Factored Data
train <- data.frame(train_x, train_y)
train_factored <- factor(train$label)
View(train_factored)
#Factored Data
train_factored <- data.frame(train_x, train_y)
train_factored$label <- factor(train_factored$label)
#MLR
multiLogistic_classifier <- multinom(label ~ ., data=train_factored, trace=FALSE)
