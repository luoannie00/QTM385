}
obs <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,12] > 100 && p4_trainMatrix[4,12] > 100 && p4_trainMatrix[8,12] > 100) {
obs <- append(obs, i)
}
}
#obs #2575, 5862
#plot_digit(x = p4_trainX[2575,], bw = FALSE)
#plot_digit(x = p4_trainX[5862,], bw = FALSE)
for (i in 1:length(obs)){
plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
}
obs <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,12] > 50 && p4_trainMatrix[4,12] > 50 && p4_trainMatrix[8,12] > 50) {
obs <- append(obs, i)
}
}
#obs #2575, 5862
#plot_digit(x = p4_trainX[2575,], bw = FALSE)
#plot_digit(x = p4_trainX[5862,], bw = FALSE)
for (i in 1:length(obs)){
plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
}
obs <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,12] > 20 && p4_trainMatrix[4,12] > 20 && p4_trainMatrix[8,12] > 20) {
obs <- append(obs, i)
}
}
#obs #2575, 5862
#plot_digit(x = p4_trainX[2575,], bw = FALSE)
#plot_digit(x = p4_trainX[5862,], bw = FALSE)
for (i in 1:length(obs)){
plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
}
obs <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,12] > 10 && p4_trainMatrix[4,12] > 10 && p4_trainMatrix[8,12] > 10) {
obs <- append(obs, i)
}
}
#obs #2575, 5862
#plot_digit(x = p4_trainX[2575,], bw = FALSE)
#plot_digit(x = p4_trainX[5862,], bw = FALSE)
for (i in 1:length(obs)){
plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
}
obs <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,12] > 5 && p4_trainMatrix[4,12] > 5 && p4_trainMatrix[8,12] > 5) {
obs <- append(obs, i)
}
}
#obs #2575, 5862
#plot_digit(x = p4_trainX[2575,], bw = FALSE)
#plot_digit(x = p4_trainX[5862,], bw = FALSE)
for (i in 1:length(obs)){
plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
}
obs <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,12] > 2 && p4_trainMatrix[4,12] > 2 && p4_trainMatrix[8,12] > 2) {
obs <- append(obs, i)
}
}
obs #2575, 5862
#plot_digit(x = p4_trainX[2575,], bw = FALSE)
#plot_digit(x = p4_trainX[5862,], bw = FALSE)
#for (i in 1:length(obs)){
#plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
#}
obs <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,12] > 3 && p4_trainMatrix[4,12] > 3 && p4_trainMatrix[8,12] > 3) {
obs <- append(obs, i)
}
}
obs #2575, 5862
#plot_digit(x = p4_trainX[2575,], bw = FALSE)
#plot_digit(x = p4_trainX[5862,], bw = FALSE)
#for (i in 1:length(obs)){
#plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
#}
obs <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,12] > 5 && p4_trainMatrix[4,12] > 5 && p4_trainMatrix[8,12] > 5) {
obs <- append(obs, i)
}
}
obs #2575, 5862
#plot_digit(x = p4_trainX[2575,], bw = FALSE)
#plot_digit(x = p4_trainX[5862,], bw = FALSE)
#for (i in 1:length(obs)){
#plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
#}
shoes <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,12] > 5 && p4_trainMatrix[4,12] > 5 && p4_trainMatrix[8,12] > 5) {
shoes <- append(shoes, i)
}
}
for (i in 1:length(shoes)){
plot_digit(x = p4_trainX[shoes[i],], bw = FALSE)
}
shoes <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[1,12] > 100 && p4_trainMatrix[12,1] > 100) {
shoes <- append(shoes, i)
}
}
shoes
for (i in 1:length(shoes)){
plot_digit(x = p4_trainX[shoes[i],], bw = FALSE)
}
obs <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[1,12] > 100 && p4_trainMatrix[12,1] > 100) {
obs <- append(obs, i)
}
}
for (i in 1:length(obs)){
plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
}
shoes <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,12] > 5 && p4_trainMatrix[4,12] > 5 && p4_trainMatrix[8,12] > 5) {
shoes <- append(shoes, i)
}
}
for (i in 1:length(shoes)){
plot_digit(x = p4_trainX[shoes[i],], bw = FALSE)
}
shoes <- append(1244, shoes)
obs <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,1] > 200 && p4_trainMatrix[4,12] > 200 && p4_trainMatrix[7,12] > 200) {
obs <- append(obs, i)
}
}
obs
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,1] > 150 && p4_trainMatrix[4,12] > 150 && p4_trainMatrix[7,12] > 150) {
obs <- append(obs, i)
}
}
obs
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,1] > 100 && p4_trainMatrix[4,12] > 100 && p4_trainMatrix[7,12] > 100) {
obs <- append(obs, i)
}
}
obs
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,1] > 50 && p4_trainMatrix[4,12] > 50 && p4_trainMatrix[7,12] > 50) {
obs <- append(obs, i)
}
}
obs
for (i in 1:length(obs)){
plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
}
obs
shoes <- append(2190, shoes)
for (i in 1:length(shoes)){
plot_digit(x = p4_trainX[shoes[i],], bw = FALSE)
}
obs <- c()
for (i in 1:7500){
p4_trainMatrix <- matrix(nrow = 12, ncol = 12, p4_trainX[i,], byrow = TRUE)
if (p4_trainMatrix[6,1] > 50 && p4_trainMatrix[4,12] > 50 && p4_trainMatrix[7,12] > 50) {
obs <- append(obs, i)
}
}
for (i in 1:length(obs)){
plot_digit(x = p4_trainX[obs[i],], bw = FALSE)
}
obs
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
library(mgcv)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
for (i in 1:length(shoes)){
plot_digit(x = p4_trainX[shoes[i],], bw = FALSE)
}
matrix(nrow = 12, ncol = 12, p3_corr, byrow = TRUE)
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
library(mgcv)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
#Bagging
num_Tree <- seq(500, 4500, 500)
optimal_num_Tree<-0
optimal_oob<-100000000
num_Tree <- seq(500, 4500, 500)
for (i in 1:length(num_Tree)){
bagMod <- ranger(p3_trainYfactored ~ . , data = p3_trainFactored,
mtry = ncol(p3_trainFactored) - 1, num.trees = num_Tree[i])
temp_oob<-bagMod$prediction.error
if (temp_oob < optimal_oob){
optimal_oob<-temp_oob
optimal_varNum<-i
}
}
FN_bagging_classifier <- ranger(p3_trainYfactored ~ . , data = p3_trainFactored, mtry =
ncol(p3_trainFactored) - 1, num.trees = 1000)
test_preds_class <- predict(FN_bagging_classifier, data = p3_validFactored)$predictions
rf_test_ms<- mean(test_preds_class != p3_validFactored$p3_validYfactored)
rf_test_ms
fourNine_result
varNum <- seq(2,120, 30)
varNum <- seq(2,120, 20)
varNum <- seq(2,100, 20) #
optimal_varNum<-0
optimal_oob<-100000000
varNum <- seq(2,120, 20)
treeNum <- seq(1000, 4000, 1000)
optimal_varNum<-0
optimal_treeNum<-0
optimal_oob<-100000000
varNum <- seq(2,120, 20) #too many var would be like bagging. but considering the high dimentionality of our x_train dataset, we decided to try a couple below 120.
treeNum <- seq(1000, 4000, 1000) #w
for (i in 1:length(varNum)){
for (j in 1:length(treeNum)){
rfMod<-ranger(p3_trainYfactored ~ . , data = p3_trainFactored, num.trees = treeNum[j],
importance = "permutation", mtry = varNum[i], classification = TRUE)
temp_oob<-rfMod$prediction.error
if (temp_oob < optimal_oob){
optimal_oob<-temp_oob
optimal_varNum<-i
optimal_treeNum<-j
}
}
}
optimal_varNum
optimal_treeNum
varNum[2]
varNum[optimal_varNum]
treeNum[optimal_treeNum]
# optimal_oob<-temp_oob
#optimal_varNum<-i
#optimal_treeNum<-j
#}
#}
#}
#varNum[optimal_varNum] #22
#treeNum[optimal_treeNum] #1000
#Since it's too computationally expensive to run the above for loop, we commented out the tuning process.
# finalized model
FN_RF_classifier<-ranger(mpg ~ ., data = car_train_factored, num.trees = 1000,
importance = "permutation", mtry = 22, classification = TRUE)
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
library(mgcv)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# optimal_oob<-temp_oob
#optimal_varNum<-i
#optimal_treeNum<-j
#}
#}
#}
#varNum[optimal_varNum] #22
#treeNum[optimal_treeNum] #1000
#Since it's too computationally expensive to run the above for loop, we commented out the tuning process.
# finalized model
FN_RF_classifier<-ranger(mpg ~ ., data = car_train_factored, num.trees = 1000,
importance = "permutation", mtry = 22, classification = TRUE)
# optimal_oob<-temp_oob
#optimal_varNum<-i
#optimal_treeNum<-j
#}
#}
#}
#varNum[optimal_varNum] #22
#treeNum[optimal_treeNum] #1000
#Since it's too computationally expensive to run the above for loop, we commented out the tuning process.
# finalized model
FN_RF_classifier<-ranger(p3_trainYfactored ~ . , data = p3_trainFactored, num.trees = 1000,
importance = "permutation", mtry = 22, classification = TRUE)
FN_RF <- predict(FN_RF_classifier, data = p3_validFactored)$predictions
FN_RF_oos<- mean(FN_RF != p3_validFactored$p3_validYfactored)
fourNine_result$RF <- FN_RF_oos
fourNine_result
optimal_varNum<-0
optimal_treeNum<-0
optimal_oob<-100000000
varNum <- seq(2,60,20) #too many var would be like bagging. but considering the high dimentionality of our x_train dataset, we decided to try a couple below 60.
treeNum <- c(1000,2000) #we wanted to tune treeNum too but don't want to spend too much time
for (i in 1:length(varNum)){
for (j in 1:2){
probTMod<-ranger(p3_trainYfactored ~ . , data = p3_trainFactored, num.trees = treeNum[j],
importance = "permutation", probability = TRUE, mtry = varNum[i], classification = TRUE)
temp_oob<-probTMod$prediction.error
if (temp_oob < optimal_oob){
optimal_oob<-temp_oob
optimal_varNum<-i
optimal_treeNum<-j
}
}
}
varNum[optimal_varNum]
treeNum[optimal_treeNum]
# finalized model
FN_probT_classifier<-ranger(p3_trainYfactored ~ . , data = p3_trainFactored,
num.trees = treeNum[optimal_treeNum], importance = "permutation",
probability = TRUE, mtry = varNum[optimal_varNum], classification = TRUE)
FN_probT <- predict(FN_probT_classifier, data = p3_validFactored)$predictions
FN_probT_oos<- mean(FN_probT != p3_validFactored$p3_validYfactored)
fourNine_result$ProbabilityTrees <- FN_probT_oos
fourNine_result
FN_probT
treeNum[optimal_treeNum]
varNum[optimal_varNum]
FN_probT
FN_probT <- predict(FN_probT_classifier, data = p3_validFactored)$class
FN_probT
FN_probT <- predict(FN_probT_classifier, data = p3_validFactored)$predictions
FN_probT
p3_validYfactored
for (i in 1:3000){
if (FN_probT[i,1] > FN_probT[i,2]){
FN_probT[i,3] = 4
}
else{
FN_probT[i,3] = 9
}
}
FN_probTree <- c()
for (i in 1:3000){
if (FN_probT[i,1] > FN_probT[i,2]){
FN_probTree[i] = 4
}
else{
FN_probTree[i] = 9
}
}
FN_probTree <- factor(FN_probTree)
FN_probT_oos<- mean(FN_probTree != p3_validFactored$p3_validYfactored)
fourNine_result$ProbabilityTrees <- FN_probT_oos
fourNine_result
fourNine_result$Bagging <- FN_bagging_oos
FN_bagging_classifier <- ranger(p3_trainYfactored ~ . , data = p3_trainFactored, mtry =
ncol(p3_trainFactored) - 1, num.trees = 1000)
FN_bagging <- predict(FN_bagging_classifier, data = p3_validFactored)$predictions
FN_bagging_oos<- mean(FN_bagging != p3_validFactored$p3_validYfactored)
fourNine_result$Bagging <- FN_bagging_oos
fourNine_result
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
library(mgcv)
library(gbm)
install.packages('gbm')
install.packages('xgboost')
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
library(mgcv)
library(gbm)
library(xgboost)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
#Boosted Trees
#GBM
FN_GBM_classifier <- gbm(p3_trainYfactored ~ . , data = as.data.frame(p3_trainFactored),
distribution = "bernoulli", n.trees = 500, cv.folds = 5)
#Boosted Trees
#GBM
#Data prep
p3_trainY_GBM <- train_y[10001:12500, 22501:25000),]
#Boosted Trees
#GBM
#Data prep
p3_trainY_GBM <- train_y[c(10001:12500, 22501:25000),]
p3_trainY_GBM <- train_y[c(10001:12500, 22501:25000),]
p3_trainY_GBM[10001:12500,] <- 0 #4->0
p3_trainY_GBM[1:2500,] <- 0 #4->0
p3_trainY_GBM[1:2500] <- 0 #4->0
p3_trainY_GBM[2501:5000] <- 1 #9->1
p3_validY_GBM <- validation_y[c(6001:7500, 13501:15000),]
p3_trainY_GBM[1:2500,] <- 0 #4->0
#Boosted Trees
#GBM
#Data prep
rm(p3_trainY_GBM)
p3_trainY_GBM <- p3p1_trainY
p3_validY_GBM <- p3p1_validY
rm(p3_validY_GBM)
p3_validY_GBM <- p3p1_validY
p3_train_GBM <- data.frame(p3_trainX, p3_trainY_GBM)
p3_valid_GBM <- data.frame(p3_validX, p3_validY_GBM)
#GBM
FN_GBM_classifier <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM),
distribution = "bernoulli", n.trees = 500, cv.folds = 5)
#ADA
FN_ADA_classifier <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM),
distribution = "bernoulli", n.trees = 500, cv.folds = 5)
FN_ADA_classifier$cv.fitted
FN_ADA_classifier$train.error
FN_ADA_classifier$cv.error
FN_ADA_classifier$fit
View(FN_ADA_classifier)
cross(FN_ADA_classifier)
FN_ADA_classifier[["fit"]]
tail(FN_ADA_classifier[["fit"]])
#ADA
treeNum <- seq(500, 3000, 500)
FN_ADA_classifier$prediction.error
mean(round(predict(FN_ADA_classifier, data = p3_train_GBM, n.trees = 60, type = "response")) != p3_train_GBM$p3_trainY_GBM)
treeErr <- seq(20, 100, 20)
p3_trainY_GBM <- p3p1_trainY
p3_train_GBM <- data.frame(p3_trainX, p3_trainY_GBM)
p3_validY_GBM <- p3p1_validY
p3_valid_GBM <- data.frame(p3_validX, p3_validY_GBM)
#ADA
optimal_treeNum<-0
optimal_err<-100000000
treeNum <- seq(500, 3000, 500)
for (i in 1:length(treeNum)){
ADAm <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM),
distribution = "bernoulli", n.trees = treeNum[i], cv.folds = 5)
temp_err <- mean(round(predict(ADAm, data = p3_train_GBM, n.trees = 60, type = "response"))
!= p3_train_GBM$p3_trainY_GBM)
if (temp_err < optimal_err){
optimal_err<-temp_err
optimal_treeNum<-i
}
}
FN_ADA_classifier <- gbm(p3_trainY_GBM ~ . , data = as.data.frame(p3_train_GBM),
distribution = "bernoulli", n.trees = treeNum[optimal_treeNum], cv.folds = 5)
FN_ADA <- mean(round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60, type = "response"))
!= p3_valid_GBM$p3_validY_GBM)
FN_ADA_oos <- mean(round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response")) != p3_valid_GBM$p3_validY_GBM)
View(FN_ADA_classifier)
treeNum[optimal_treeNum]
FN_ADA_oos <- mean(round(predict(FN_GBM_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response")) != p3_valid_GBM$p3_validY_GBM)
FN_ADA_oos <- mean(round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response")) != p3_valid_GBM$p3_validY_GBM)
predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response")
round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response")
round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
round(predict(FN_ADA_classifier, newdata = p3_valid_GBM, n.trees = 60,
type = "response"))
fourNine_result$ADA <- FN_ADA_oos
fourNine_result
