if (logistic_cv_pred[k] != p2_train[-folds[[i]],]$p2_trainY[k]){
countMs = countMs + 1
}
}
mis_rate[i] <- countMs / length(logistic_cv_pred)
}
p2p1_epe<-mean(mis_rate)
p2p1_epe #0.0036
#oos
countMs = 0
logistic_valid <- predict(p2p1_logistic, newdata = p2_validX, type = "response")
#classify predictions
for (j in 1:length(logistic_valid)){
if (logistic_valid[j] > 0.5) {
logistic_valid[j] <- 1
}
else {
logistic_valid[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_valid)) {
if (logistic_valid[k] != p2_validY[k]){
print(k) #misclassified: 271, 1057, 2203, 2259, 2311, 2555
countMs = countMs + 1
}
}
p2p1_oos <- countMs / length(logistic_valid)
p2p1_oos #0.002
zeroOne_result <- matrix(nrow = 1, ncol = 2)
zeroOne_result <- data.frame("In Sample" = p2p1_epe, "OOS" = p2p1_oos)
View(zeroOne_result)
p3_trainX <- train_x[c(10001:12500, 22501:25000),]
p3_trainY <- train_y[c(10001:12500, 22501:25000),]
p3_validX <- validation_x[c(6001:7500, 13501:15000),]
p3_validY <- validation_y[c(6001:7500, 13501:15000),]
row.names(fourNine_result) <- "Logistics"
View(fourNine_result)
fourNine_result <- data.frame("In Sample" = p3p1_epe, "OOS" = p3p1_oos)
#recode for logistic regression
length(p3_trainY)
p3p1_trainY <- matrix(nrow = 5000, ncol = 1)
p3p1_trainY[1:2500,] = 0 #4 -> 0
p3p1_trainY[2501:5000,] = 1 #9 -> 1
length(p3_validY)
p3p1_validY <- matrix(nrow = 3000, ncol = 1)
p3p1_validY[1:1500,] = 0 #4 -> 0
p3p1_validY[1501:3000,] = 1 #9 -> 1
#logistic regression model
p3_logTrain <- data.frame(p3_trainX, p3p1_trainY)
p3p1_logistic <- glm(data=p3_logTrain,
formula = p3p1_trainY ~ .,
family = binomial(link='logit'),
control = list(maxit = 100))
#in-sample EPE
folds <- create_folds(seq(1,nrow(p3_logTrain)), k = 10, type = "basic")
mis_rate <- c()
for(i in 1:length(folds)){
countMs = 0
logistic_cv <- glm(p3p1_trainY ~ ., data=p3_logTrain[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p3_logTrain[-folds[[i]],], type = "response")
#classify predictions
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_cv_pred)) {
if (logistic_cv_pred[k] != p3_logTrain[-folds[[i]],]$p3p1_trainY[k]){
countMs = countMs + 1
}
}
mis_rate[i] <- countMs / length(logistic_cv_pred)
}
p3p1_epe<-mean(mis_rate)
p3p1_epe #0.0508
#oos
countMs = 0
logistic_valid <- predict(p3p1_logistic, newdata = p3_validX, type = "response")
#classify predictions
for (j in 1:length(logistic_valid)){
if (logistic_valid[j] > 0.5) {
logistic_valid[j] <- 1
}
else {
logistic_valid[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_valid)) {
if (logistic_valid[k] != p3p1_validY[k]){
countMs = countMs + 1
}
}
p3p1_oos <- countMs / length(logistic_valid)
p3p1_oos #0.035
fourNine_result <- data.frame("In Sample" = p3p1_epe, "OOS" = p3p1_oos)
row.names(fourNine_result) <- "Logistics"
View(fourNine_result)
p3_train <- data.frame(p3_trainX, p3_trainY)
# QDA
FN_QDA_classifier<-qda(p3_trainY ~ ., data=p3_train)
# Naive Bayes classifier: Gaussian naive bayes
# we won't apply KDE naive bayes here since the sample size is relatively small
naiveBayes_x<-p3_trainX
naiveBayes_y<-as.character(p3_train$p3_trainY)
# Naive Bayes classifier: Gaussian naive bayes
FN_naiveBayes<-naive_bayes(as.character(p3_train$p3_trainY) ~ p3_trainX, data=wine_train,usekernel=FALSE)
# Naive Bayes classifier: KDE naive bayes
#we use KDE since data was high dimensional and has a lot of obs.
FN_naiveBayes<-naive_bayes(as.character(p3_train$p3_trainY) ~ p3_trainX, data=p3_train,usekernel=TRUE)
# Naive Bayes classifier: KDE naive bayes
#we use KDE since data was high dimensional and has a lot of obs.
FN_naiveBayes<-naive_bayes(as.character(p3_train$p3_trainY) ~ ., data=p3_train,usekernel=TRUE)
View(FN_naiveBayes)
# Naive Bayes classifier: KDE naive bayes
#we use KDE since data was high dimensional and has a lot of obs.
FN_naiveBayes_classifier<-naive_bayes(as.character(p3_train$p3_trainY) ~ ., data=p3_train,usekernel=TRUE)
p3_valid <- data.frame(p3_validX, p3_validY)
FN_QDA <- predict(FN_QDA_classifier,p3_valid)$p3_validY
FN_QDA <- predict(FN_QDA_classifier,p3_valid)
View(FN_QDA)
FN_QDA <- predict(FN_QDA_classifier,p3_valid)$class
View(FN_QDA)
FN_naiveBayes <- predict(FN_naiveBayes_classifier, p3_valid, type= "class")
View(FN_naiveBayes_classifier)
p3_valid_NB <- data.frame(p3_validX, as.character(p3_validY))
FN_naiveBayes <- predict(FN_naiveBayes_classifier, p3_valid_NB, type= "class")
FN_naiveBayes <- predict(FN_naiveBayes_classifier, p3_valid, type= "class")
View(FN_naiveBayes)
length(FN_naiveBayes)
length(FN_QDA)
FN_QDA_oos <- mean(FN_QDA != p3_valid$p3_validY)
FN_QDA_oos #0.035
fourNine_result <- data.frame("Logistics" = p3p1_oos)
fourNine_result
fourNine_result <- append(FN_QDA_oos)
fourNine_result <- append("QDA" = FN_QDA_oos)
fourNine_result$QDA <- FN_QDA_oos
fourNine_result
FN_naiveBayes_oos <- mean(FN_naiveBayes != p3_valid$p3_validY)
FN_naiveBayes_oos #0.06533333
fourNine_result$NaiveBayes <- FN_naiveBayes_oos
fourNine_result
fourNine_result
fourNine_result <- data.frame("Logistic" = p3p1_oos)
fourNine_result
fourNine_result$QDA <- FN_QDA_oos
fourNine_result$NaiveBayes <- FN_naiveBayes_oos
fourNine_result
FN_lasso <- predict(p3_lasso,p3_valid)
p3_lTrainX <- data.matrix(p3_trainX)
p3_lTrainY <- data.matrix(p3_trainY)
#scientific lambda
p3_cv_lasso <- cv.glmnet(x=p3_lTrainX, y=p3_lTrainY, family="binomial",
control = list(maxit = 100), alpha=1, nfolds=10,
type.measure="mse")
#our chosen value of lambda is
p3_cv_lasso$lambda.min
p3_lasso <- glmnet(x=p3_lTrainX, y=p3_lTrainY, family="binomial", alpha=1,
lambda=p3_cv_lasso$lambda.min)
p3_lasso$beta #coefficients
FN_lasso <- predict(p3_lasso,p3_valid)
p3_lvalidY <- data.matrix(p3_validY)
p3_lvalidX <- data.matrix(p3_validX)
FN_lasso <- predict(p3_lasso, p3_lvalidX)
View(FN_lasso)
p3_trainX <- train_x[c(10001:12500, 22501:25000),]
p3_trainY <- train_y[c(10001:12500, 22501:25000),]
p3_validX <- validation_x[c(6001:7500, 13501:15000),]
p3_validY <- validation_y[c(6001:7500, 13501:15000),]
FN_QDA
FN_lasso <- predict(p3_lasso,family="binomial", p3_lvalidX)
FN_lasso
p3_lvalidX <- data.matrix(p3_validY)
p3_lvalidY <- data.matrix(p3_validY)
p3_lvalidX <- data.matrix(p3_validX)
FN_lasso <- predict(p3_lasso, p3_lvalidX, type = "response")
FN_lasso
FN_lasso <- predict(p3_lasso, newdata = p3_lvalidX, type = "response")
#p3_lvalidX <- data.matrix(p3_validX)
FN_lasso <- predict(p3_lasso, newdata = p3_validX, type = "response")
p3_lasso
p3_valid <- data.frame(p3_validX, p3_validY)
p3_lvalid <- p3_valid.matrix(p3_validY ~., p3_valid)[,-1]
p3_lvalid <- matrix(p3_validY ~., p3_valid)[,-1]
FN_lasso <- p3_lasso %>% predict(newx = p3_validX)
FN_lasso <- predict(p3_lasso, newx = p3_validX)
FN_lasso <- predict(p3_lasso, newdata = p3_validX)
View(test_x)
FN_lasso <- predict(p3_lasso, p3_validX)
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
FN_lasso <- predict(p3_lasso, p3_validX)
p3_lvalidX <- as.matrix(p3_validX)
FN_lasso <- predict(p3_lasso, p3_lvalidX)
View(FN_lasso)
FN_QDA_oos <- mean(FN_QDA != p3_valid$p3_validY, type = "response")
FN_QDA_oos <- mean(FN_QDA != p3_valid$p3_validY, type = "class")
FN_lasso <- predict(p3_lasso, p3_lvalidX, type = "class")
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# data read-in
train_x <- read.csv('MNISTTrainXV2.csv')
train_y <- read.csv('MNISTTrainY.csv')
test_x <- read.csv('MNISTTestXRand.csv')
test_y <- read.csv("MNISTTestYRand.csv")
validation_x <- read.csv('MNISTValidationX.csv')
validation_y <- read.csv('MNISTValidationY.csv')
# plot function
plot_digit <- function(x, bw = FALSE,...){
if(sqrt(length(x)) != round(sqrt(length(x)))){
stop(print("Not a square image! Something is wrong here."))
}
n <- sqrt(length(x))
if(bw == TRUE){
x <- as.numeric(x > 50)*256
}
par(pty = "s")
image(matrix(as.matrix(x), nrow = n)[,n:1], col = gray(12:1 / 12), ...)
}
#Example
plot_digit(x = train_x[1,], bw = FALSE, main = "True Class = 0") #x vector length = square of integer
plot_digit(x = train_x[1,], bw = TRUE, main = "True Class = 0")
p3_trainX <- train_x[c(10001:12500, 22501:25000),]
p3_trainY <- train_y[c(10001:12500, 22501:25000),]
p3_validX <- validation_x[c(6001:7500, 13501:15000),]
p3_validY <- validation_y[c(6001:7500, 13501:15000),]
#recode for logistic regression
length(p3_trainY)
p3p1_trainY <- matrix(nrow = 5000, ncol = 1)
p3p1_trainY[1:2500,] = 0 #4 -> 0
p3p1_trainY[2501:5000,] = 1 #9 -> 1
length(p3_validY)
p3p1_validY <- matrix(nrow = 3000, ncol = 1)
p3p1_validY[1:1500,] = 0 #4 -> 0
p3p1_validY[1501:3000,] = 1 #9 -> 1
#logistic regression model
p3_logTrain <- data.frame(p3_trainX, p3p1_trainY)
p3p1_logistic <- glm(data=p3_logTrain,
formula = p3p1_trainY ~ .,
family = binomial(link='logit'),
control = list(maxit = 100))
#in-sample EPE
folds <- create_folds(seq(1,nrow(p3_logTrain)), k = 10, type = "basic")
mis_rate <- c()
for(i in 1:length(folds)){
countMs = 0
logistic_cv <- glm(p3p1_trainY ~ ., data=p3_logTrain[folds[[i]],], trace=FALSE)
logistic_cv_pred <- predict(logistic_cv, newdata = p3_logTrain[-folds[[i]],], type = "response")
#classify predictions
for (j in 1:length(logistic_cv_pred)){
if (logistic_cv_pred[j] > 0.5) {
logistic_cv_pred[j] <- 1
}
else {
logistic_cv_pred[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_cv_pred)) {
if (logistic_cv_pred[k] != p3_logTrain[-folds[[i]],]$p3p1_trainY[k]){
countMs = countMs + 1
}
}
mis_rate[i] <- countMs / length(logistic_cv_pred)
}
p3p1_epe<-mean(mis_rate)
p3p1_epe #approx. 0.0508; numerically unstable
#oos
countMs = 0
logistic_valid <- predict(p3p1_logistic, newdata = p3_validX, type = "response")
#classify predictions
for (j in 1:length(logistic_valid)){
if (logistic_valid[j] > 0.5) {
logistic_valid[j] <- 1
}
else {
logistic_valid[j] <- 0
}
}
#Count of Misclassifications
for (k in 1:length(logistic_valid)) {
if (logistic_valid[k] != p3p1_validY[k]){
countMs = countMs + 1
}
}
p3p1_oos <- countMs / length(logistic_valid)
p3p1_oos #0.035
fourNine_result <- data.frame("Logistic" = p3p1_oos)
FN_lasso_oos <- mean(FN_lasso != p3_valid$p3_validY, type = "class")
p3_lTrainX <- data.matrix(p3_trainX)
p3_lTrainY <- data.matrix(p3_trainY)
#scientific lambda
p3_cv_lasso <- cv.glmnet(x=p3_lTrainX, y=p3_lTrainY, family="binomial",
control = list(maxit = 100), alpha=1, nfolds=10,
type.measure="mse")
p3_lasso <- glmnet(x=p3_lTrainX, y=p3_lTrainY, family="binomial", alpha=1,
lambda=p3_cv_lasso$lambda.min)
p3_lvalidX <- as.matrix(p3_validX)
FN_lasso <- predict(p3_lasso, p3_lvalidX, type = "class")
FN_lasso_oos <- mean(FN_lasso != p3_valid$p3_validY)
View(FN_lasso)
FN_lasso <- predict(p3_lasso, newdata = p3_lvalidX, type = "class")
FN_lasso <- predict(p3_lasso, p3_lvalidX, type = "class")
FN_lasso_oos <- mean(FN_lasso != p3_validY)
FN_QDA_oos #0.06533333
FN_lasso_oos #0.06533333
#LASSO
p3_lTrainX <- data.matrix(p3_trainX)
p3_lTrainY <- data.matrix(p3_trainY)
#scientific lambda
p3_cv_lasso <- cv.glmnet(x=p3_lTrainX, y=p3_lTrainY, family="binomial",
control = list(maxit = 100), alpha=1, nfolds=10,
type.measure="mse")
#our chosen value of lambda is
p3_cv_lasso$lambda.min
p3_lasso <- glmnet(x=p3_lTrainX, y=p3_lTrainY, family="binomial", alpha=1,
lambda=p3_cv_lasso$lambda.min)
p3_lasso$beta #coefficients
#correlations
p3_corr <- c()
for (i in 1:length(p3_lasso$beta)){
if (p3_lasso$beta[i] > 0) {
p3_corr[i] = 1
}
else if (p3_lasso$beta[i] < 0) {
p3_corr[i] = -1
}
else {
p3_corr[i] = 0
}
}
cf1 <- colnames(p3_trainX)
names(p3_corr) <- cf1
matrix(nrow = 12, ncol = 12, p3_corr, byrow = TRUE)
p3_train <- data.frame(p3_trainX, p3_trainY)
p3_valid <- data.frame(p3_validX, p3_validY)
# QDA
FN_QDA_classifier <- qda(p3_trainY ~ ., data=p3_train)
FN_QDA <- predict(FN_QDA_classifier,p3_valid)$class
FN_QDA_oos <- mean(FN_QDA != p3_valid$p3_validY)
FN_QDA_oos #0.06533333
fourNine_result$QDA <- FN_QDA_oos
# Naive Bayes classifier: KDE naive bayes
#we use KDE since data was high dimensional and has a lot of obs.
FN_naiveBayes_classifier<-naive_bayes(as.character(p3_train$p3_trainY) ~ ., data=p3_train,usekernel=TRUE)
FN_naiveBayes <- predict(FN_naiveBayes_classifier, p3_valid, type= "class")
FN_naiveBayes_oos <- mean(FN_naiveBayes != p3_valid$p3_validY)
FN_naiveBayes_oos #0.16
fourNine_result$NaiveBayes <- FN_naiveBayes_oos
fourNine_result
#lasso
p3_lvalidX <- as.matrix(p3_validX)
FN_lasso <- predict(p3_lasso, p3_lvalidX, type= "class")
FN_lasso_oos <- mean(FN_lasso != p3_validY)
FN_lasso_oos #0.16
fourNine_result$lasso <- FN_lasso_oos
fourNine_result
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
#Create a vector of potential C values. We tried cost from -50 to 50 since we don't want cost to be too high that the model overfits.
cost_vals <- exp(seq(-50,50,length = 500))
#SVM data prep
p3_trainYfactored <- factor(p3_trainY)
p3_trainFactored <- data.frame(p3_trainX, p3_trainYfactored)
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "vanilladot",
C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
#Extract the 5fold CV est and store
temp_p3 <- kernlab::cross(svm1)
if (temp_p3<optimal_epe){
optimal_epe<-temp_p3
optimal_cost<-cost_vals[i]
}
}
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
library(glmnet)
library(naivebayes)
library(mgcv)
#rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "vanilladot",
C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
#Extract the 5fold CV est and store
temp_p3 <- kernlab::cross(svm1)
if (temp_p3<optimal_epe){
optimal_epe<-temp_p3
optimal_cost<-cost_vals[i]
}
}
cost_vals
#Create a vector of potential C values. We tried cost from -50 to 50 since we don't want cost to be too high that the model overfits.
cost_vals <- exp(seq(-50,50,length = 50))
cost_vals
#Create a vector of potential C values. We tried cost from -50 to 50 since we don't want cost to be too high that the model overfits.
cost_vals <- exp(seq(-3,3,length = 30))
cost_vals
#Create a vector of potential C values. We set our cost in a way that the cost we try won't be too high that the model overfits.
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "vanilladot",
C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
#Extract the 5fold CV est and store
temp_p3 <- kernlab::cross(svm1)
if (temp_p3<optimal_epe){
optimal_epe<-temp_p3
optimal_cost<-cost_vals[i]
}
}
optimal_cost
# finalized model
FN_linearSVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "vanilladot",
C = optimal_cost, cross = 5, prob.model = TRUE, kpar = list())
FN_linearSVM <- predict(FN_linearSVM_classfier, newdata = p3_validFactored , type = "response")
p3_validYfactored <- factor(p3_validY)
p3_validFactored <- data.frame(p3_validX, p3_validYfactored)
FN_linearSVM <- predict(FN_linearSVM_classfier, newdata = p3_validFactored , type = "response")
FN_linearSVM_oos<- mean(FN_linearSVM != p3_validYfactored)
fourNine_result$linearSVM <- FN_linearSVM_oos
fourNine_result
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for (j in 2:3){ #since usually 2nd or 3rd degree would be enough
for(i in 1:length(cost_vals)){
#Train the SVM
svm2 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "polydot",
kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p3 <- kernlab::cross(svm2)
if (temp_p3<optimal_epe){
optimal_epe<-temp_p3
optimal_cost<-cost_vals[i]
optimal_degree<-j
}
}
}
# finalized model
FN_polySVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc",
kernel = "polydot", kpar = list(degree = optimal_degree), C = optimal_cost,
cross = 5, prob.model = TRUE)
FN_polySVM <- predict(FN_polySVM_classfier, newdata = p3_validFactored , type = "response")
FN_polySVM_oos<- mean(FN_polySVM != p3_validYfactored)
fourNine_result$polySVM <- FN_polySVM_oos
fourNine_result
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
#Train the SVM
svm3 <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc", kernel = "rbfdot",
C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p3 <- kernlab::cross(svm3)
if (temp_p3<optimal_epe){
optimal_epe<-temp_p3
optimal_cost<-cost_vals[i]
}
}
# finalized model
FN_radialSVM_classfier <- ksvm(p3_trainYfactored ~ . , data = p3_trainFactored, type = "C-svc",
kernel = "rbfdot", C = optimal_cost, cross = 5, prob.model = TRUE)
FN_radialSVM <- predict(FN_radialSVM_classfier, newdata = p3_validFactored , type = "response")
FN_radialSVM_oos<- mean(FN_radialSVM != p3_validYfactored)
fourNine_result$radialSVM <- FN_radialSVM_oos
fourNine_result
