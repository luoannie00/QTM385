library(ggplot2)
library(data.table)
library(tidyverse)
library(glmnet)
library(MASS)
library(splitTools)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
cancer_train <- read.csv('cancerTrain.csv')
cancer_test <- read.csv('cancerTest.csv')
wine_train <- read.csv('wineTrain.csv')
p2p1_df1 <- data.frame(matrix(nrow = 10, ncol = 10))
malignant_df <- cancer_train %>% filter(Malignant == 1)
malignant_prob <- nrow(malignant_df)/500
non_malignant_df <- cancer_train %>% filter(Malignant == 0)
non_malignant_prob <- nrow(non_malignant_df)/500
for (i in 1:10) {
for (j in 1:10) {
temp_df1 <- malignant_df %>% filter(UCellSize == i & SECellSize == j)
temp_df2 <- non_malignant_df %>% filter(UCellSize == i & SECellSize == j)
if (nrow(temp_df1) == 0) {
p2p1_df1[i,j] <- NA
next
}
result_numerator <- nrow(temp_df1)/nrow(malignant_df) * malignant_prob
result_denominator <- result_numerator + nrow(temp_df2)/nrow(non_malignant_df) * non_malignant_prob
p2p1_df1[i,j] <- result_numerator/result_denominator
}
}
p2p1_df1
p2p1_df2 <- data.frame(matrix(nrow = 10, ncol = 10))
for (i in 1:10) {
for (j in 1:10) {
if (is.na(p2p1_df1[i,j])) {
next
}
else if (p2p1_df1[i,j] > 0.5) {
p2p1_df2[i,j] <- 1
}
else if (p2p1_df1[i,j] < 0.5) {
p2p1_df2[i,j] <- 0
}
#how to deal with it???
else if (p2p1_df1[i,j] == 0.5) {
p2p1_df2[i,j] <- 1
}
}
}
p2p1_df2
cancer_train <- read.csv('cancerTrain.csv')
cancer_test <- read.csv('cancerTest.csv')
wine_train <- read.csv('wineTrain.csv')
wine_train
wine_train
ggplot(data=wine_train,aes(x=Color, y= OD280))+
geom_point()
wine_train
ggplot(data=wine_train,aes(x=Color, y= OD280),colors=Class)+
geom_point()
wine_train
ggplot(data=wine_train,aes(x=Color, y= OD280))+
geom_point(aes(colors=Class))
wine_train
ggplot(data=wine_train,aes(x=Color, y= OD280))+
geom_point(colors=Class)
wine_train
ggplot(data=wine_train,aes(x=Color, y= OD280,color=Class))+
geom_point()
wine_train
ggplot(data=wine_train,aes(x=Color, y= OD280,color=factor(Class)))+
geom_point()
library(naivebayes)
install.packages("naivebayes")
# add
library(naivebayes)
library(nnet)
#LDA
LDA_classifier<-lda(Class ~ Color + OD280, data=wine_train)
LDA_classifier
# QDA
QDA_classifier<-qda(Class ~ Color + OD280, data=wine_train)
wine_train
# multinomial logistic regression classifier
wine_train_factored<-wine_train
wine_train_factored$Class<-factor(wine_train_factored$Class)
multiLogistic_classifier<- multinom(Class ~ Color + OD280, data=wine_train_factored)
# LDA classifier
LDA_classifier<-lda(Class ~ Color + OD280, data=wine_train)
# QDA classifier
QDA_classifier<-qda(Class ~ Color + OD280, data=wine_train)
# Naive Bayes classifier: Gaussian naive bayes
# we won't apply KDE naive bayes here since the sample size is relatively small
naiveBayes_x<-data.frame("color"=wine_train$Color,"OD280"=wine_train $OD280)
naiveBayes_y<-as.character(wine_train$Class)
naiveBayes_classifier<-naive_bayes(x=naiveBayes_x,y=naiveBayes_y,usekernel=FALSE)
# multinomial logistic regression classifier
wine_train_factored<-wine_train
wine_train_factored$Class<-factor(wine_train_factored$Class)
multiLogistic_classifier<- multinom(Class ~ Color + OD280, data=wine_train_factored)
# LDA classifier
LDA_classifier<-lda(Class ~ Color + OD280, data=wine_train)
# QDA classifier
QDA_classifier<-qda(Class ~ Color + OD280, data=wine_train)
# Naive Bayes classifier: Gaussian naive bayes
# we won't apply KDE naive bayes here since the sample size is relatively small
naiveBayes_x<-data.frame("color"=wine_train$Color,"OD280"=wine_train $OD280)
naiveBayes_y<-as.character(wine_train$Class)
naiveBayes_classifier<-naive_bayes(x=naiveBayes_x,y=naiveBayes_y,usekernel=FALSE)
predict(multiLogistic_classifier, x=wine_train_factored)
multiLogistic_pred<-data.frame("color"=wine_train_factored$Color,
"OD280"=wine_train_factored$OD280,
"realClass"=wine_train_factored$Class,
"predClass"=predict(multiLogistic_classifier, x=wine_train_factored))
multiLogistic_pred
