#delete variable episode_name
o_train <- o_train %>% select(-episode_name)
library(tidyverse)
library(glmnet)
library(caret)
library(Metrics)
library(splines)
library(mgcv)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
setwd('/Users/billyge/Desktop/Emory Fall 2022/QTM 385-4/problem sets/ps github/ps3')
library(tidyverse)
library(glmnet)
library(caret)
library(Metrics)
library(splines)
library(mgcv)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
setwd('/Users/billyge/Desktop/Emory Fall 2022/QTM 385-4/problem sets/ps github/ps3')
library(tidyverse)
library(glmnet)
library(caret)
library(Metrics)
library(splines)
library(mgcv)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
#setwd('/Users/billyge/Desktop/Emory Fall 2022/QTM 385-4/problem sets/ps github/ps3')
setwd('/Users/annie/Desktop/QTM385/QTM350_Github/QTM350 Github/QTM385/ps3')
rm(list = ls())
c_test <- read.csv('college_test.csv')
c_train <- read.csv('college_train.csv')
o_test <- read.csv('office_test.csv')
o_train <- read.csv('office_train.csv')
#delete variable episode_name
o_train <- o_train %>% select(-episode_name)
#split variable season into 9 variables
dummy_o_train <- caret::dummyVars("~.", data=o_train)
o_train <- data.frame(predict(dummy_o_train, newdata=o_train))
#full model
p2p1_m <- lm(imdb_rating ~ ., data=o_train)
#LOOCV
lm_pred_metrics <- function(fit_model) {
LOOCV <- mean((fit_model$residuals / (1-hatvalues(fit_model)))^2)
return (LOOCV)
}
lm_pred_metrics(p2p1_m) #LOOCV is 0.2343854
#plot
cf <- coef(summary(p2p1_m, complete=TRUE))
cf <- data.frame(rownames(cf), cf)
ggplot(data=cf, aes(x=rownames.cf., y=Estimate)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
fullOLS <- ggplot(data=cf, aes(x=rownames.cf., y=Estimate)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
fullOLS
library(tidyverse)
library(glmnet)
library(caret)
library(Metrics)
library(splines)
library(mgcv)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
#setwd('/Users/billyge/Desktop/Emory Fall 2022/QTM 385-4/problem sets/ps github/ps3')
setwd('/Users/annie/Desktop/QTM385/QTM350_Github/QTM350 Github/QTM385/ps3')
rm(list = ls())
c_test <- read.csv('college_test.csv')
c_train <- read.csv('college_train.csv')
o_test <- read.csv('office_test.csv')
o_train <- read.csv('office_train.csv')
#delete variable episode_name
o_train <- o_train %>% select(-episode_name)
#split variable season into 9 variables
dummy_o_train <- caret::dummyVars("~.", data=o_train)
o_train <- data.frame(predict(dummy_o_train, newdata=o_train))
#full model
p2p1_m <- lm(imdb_rating ~ ., data=o_train)
#LOOCV
lm_pred_metrics <- function(fit_model) {
LOOCV <- mean((fit_model$residuals / (1-hatvalues(fit_model)))^2)
return (LOOCV)
}
lm_pred_metrics(p2p1_m) #LOOCV is 0.2343854
#plot
cf <- coef(summary(p2p1_m, complete=TRUE))
cf <- data.frame(rownames(cf), cf)
fullOLS <- ggplot(data=cf, aes(x=rownames.cf., y=Estimate)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
fullOLS
#subset of important predictors
p2p1_imp_pred <- o_train %>% select(seasonSeason.3, seasonSeason.1, paul_lieberstein, mindy_kaling, justin_spitzer, greg_daniels, b_j_novak, imdb_rating)
p2p2_pred <- data.matrix(o_train[,-38])
p2p2_out <- data.matrix(o_train$imdb_rating)
#100 possible values of lambda
p2p2_ridgem <- glmnet(x=p2p2_pred, y=p2p2_out, family="gaussian", alpha=0, nlambda=100)
print(p2p2_ridgem)
p2p2_cv_ridgem <- cv.glmnet(x=p2p2_pred, y=p2p2_out, family="gaussian", alpha=0, nfolds=10, type.measure="mse")
plot(p2p2_cv_ridgem)
print(p2p2_cv_ridgem)
p2p2_optimal_ridgem <- glmnet::glmnet(x=p2p2_pred, y=p2p2_out, family="gaussian", alpha=0, lambda=p2p2_cv_ridgem$lambda.min)
#plot
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
cf1 <- append(cf1, rownames(p2p2_optimal_ridgem$beta)[i])
cf2 <- append(cf2, unname(p2p2_optimal_ridgem$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
p2_ridgePlot <- ggplot(data=cf, aes(x=cf1, y=cf2)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
p2_ridgePlot
p2p3_pred <- data.matrix(o_train[,-38])
p2p3_out <- data.matrix(o_train$imdb_rating)
#100 possible values of lambda
p2p3_lassom <- glmnet(x=p2p3_pred, y=p2p3_out, family="gaussian", alpha=1, nlambda=100)
#print(p2p3_lassom)
p2p3_cv_lassom <- cv.glmnet(x=p2p3_pred, y=p2p3_out, family="gaussian", alpha=1, nfolds=10, type.measure="mse")
plot(p2p3_cv_lassom)
print(p2p3_cv_lassom)
#0.03343 is the initial p2p3_cv_lassom$lambda.min we get
p2p3_optimal_lassom <- glmnet(x=p2p3_pred, y=p2p3_out, family="gaussian", alpha=1, lambda=p2p3_cv_lassom$lambda.min)
#plot
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
cf1 <- append(cf1, rownames(p2p3_optimal_lassom$beta)[i])
cf2 <- append(cf2, unname(p2p3_optimal_lassom$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
p2_LASSOPlot <- ggplot(data=cf, aes(x=cf1, y=cf2)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
p2_LASSOPlot
