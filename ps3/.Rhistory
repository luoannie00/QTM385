}
lm_pred_metrics(p2p1_m) #LOOCV is 0.2343854
cf <-coef(summary(p2p1_m,complete = TRUE))
cf <- data.frame(rownames(cf), cf)
ggplot(data = cf, aes(x = rownames.cf., y = Estimate)) +
geom_bar(stat = "identity", width = 0.75) +
coord_flip() +
labs(x = "\n Features \n", y = "Coefficient Estimate \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
p2p1_imp_pred <- subset(o_train, select=c(season, greg_daniels, b_j_novak, imdb_rating))
p2p1_imp_pred <- filter(p2p1_imp_pred, season == "Season 7" | season == "Season 5" | season == "Season 4" | season == "Season 3" | season == "Season 2")
p2p1_imp_pred_m <- lm(imdb_rating~., data = p2p1_imp_pred)
#LOOCV
lm_pred_metrics(p2p1_imp_pred_m) #LOOCV is 0.1831386
#dummied out variables
o_train$season <- factor(o_train$season)
dummy_o_train <- caret::dummyVars("~.", data = o_train)
o_train <- data.frame(predict(dummy_o_train, newdata = o_train))
#ridge
p2p2_pred <- as.matrix(o_train[,-38])
p2p2_out <- as.matrix(o_train$imdb_rating)
cv.glmnet(x = p2p2_pred, y = p2p2_out, family = "gaussian", alpha = 0, nfolds = 10, type.measure = "mse") #100 possible values of lambda by default
library(ggplot2)
library(data.table)
library(dplyr)
library(glmnet)
library(caret)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
#setwd('/Users/billyge/Desktop/Emory Fall 2022/QTM 385-4/problem sets/ps github/ps3')
setwd('/Users/annie/Desktop/QTM385/QTM350_Github/QTM350 Github/QTM385/ps3')
rm(list = ls())
c_test <- read.csv('college_test.csv')
c_train <- read.csv('college_train.csv')
o_test <- read.csv('office_test.csv')
o_train <- read.csv('office_train.csv')
o_train$episode_name <- NULL #feature that's observation specific is taken out
o_train$season <- factor(o_train$season)
dummy_o_train <- caret::dummyVars("~.", data = o_train)
o_train <- data.frame(predict(dummy_o_train, newdata = o_train))
View(o_test)
View(o_train)
#OLS
p2p1_m <- lm(imdb_rating~., data = o_train)
#LOOCV
lm_pred_metrics <- function(fit_model){
LOOCV <- mean((fit_model$residuals / (1-hatvalues(fit_model)))^2)
return(c("LOOCV" = LOOCV))
}
lm_pred_metrics(p2p1_m) #LOOCV is 0.2343854
cf <-coef(summary(p2p1_m,complete = TRUE))
cf <- data.frame(rownames(cf), cf)
View(cf)
ggplot(data = cf, aes(x = rownames.cf., y = Estimate)) +
geom_bar(stat = "identity", width = 0.75) +
coord_flip() +
labs(x = "\n Features \n", y = "Coefficient Estimate \n") +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
library(tidyverse)
library(glmnet)
library(caret)
library(Metrics)
library(splines)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
setwd('/Users/annie/Desktop/QTM385/PS3')
rm(list = ls())
c_test <- read.csv('college_test.csv')
c_train <- read.csv('college_train.csv')
o_test <- read.csv('office_test.csv')
o_train <- read.csv('office_train.csv')
#delete variable episode_name
o_train <- o_train %>% select(-episode_name)
#split variable season into 9 variables
dummy_o_train <- caret::dummyVars("~.", data=o_train)
o_train <- data.frame(predict(dummy_o_train, newdata=o_train))
#full model
p2p1_m <- lm(imdb_rating ~ ., data=o_train)
#LOOCV
lm_pred_metrics <- function(fit_model) {
LOOCV <- mean((fit_model$residuals / (1-hatvalues(fit_model)))^2)
return (LOOCV)
}
lm_pred_metrics(p2p1_m) #LOOCV is 0.2343854
#plot
cf <- coef(summary(p2p1_m, complete=TRUE))
cf <- data.frame(rownames(cf), cf)
ggplot(data=cf, aes(x=rownames.cf., y=Estimate)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
#subset of important predictors
p2p1_imp_pred <- o_train %>% select(season.Season.3, season.Season.1, paul_lieberstein, mindy_kaling, justin_spitzer, greg_daniels, b_j_novak, imdb_rating)
#important predictors model
p2p1_imp_pred_m <- lm(imdb_rating ~ ., data=p2p1_imp_pred)
#LOOCV
lm_pred_metrics(p2p1_imp_pred_m) #LOOCV is 0.2543553
p2p2_pred <- as.matrix(o_train[,-38])
p2p2_out <- as.matrix(o_train$imdb_rating)
#100 possible values of lambda
p2p2_ridgem <- glmnet::glmnet(x=p2p2_pred, y=p2p2_out, family="gaussian", alpha=0, nlambda=100)
print(p2p2_ridgem)
p2p2_cv_ridgem <- cv.glmnet(x=p2p2_pred, y=p2p2_out, family="gaussian", alpha=0, nfolds=10, type.measure="mse")
plot(p2p2_cv_ridgem)
#min(p2p2_ridgem$lambda)
print(p2p2_cv_ridgem)
p2p2_optimal_ridgem <- glmnet::glmnet(x=p2p2_pred, y=p2p2_out, family="gaussian", alpha=0, lambda=0.74)
#plot
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
cf1 <- append(cf1, rownames(p2p2_optimal_ridgem$beta)[i])
cf2 <- append(cf2, unname(p2p2_optimal_ridgem$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
p2p3_pred <- as.matrix(o_train[,-38])
p2p3_out <- as.matrix(o_train$imdb_rating)
#100 possible values of lambda
p2p3_lassom <- glmnet::glmnet(x=p2p3_pred, y=p2p3_out, family="gaussian", alpha=1, nlambda=100)
print(p2p3_lassom)
p2p3_cv_lassom <- cv.glmnet(x=p2p3_pred, y=p2p3_out, family="gaussian", alpha=1, nfolds=10, type.measure="mse")
plot(p2p3_cv_lassom)
print(p2p3_cv_lassom)
p2p3_optimal_lassom <- glmnet::glmnet(x=p2p3_pred, y=p2p3_out, family="gaussian", alpha=1, lambda=0.03343)
#plot
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
cf1 <- append(cf1, rownames(p2p3_optimal_lassom$beta)[i])
cf2 <- append(cf2, unname(p2p3_optimal_lassom$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
data.frame(
epe_full_lm = lm_pred_metrics(p2p1_m),
epe_subset_lm = lm_pred_metrics(p2p1_imp_pred_m),
epe_optimal_ridge = min(p2p2_cv_ridgem$cvm),
epe_optimal_lasso = min(p2p3_cv_lassom$cvm))
#delete variable episode_name
o_test <- o_test %>% select(-episode_name)
#split variable season into 9 variables
dummy_o_test <- caret::dummyVars("~.", data=o_test)
o_test <- data.frame(predict(dummy_o_test, newdata=o_test))
p2p4_pred_mat <- as.matrix(o_test[,-38])
p2p4_pred_df_full <- o_test[,-38]
p2p4_pred_df_imp <- o_test %>% select(seasonSeason.3, seasonSeason.1, paul_lieberstein, mindy_kaling, justin_spitzer, greg_daniels, b_j_novak, imdb_rating)
#delete variable episode_name
o_test <- o_test %>% select(-episode_name)
View(o_test)
setwd('/Users/annie/Desktop/QTM385/PS3')
rm(list = ls())
c_test <- read.csv('college_test.csv')
c_train <- read.csv('college_train.csv')
o_test <- read.csv('office_test.csv')
o_train <- read.csv('office_train.csv')
#delete variable episode_name
o_test <- o_test %>% select(-episode_name)
View(o_train)
library(tidyverse)
library(glmnet)
library(caret)
library(Metrics)
library(splines)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
setwd('/Users/annie/Desktop/QTM385/PS3')
rm(list = ls())
c_test <- read.csv('college_test.csv')
c_train <- read.csv('college_train.csv')
o_test <- read.csv('office_test.csv')
o_train <- read.csv('office_train.csv')
#delete variable episode_name
o_train <- o_train %>% select(-episode_name)
#split variable season into 9 variables
dummy_o_train <- caret::dummyVars("~.", data=o_train)
o_train <- data.frame(predict(dummy_o_train, newdata=o_train))
#full model
p2p1_m <- lm(imdb_rating ~ ., data=o_train)
#LOOCV
lm_pred_metrics <- function(fit_model) {
LOOCV <- mean((fit_model$residuals / (1-hatvalues(fit_model)))^2)
return (LOOCV)
}
lm_pred_metrics(p2p1_m) #LOOCV is 0.2343854
#plot
cf <- coef(summary(p2p1_m, complete=TRUE))
cf <- data.frame(rownames(cf), cf)
ggplot(data=cf, aes(x=rownames.cf., y=Estimate)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
#subset of important predictors
p2p1_imp_pred <- o_train %>% select(season.Season.3, season.Season.1, paul_lieberstein, mindy_kaling, justin_spitzer, greg_daniels, b_j_novak, imdb_rating)
#important predictors model
p2p1_imp_pred_m <- lm(imdb_rating ~ ., data=p2p1_imp_pred)
#LOOCV
lm_pred_metrics(p2p1_imp_pred_m) #LOOCV is 0.2543553
p2p2_pred <- as.matrix(o_train[,-38])
p2p2_out <- as.matrix(o_train$imdb_rating)
#100 possible values of lambda
p2p2_ridgem <- glmnet::glmnet(x=p2p2_pred, y=p2p2_out, family="gaussian", alpha=0, nlambda=100)
print(p2p2_ridgem)
p2p2_cv_ridgem <- cv.glmnet(x=p2p2_pred, y=p2p2_out, family="gaussian", alpha=0, nfolds=10, type.measure="mse")
plot(p2p2_cv_ridgem)
#min(p2p2_ridgem$lambda)
print(p2p2_cv_ridgem)
p2p2_optimal_ridgem <- glmnet::glmnet(x=p2p2_pred, y=p2p2_out, family="gaussian", alpha=0, lambda=0.74)
#plot
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
cf1 <- append(cf1, rownames(p2p2_optimal_ridgem$beta)[i])
cf2 <- append(cf2, unname(p2p2_optimal_ridgem$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
p2p3_pred <- as.matrix(o_train[,-38])
p2p3_out <- as.matrix(o_train$imdb_rating)
#100 possible values of lambda
p2p3_lassom <- glmnet::glmnet(x=p2p3_pred, y=p2p3_out, family="gaussian", alpha=1, nlambda=100)
print(p2p3_lassom)
p2p3_cv_lassom <- cv.glmnet(x=p2p3_pred, y=p2p3_out, family="gaussian", alpha=1, nfolds=10, type.measure="mse")
plot(p2p3_cv_lassom)
print(p2p3_cv_lassom)
p2p3_optimal_lassom <- glmnet::glmnet(x=p2p3_pred, y=p2p3_out, family="gaussian", alpha=1, lambda=0.03343)
#plot
cf1 <- c()
cf2 <- c()
for (i in 1:37) {
cf1 <- append(cf1, rownames(p2p3_optimal_lassom$beta)[i])
cf2 <- append(cf2, unname(p2p3_optimal_lassom$beta)[i])
}
cf <- data.frame(cf1, cf2)
ggplot(data=cf, aes(x=cf1, y=cf2)) +
geom_bar(stat="identity", width=0.75) + coord_flip() +
labs(x="Features", y="Coefficient Estimate") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5),
axis.title.x = element_text(face="bold", colour="red", size = 12),
axis.title.y = element_text(face="bold", colour="red", size = 12))
data.frame(
epe_full_lm = lm_pred_metrics(p2p1_m),
epe_subset_lm = lm_pred_metrics(p2p1_imp_pred_m),
epe_optimal_ridge = min(p2p2_cv_ridgem$cvm),
epe_optimal_lasso = min(p2p3_cv_lassom$cvm))
#delete variable episode_name
o_test <- o_test %>% select(-episode_name)
#split variable season into 9 variables
dummy_o_test <- caret::dummyVars("~.", data=o_test)
o_test <- data.frame(predict(dummy_o_test, newdata=o_test))
p2p4_pred_mat <- as.matrix(o_test[,-38])
p2p4_pred_df_full <- o_test[,-38]
p2p4_pred_df_imp <- o_test %>% select(season.Season.3, season.Season.1, paul_lieberstein, mindy_kaling, justin_spitzer, greg_daniels, b_j_novak, imdb_rating)
#predictions
o_test$full_lm <- predict(p2p1_m, newdata=p2p4_pred_df_full)
o_test$subset_lm <- predict(p2p1_imp_pred_m, newdata=p2p4_pred_df_imp)
o_test$optimal_ridge <- predict(p2p2_optimal_ridgem, newx=p2p4_pred_mat)
o_test$optimal_lasso <- predict(p2p3_optimal_lassom, newx=p2p4_pred_mat)
#mse comparison
data.frame(
mse_full_lm = mse(o_test$imdb_rating, o_test$full_lm),
mse_subset_lm = mse(o_test$imdb_rating, o_test$subset_lm),
mse_optimal_ridge = mse(o_test$imdb_rating, o_test$optimal_ridge),
mse_optimal_lasso = mse(o_test$imdb_rating, o_test$optimal_lasso))
#initial recoding: use c_train_q4 moving on for consistence
c_train_q4 <- c_train
c_train_q4$Outstate <- log(c_train$Outstate)
approachPlot <- qplot(x=S.F.Ratio, y=Outstate, data=c_train_q4, geom=c("point","smooth"), ylab="out of state tuition", xlab="student/faculty ratio")
approachPlot
# It looks quite linear, but could also be a second/third degree polynomial relationship. Compare the first smooth graph with the second one plotting in linear model, we can see that the relationship has a linear trend.
lmApprochPlot <- qplot(x=S.F.Ratio, y=Outstate, data=c_train_q4, geom=c("point","smooth"), method="lm", ylab="out of state tuition", xlab="student/faculty ratio")
lmApprochPlot
# we reuse the function from problem set 2 for the wealth of non-simulation based estimates of the expected prediction error to make this decision
lm_pred_metrics <- function(fit_model) {
#log-likelihood
N <- length(fit_model$residuals)
sig <- sigma(fit_model)
res <- fit_model$residuals
ll <- -N/2*log10(2*pi) - N/2*log10(sig^2) - 1/(2*sig^2)*sum(res^2)
#AIC
d <- length(fit_model$coefficients) + 1
AIC <- -2*ll + 2*(d)
#BIC
BIC <- -2*ll + d*log10(N)
#LOOCV
LOOCV <- (res[1] / (1-hatvalues(fit_model)[1]))^2
if (N>=2){
for (i in 2:N){
LOOCV <- LOOCV + (res[i] / (1-hatvalues(fit_model)[i]))^2
}
}
LOOCV_final <- LOOCV / N
return(c("AIC"=AIC, "BIC"=BIC, "LOOCV"=LOOCV_final))
}
p4p1_model_1 <- lm(c_train_q4$Outstate~poly(c_train_q4$S.F.Ratio,1))
p4p1_model_2 <- lm(c_train_q4$Outstate~poly(c_train_q4$S.F.Ratio,2))
p4p1_model_3 <- lm(c_train_q4$Outstate~poly(c_train_q4$S.F.Ratio,3))
p4p1_model_4 <- lm(c_train_q4$Outstate~poly(c_train_q4$S.F.Ratio,4))
p4p1_model_5 <- lm(c_train_q4$Outstate~poly(c_train_q4$S.F.Ratio,5))
p4p1_model_6 <- lm(c_train_q4$Outstate~poly(c_train_q4$S.F.Ratio,6))
p4p1_model_7 <- lm(c_train_q4$Outstate~poly(c_train_q4$S.F.Ratio,7))
p4p1_model_8 <- lm(c_train_q4$Outstate~poly(c_train_q4$S.F.Ratio,8))
p4p1_model_9 <- lm(c_train_q4$Outstate~poly(c_train_q4$S.F.Ratio,9))
p4p1_model_10 <- lm(c_train_q4$Outstate~poly(c_train_q4$S.F.Ratio,10))
p4p1_df <- t(cbind(lm_pred_metrics(p4p1_model_1), lm_pred_metrics(p4p1_model_2), lm_pred_metrics(p4p1_model_3), lm_pred_metrics(p4p1_model_4), lm_pred_metrics(p4p1_model_5), lm_pred_metrics(p4p1_model_6), lm_pred_metrics(p4p1_model_7), lm_pred_metrics(p4p1_model_8), lm_pred_metrics(p4p1_model_9), lm_pred_metrics(p4p1_model_10)))
num <- c(1:10)
p4p1_df <- data.frame(num, p4p1_df)
#plot LOOCV for various degree of polynomial
ggplot(data=p4p1_df) +
geom_point(aes(x=num, y=LOOCV.1)) +
geom_line(aes(x=num, y=LOOCV.1)) +
xlab("degree polynomial") + ylab('LOOCV') +
theme_bw()
# We use LOOCV as our measure of EPE and find that third order of global polynomial that minimizes EPE. We choose LOOCV as there is less worry about bias and variance than other approaches and it's asymptotically unbiased for the  the EPE.From the graph above, we could see that the LOOCV reaches minimum for degree polynomial=3.
#optimal polynomial model
#train the model with third degree of polynomial
optimalPolyModel <- lm(c_train_q4$Outstate ~ poly(c_train_q4$S.F.Ratio,3))
polyPredResult <- data.frame(S.F.Ratio=c_train_q4$S.F.Ratio,
predOutState=optimalPolyModel$fitted.values,
originalOutState=c_train_q4$Outstate)
#plot our polynomial model
polyPredResultPlot <- ggplot(data=polyPredResult) +
geom_point(aes(x=S.F.Ratio, y=predOutState, col='prediction')) +
geom_point(aes(x=S.F.Ratio, y=originalOutState, col='original')) +
scale_colour_manual(name="Legend", values=c('black','red')) +
ylab('OutState') + theme_bw()
polyPredResultPlot
#natural spline(cubic)
#find the optimal df with our measure of EPE
p4_1_num <- seq(1,20,1)
p4_1_aic <- c()
p4_1_bic <- c()
p4_1_loocv <- c()
for (i in 1:20) {
cubicSpline <- lm(Outstate ~ ns(S.F.Ratio,df=i), data=c_train_q4)
tempCheck <- unname(lm_pred_metrics(cubicSpline))
p4_1_aic <- append(p4_1_aic, tempCheck[1])
p4_1_bic <- append(p4_1_bic, tempCheck[2])
p4_1_loocv <- append(p4_1_loocv, tempCheck[3])
}
p4_1_dat <- data.frame(p4_1_num, p4_1_aic, p4_1_bic, p4_1_loocv)
#plot how each df perform with each measure of EPE
ggplot(data=p4_1_dat) +
geom_point(aes(x=p4_1_num, y=p4_1_aic)) +
geom_line(aes(x=p4_1_num, y=p4_1_aic)) +
xlab('df') + ylab('aic') +
theme_bw()
ggplot(data=p4_1_dat) +
geom_point(aes(x=p4_1_num, y=p4_1_bic)) +
geom_line(aes(x=p4_1_num, y=p4_1_bic)) +
xlab('df') + ylab('bic') +
theme_bw()
ggplot(data=p4_1_dat) +
geom_point(aes(x=p4_1_num, y=p4_1_loocv)) +
geom_line(aes(x=p4_1_num, y=p4_1_loocv)) +
xlab('df') + ylab('loocv') +
theme_bw()
#optimal Natural Spline model
optimalCubicModel <- lm(Outstate ~ ns(S.F.Ratio,df=4), data=c_train_q4)
nsPredResult <- data.frame(S.F.Ratio=c_train_q4$S.F.Ratio,
predOutState=optimalCubicModel$fitted.values,
originalOutState=c_train_q4$Outstate)
predResultPlot <- ggplot(data=nsPredResult) +
geom_point(aes(x=S.F.Ratio, y=predOutState, col='prediction')) +
geom_point(aes(x=S.F.Ratio, y=originalOutState, col='original')) +
scale_colour_manual(name="Legend", values=c('black','red')) +
ylab('OutState') + theme_bw()
predResultPlot
#optimal smoothing spline model
optimalSSModel <- smooth.spline(x=c_train_q4$S.F.Ratio, y=c_train_q4$Outstate)
ssPredResult <- data.frame(S.F.Ratio=c_train_q4$S.F.Ratio,predOutState=predict(optimalSSModel, x=c_train_q4$S.F.Ratio),                         originalOutState=c_train_q4$Outstate)
predResultPlot <- ggplot(data=ssPredResult) +
geom_point(aes(x=S.F.Ratio, y=predOutState.y, col='prediction')) +
geom_point(aes(x=S.F.Ratio, y=originalOutState, col='original')) +
scale_colour_manual(name="Legend", values=c('black','red')) +
ylab('OutState') + theme_bw()
predResultPlot
#recode testing dataframe
c_test_q4 <- c_test
c_test_q4$Outstate <- log(c_test$Outstate)
#train three optimal models and calculate mse
test_poly_MSE = mse(predict(optimalPolyModel,x=c_test_q4$S.F.Ratio),c_test_q4$Outstate)
test_ns_MSE = mse(predict(optimalCubicModel,x=c_test_q4$S.F.Ratio),c_test_q4$Outstate)
ssTestPredResult <- data.frame(predOutState=predict(optimalSSModel, x=c_test_q4$S.F.Ratio),                         originalOutState=c_test_q4$Outstate)
test_ss_MSE = mse(ssTestPredResult$predOutState.y,c_test_q4$Outstate)
table<-data.frame(polyTestResult=test_poly_MSE,naturalSplineResult=test_ns_MSE,smoothingSplineResult=test_ss_MSE)
table
# set things as matrix
lasso_predictor <- data.matrix(c_train_q4[,-9])
lasso_outcome <- data.matrix(c_train_q4$Outstate)
c_lasso <- glmnet(x=lasso_predictor, y=lasso_outcome, family="gaussian", alpha=1, nlambda=100)
print(c_lasso)
c_lasso_cv <- cv.glmnet(x=lasso_predictor, y=lasso_outcome, family="gaussian", alpha=1, nfolds=10, type.measure="mse")
print(c_lasso_cv)
plot(c_lasso_cv)
optimalLambda<-c_lasso_cv$lambda.min
smallVarLambda<-c_lasso_cv$lambda.1se
c_lasso_optimal <- glmnet(x=lasso_predictor, y=lasso_outcome, family="gaussian", alpha=1, lambda=optimalLambda)
#there are 14 variables in our optimal list (shown below)
variableList<-c_lasso_optimal$beta
variableList
#We decide to go with the second approach
# From the plot() and cv function in part two, we alreay know the min mse Lambda(optimalLambda) and largest value of lambda that error within 1 standard error of the min.(smallVarLambda) as the following:
optimalLambda<-c_lasso_cv$lambda.min
smallVarLambda<-c_lasso_cv$lambda.1se
# models trained with these two lambda values
c_lasso_optimal <- glmnet(x=lasso_predictor, y=lasso_outcome, family="gaussian", alpha=1, lambda=optimalLambda)
c_lasso_largeLambda <- glmnet(x=lasso_predictor, y=lasso_outcome, family="gaussian", alpha=1, lambda=smallVarLambda)
# print the included predictors
c_lasso_optimal$beta
c_lasso_largeLambda$beta
#In conclusion, we pick the following 5 predictors:
#Private/S.F.Ratio/PhD/perc.alumni/Grad.Rate
c_test_q4_selected<-data.frame(Outstate=c_test_q4$Outstate,Private=c_test_q4$Private,S.F.Ratio=c_test_q4$S.F.Ratio,PhD=c_test_q4$PhD,perc.alumni=c_test_q4$perc.alumni,Grad.Rate=c_test_q4$Grad.Rate)
c_train_q4_selected<-data.frame(Outstate=c_train_q4$Outstate,Private=c_train_q4$Private,S.F.Ratio=c_train_q4$S.F.Ratio,PhD=c_train_q4$PhD,perc.alumni=c_train_q4$perc.alumni,Grad.Rate=c_train_q4$Grad.Rate)
#try to build model with GAM
# if we include all variables in smooth, the model and gcv is:
fullGam<-gam(Outstate ~ Private+s(S.F.Ratio) + s(PhD) + s(perc.alumni) + s(Grad.Rate),data=c_train_q4_selected,family = gaussian())
library(mgcv)
#try to build model with GAM
# if we include all variables in smooth, the model and gcv is:
fullGam<-gam(Outstate ~ Private+s(S.F.Ratio) + s(PhD) + s(perc.alumni) + s(Grad.Rate),data=c_train_q4_selected,family = gaussian())
fullGamGcv<-fullGam$gcv.ubre.dev
fullGamGcv
#if we want to include one interaction term w te() tensor spline
temMinGcv<-100
#summary()
for (i in 3:6){
for (j in 3:6){
if (i==j){
break
}
df<-data.frame(c_train_q4_selected[i],c_train_q4_selected[j],c_train_q4_selected[1])
names(df)[1] <- "x1"
names(df)[2] <- "x2"
names(df)[3] <- "y"
tempModel<-gam(Outstate ~ Private+s(S.F.Ratio) + s(PhD) + s(perc.alumni) + s(Grad.Rate)+te(df$x1,df$x2),
data=c_train_q4_selected,
family = gaussian())
tempGcv<-tempModel$gcv.ubre.dev
if (temMinGcv>tempGcv){
temMinGcv<-tempGcv
num1<-i
num2<-j
bestTEModel<-tempModel
}
}
}
# gcv and interaction terms are:
temMinGcv # also the smallest gcv we got so far
c_train_q4_selected[num1]
c_train_q4_selected[num2]
#if we want to include one interaction term w s()
sMinGcv<-100
#summary()
for (i in 3:6){
for (j in 3:6){
if (i==j){
break
}
df<-data.frame(c_train_q4_selected[i],c_train_q4_selected[j],c_train_q4_selected[1])
names(df)[1] <- "x1"
names(df)[2] <- "x2"
names(df)[3] <- "y"
tempModel<-gam(Outstate ~ Private+s(S.F.Ratio) + s(PhD) + s(perc.alumni) + s(Grad.Rate)+s(df$x1,df$x2),
data=c_train_q4_selected,
family = gaussian())
tempGcv<-tempModel$gcv.ubre.dev
if (sMinGcv>tempGcv){
sMinGcv<-tempGcv
snum1<-i
snum2<-j
bestSModel<-tempModel
}
}
}
sMinGcv
c_train_q4_selected[snum1]
c_train_q4_selected[snum2]
# so we have:
#bestTEModel
#bestSModel
#fullGam
plot(bestTEModel,pages=1)
plot(bestSModel,pages=1)
plot(fullGam,pages=1)
