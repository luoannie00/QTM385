
---
title: 'Problem Set #5'
author: "Kevin McAlister"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: no
    toc_depth: '2'
  prettydoc::html_pretty:
    df_print: kable
    highlight: github
    theme: leonids
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
---

```{r, include=FALSE}
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
# add
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

```{r}
# data read-in
car_train <- read.csv('carsTrain.csv')
car_test <- read.csv('carsTest.csv')
svm_train <- read.csv('simSVMTrain.csv')
svm_test<-read.csv("simSVMTestF.csv")
```

Collaborators: Billy Ge, Latifa Tan, Annie Luo
 

***

## Problem 1: An Unintuitive Result

The support vector classifier generalizes the optimal separating hyperplane by allowing *slack variables* to handle cases where the points are misclassified (or within the margin).  A somewhat uninuitive result related to the support vector classifier arises when we have a two-class problem where the data is only *barely separable* - a SVC with a small cost that misclassifies a few data points may outperform an optimally separating hyperplane (with no points inside the margin, let alone misclassified) on out-of-sample data.  Let's show this via a simulated example.

### Part 1

`simSVMTrain.csv` contains 729 instances of two predictors and a binary class label.  In this simulated example, the classes are **linearly separable**.  However, they are generated from two separate data generating processes.  As shown in the below figure, the first DGP generates data that is separable with a large margin.  The second DGP generates data that is still linearly separable, but with less of a margin than the first.  This kind of conditional separating behavior is "exaggerated" in this data set, but it does occur quite frequently in real data sets when there is an important omitted predictor that conditions how classes are separated.

```{r, include=TRUE, eval = TRUE}
# train <- fread("simSVMTrain.csv")
# par(mfrow = c(1,2))
# plot(train$X1,train$X2, col = train$class + 3, type = "n", xlab = "X1", ylab = "X2")
# points(train$X1[train$class == -1 & train$group == 1],train$X2[train$class == -1 & train$group == 1], pch = "+", col = "blue")
# points(train$X1[train$class == 1 & train$group == 1],train$X2[train$class == 1 & train$group == 1], pch = "+", col = "red")
# points(train$X1[train$class == -1 & train$group == 2],train$X2[train$class == -1 & train$group == 2], pch = "+", col = "green")
# points(train$X1[train$class == 1 & train$group == 2],train$X2[train$class == 1 & train$group == 2], pch = "+", col = "orange")
# plot(train$X1,train$X2, col = as.numeric(train$class) + 3, pch = "+", xlab = "X1", ylab = "X2")
# par(mfrow = c(1,1))
```

While it may not immediately appear to be the case in the combined data set, the classes are actually linearly separable!  Prove that this is true by 1) plotting the data and corresponding labels, 2) computing the optimal separating hyperplane and corresponding margins and plotting them against the data, and 3) showing that the **signed distance** from the hyperplane multiplied by the class label is greater than or equal to one for all observations in the data set. (Note: When you compute the signed distance, you may find that a few observations have a signed distance of .999---.  This can happen due to a combination of rounding and our approximation to the perfectly separable case with a finite cost.)

Some hints:

  1) We can use SVM software to get the separating hyperplane and margins in the perfectly separable case when we set `cost` to be very high.  `cost = 10000` should be sufficient.
  
  2) There's no need for a nonlinear kernel here since the decision boundary is linear.  Make sure to set the kernel to linear or vanilladot since most SVM software sets the kernel to radial/Gaussian by default.
  
  3) For the two predictor case, the separating hyperplane can be computed by finding all values of $\boldsymbol{x} \in \mathbb{R}^2$ such that $\hat{\alpha} + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0$.  Given the coefficients (which can usually be found using `coef()` or a similar argument in R or `svm.decision_function` in `sklearn`) and a value of $x_1$, we can uniquely compute $x_2$ that solves the equation.
  
  4) The predictors are already normalized, so don't worry about that here - this means that you don't need to rescale the coefficients by the vector norm, $\| \beta \|$.
  
  5) Since the SVM coefficients are normalized in the actual computation, we can compute the margins by finding values of $\boldsymbol{x}$ that solve $\hat{\alpha} + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = \pm 1$
  
  6) It is possible that all of the signed distances will be less than or equal to -1!  Because there is no real requirement that the SVC assigns $y = -1$ to be the negative distance class, we could just have everything reversed.  This is equivalent, so don't think that there's something wrong!
  
  7) I've written a lot of helpful stuff at the end of this problem set.  Check it out to get some clarity on how to approach the problem.
  

```{r}
svm_m1 <- ksvm(class ~ X1 + X2 , data = svm_train, type = "C-svc", kernel = "vanilladot", 
               C = 10000, cross = 5, prob.model = TRUE)

neg_intercept <- b(svm_m1)
svobs <- svm_train[alphaindex(svm_m1)[[1]],] #support vector observations
beta1 <- sum(svobs[,'X1']*coef(svm_m1)[[1]])
beta2 <- sum(svobs[,'X2']*coef(svm_m1)[[1]])

#decision boundary
x1 <- c(0,0.2)
x2 <- c((neg_intercept)/beta2 - beta1/beta2*0, (neg_intercept)/beta2 - beta1/beta2*0.2)
p1p1_df1 <- data.frame(x1,x2)

#supporting vector margins
x1 <- c(0,0.2)
x2 <- c((1+neg_intercept)/beta2 - beta1/beta2*0, (1+neg_intercept)/beta2 - beta1/beta2*0.2)
p1p1_df2 <- data.frame(x1,x2)

x1 <- c(0,0.2)
x2 <- c((-1+neg_intercept)/beta2 - beta1/beta2*0, (-1+neg_intercept)/beta2 - beta1/beta2*0.2)
p1p1_df3 <- data.frame(x1,x2)
```

```{r}
reg_line1 <- lm(data=p1p1_df1, x2 ~ x1)
reg_line2 <- lm(data=p1p1_df2, x2 ~ x1)
reg_line3 <- lm(data=p1p1_df3, x2 ~ x1)
plot(svm_train$X1,svm_train$X2, col = as.numeric(svm_train$class) + 3, pch = "+", xlab = "X1", ylab = "X2")
abline(reg_line1, col='black')
abline(reg_line2, col='green')
abline(reg_line3, col='green')
```

```{r}
svm_train$signed_dist <- svm_train$class*(-neg_intercept+beta1*svm_train$X1+beta2*svm_train$X2)
svm_train %>% filter(signed_dist < 1)
#while we see that there are three observations whose signed_dist is less than 1, all their values are around 0.9999, showing us that they would've been 1 had there been no rounding errors or approximating erros.
```

  
### Part 2

Now, let's think about out of sample performance.  `simSVMTestF.csv` includes 200 observations that are generated using the same DGP used to generate the training data just with slightly different levels of occurrence for each group (feel free to verify this yourself!).    

Using `cost = 10000`, compute the proportion of observations misclassified in the training set, a 5-fold cross validation measure of the expected proportion of observations misclassified in a test set, and the actual proportion of observations misclassified using the trained SVC to create predictions for the test set.  What do you see from these measures?  How can it be the case that data generated from the exact same process can yield training errors?  Think about the SVC and **overfitting**.

Now repeat this process for 50 different values of `cost` ranging from very small to very large - a reasonable set of cost values can be generated using `exp(seq(-10,10,length = 50))` in R.   Make a plot that shows the training error, 10 fold CV estimate, and actual proportion of misclassified points in the test set plotted against the cost values.

There's a decent amount of information to unpack from this graph!  

  1) What value of cost returns the lowest proportion of misclassified points in the test set?  How does this cost value compare to the optimal separating hyperplane with minimal slackness?  Provide some reasoning for this result.
  
  2) Does the K-fold CV error estimate more closely correspond to the training error or the test error?  In a few sentences, provide some reasoning for this result.  Think very carefully about how cross validation would work for the SVC and why we might see this relationship - how does drawing the decision boundary for **support vector** classification differ from the other models we've discussed in class?
  
Note that your findings for this problem are most prevalent when the data are perfectly separable and this relationship tends to disappear as there is more crossover between classes.  This may seem like the most common scenario, but an SVM using the radial kernel is **guaranteed** to perfectly separate any training data in the two class problem so long as there are no two points with the same $\boldsymbol x_i$ that have different classes!  Just something to be wary of and why a combined $K$-fold and validation set approach is preferable to just using $K$-fold cross validation!

```{r}
linearSVM_classfier <- ksvm(class ~ X1+X2, data = svm_train, type = "C-svc", kernel = "vanilladot", 
             C = 10000, cross = 5, prob.model = TRUE)
# 1) training misclassification rate
train_preds_class <- predict(linearSVM_classfier, newdata = svm_train, type = "response")
linearSVM_train_ms<- mean(train_preds_class != svm_train$class)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(svm_train)), k = 5, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
  linearSVM_Mod <- ksvm(class ~ X1+X2 , data = svm_train[folds[[i]],], type = "C-svc", 
                        kernel = "vanilladot",C = 10000, cross = 5, 
                        prob.model = TRUE)
  preds <- predict(linearSVM_Mod, newdata = svm_train[-folds[[i]],], 
                              type = "response")
  kf_misclass[i]<- mean(preds != svm_train[-folds[[i]],]$class)
}
linearSVM_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(linearSVM_classfier, newdata = svm_test, type = "response")
linearSVM_test_ms<- mean(test_preds_class != svm_test$class)
#conclusion
data.frame('name' = c('linearSVM_train_ms','linearSVM_kfold_ms','linearSVM_test_ms'),
           'estimate' = c(linearSVM_train_ms,linearSVM_kfold_ms,linearSVM_test_ms))
```
From the result table, we can see that the training dataset is perfectly separable, since as we set Cost to be extremely high, the misclassification rate in the training set is zero. Our linear model perfectly separates the classes in training data. However, while we see a relatively low expected rate of misclassification, the actual misclassification rate in test data was much higher than expected. This is largely due to overfitting with such a high Cost value. The zero misclassification rate on training set shows us that there the classifier may have been overfitted, where the SVM picks up noise as signals, while noise in training data are not transferrable to test data.


```{r}
train_ms <- c()
kfold_ms <- c()
test_ms <- c()
#Create a vector of potential C values
cost_vals <- exp(seq(-10,10,length = 50))
for(i in 1:length(cost_vals)){
  #Train the SVM
  svm1 <- ksvm(class ~ X1+X2 , data = svm_train, type = "C-svc", kernel = "vanilladot", 
                C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
  # 1) training misclassification rate
  train_preds_class <- predict(svm1, newdata = svm_train, type = "response")
  train_ms[i] <- mean(train_preds_class != svm_train$class)
  # 2) k-fold cv misclassification rate
  folds <- create_folds(seq(1,nrow(svm_train)), k = 10, type = "basic")
  kf_misclass <- c()
  for(j in 1:length(folds)){
    linearSVM_Mod <- ksvm(class ~ X1+X2 , data = svm_train[folds[[j]],], type = "C-svc", 
                          kernel = "vanilladot",C = cost_vals[i], cross = 5, 
                          prob.model = TRUE, kpar = list())
    preds <- predict(linearSVM_Mod, newdata = svm_train[-folds[[j]],], 
                                type = "response")
    kf_misclass[j]<- mean(preds != svm_train[-folds[[j]],]$class)
  }
  kfold_ms[i] <- mean(kf_misclass)
  # 3) test set misclassification rate
  test_preds_class <- predict(svm1, newdata = svm_test, type = "response")
  test_ms[i] <- mean(test_preds_class != svm_test$class)
}
```

```{r}
p1p2_df <- data.frame(cost_vals,train_ms,kfold_ms,test_ms)
p1p2_df[which.min(train_ms),][1] #1902.683
p1p2_df[which.min(test_ms),][1] #0.8153958	
ggplot(data=p1p2_df) +
  geom_line(aes(x=cost_vals,y=train_ms,col='train_ms')) +
  geom_line(aes(x=cost_vals,y=kfold_ms,col='kfold_ms')) +
  geom_line(aes(x=cost_vals,y=test_ms,col='test_ms')) +
  scale_colour_manual(name="Legend",
                      values=c("red","blue","green")) +
  ylab('error estimates') +
  theme_bw()
```
1) In the test set, the value of cost with lowest error is 0.815. In the training set, the value of cost with lowest error is 1902. We can see that the training set prefers a high cost, which is much higher than what the test set prefers. This is not surprising because the training error being 0 signals overfitting, while a small cost gives a model more flexibility and thus generalizability. 

2) Kfold CV error is much closer to training error than test error. This is because the SVM only chooses a few observations to be part of its support vectors, and those few observations are very likely to be in the training folds, therefore giving very similar models as the model created based on whole training set. And the "test" set that are not in the folds are likely not within the boundries and thus not misclassified, which makes cv error similar to 0, which is the training error in our perfectly separated case. However, the decision boundary and margins based on support vector found in training dataset are less likely to fit the real test set well, leading to much higher misclassification rates. In addition, our predicted variable, class, is a binary variable with only 1 or -1 values. This may cause our estimation of loss to be inaccurate.


## Problem 2: Some Data on Cars
```{r}
#data_prep
set.seed(1234)
car_test_factored<-car_test
car_test_factored$mpg<-factor(car_test$mpg)
car_train_factored<-car_train
car_train_factored$mpg<-factor(car_train$mpg)
```

### multinomial logistic regression 
```{r}
multiLogistic_classifier<- multinom(mpg ~ ., data=car_train_factored, trace=FALSE)
# 1) training misclassification rate
multiLog_pred<-predict(multiLogistic_classifier, newdata =car_train_factored)
multiLog_train_ms<- mean(multiLog_pred != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
  multiLogMod<- multinom(mpg ~ ., data=car_train_factored[folds[[i]],], trace=FALSE)
  preds <- predict(multiLogMod, newdata =car_train_factored[-folds[[i]],])
  #Proportion Misclassified
  kf_misclass[i] <- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
multiLog_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
multiLog_pred<-predict(multiLogistic_classifier, newdata =car_test_factored)
multiLog_test_ms<- mean(multiLog_pred != car_test_factored$mpg)
```

### QDA
```{r}
QDA_classifier<-qda(mpg ~ ., data=car_train_factored)
# 1) training misclassification rate
QDA_pred<- predict(QDA_classifier,newdata=car_train_factored)$class
QDA_train_ms<- mean(QDA_pred != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
  QDAMod<- qda(mpg ~ ., data=car_train_factored[folds[[i]],])
  preds <- predict(QDAMod, newdata =car_train_factored[-folds[[i]],])$class
  #Proportion Misclassified
  kf_misclass[i] <- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
QDA_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
QDA_pred<- predict(QDA_classifier,newdata=car_test_factored)$class
QDA_test_ms<- mean(QDA_pred != car_test_factored$mpg)
```

### KNN
```{r}
# data prep
car_train_x<- as.matrix(car_train[,-1])
car_train_y<-car_train_factored$mpg
car_test_x<- as.matrix(car_test[,-1])
car_test_y<-car_test_factored$mpg
# find the optimal k neighbor
prob_cv_err<-c()
misclass_cv_err<-c()
k_test<- seq(1,200)
for (i in 1:length(k_test)){
  # TRUE for prob of misclassification
  knn_classifier_temp<- knn.cv(train=car_train_x,cl=car_train_y,k=k_test[i],prob=TRUE)
  # Misclass rate
  misclass_cv_err[i] <- mean(knn_classifier_temp != car_train_y)
  #Prob rate for binary output
  probs <- attr(knn_classifier_temp, "prob")# return prob of misclassification
  correct_class <- as.numeric(knn_classifier_temp == car_train_y)# 1 if the same
  prob_correct <- (correct_class*probs) + ((1 - correct_class)*(1 - probs))
  prob_cv_err[i] <- mean(1 - prob_correct)
}
plot(k_test, misclass_cv_err, type = "l", lwd = 3, xlab = "Number of Neighbors", ylab = "Loss")
lines(k_test, prob_cv_err, col = "red", lwd = 3)
legend("bottomright", c("Misclass","Prob. Incorrect"), col = c("black","red"), lwd = c(3,3))
df<-data.frame(n=k_test,probError=prob_cv_err,misclassRate=misclass_cv_err)
optimal_n<-df[which.min(df$misclassRate),]$n # go with n=13
# finalized model
knn_classifier<- knn.cv(train=car_train_x,cl=car_train_y,k=optimal_n,prob=TRUE)
# 1) training misclassification rate
knn_train_ms<-mean(knn_classifier != car_train_y) 
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
  knnMod<- knn(train=car_train_factored[folds[[i]],][,-1],
               test=car_train_factored[-folds[[i]],][,-1],
               cl=car_train_factored[folds[[2]],]$mpg,k=optimal_n,prob=TRUE)
  kf_misclass[i]<-mean(knnMod != car_train_factored[-folds[[i]],]$mpg)
}
knn_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
knn_test_Mod<- knn(train=car_train_x,test=car_test_x,cl=car_train_y,k=optimal_n)
knn_test_ms<-mean(knn_test_Mod != car_test_y) 
```

### Support Vector Classifier w/ Linear Separator
```{r}
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
  #Train the SVM
  svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "vanilladot", 
                C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
  #Extract the 5fold CV est and store
  temp_p2 <- kernlab::cross(svm1)
  if (temp_p2<optimal_epe){
    optimal_epe<-temp_p2
    optimal_cost<-cost_vals[i]
  }
}
# finalized model
linearSVM_classfier <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "vanilladot", 
             C = optimal_cost, cross = 5, prob.model = TRUE, kpar = list())
# 1) training misclassification rate
train_preds_class <- predict(linearSVM_classfier, newdata = car_train_factored, type = "response")
linearSVM_train_ms<- mean(train_preds_class != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
  linearSVM_Mod <- ksvm(mpg ~ . , data = car_train_factored[folds[[i]],], type = "C-svc", 
                        kernel = "vanilladot",C = optimal_cost, cross = 5, 
                        prob.model = TRUE, kpar = list())
  preds <- predict(linearSVM_Mod, newdata = car_train_factored[-folds[[i]],], 
                              type = "response")
  kf_misclass[i]<- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
linearSVM_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(linearSVM_classfier, newdata = car_test_factored, type = "response")
linearSVM_test_ms<- mean(test_preds_class != car_test_factored$mpg)
```

### Support Vector Machine w/ Polynomial Kernel
```{r}
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for (j in 2:3){
  for(i in 1:length(cost_vals)){
    #Train the SVM
    svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot", 
                 kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
    #Extract the 5fold CV est and store
    temp_p2 <- kernlab::cross(svm1)
    if (temp_p2<optimal_epe){
      optimal_epe<-temp_p2
      optimal_cost<-cost_vals[i]
      optimal_degree<-j
    }
  }
}
# finalized model
polySVM_classfier <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot", 
             kpar = list(degree = optimal_degree), C = optimal_cost, cross = 5, prob.model = TRUE)
# 1) training misclassification rate
train_preds_class <- predict(polySVM_classfier, newdata = car_train_factored, type = "response")
polySVM_train_ms<- mean(train_preds_class != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
  polySVM_Mod <- ksvm(mpg ~ . , data = car_train_factored[folds[[i]],], type = "C-svc", 
                             kernel = "polydot", kpar = list(degree = optimal_degree), 
                             C = optimal_cost, cross = 5, prob.model = TRUE)
  preds <- predict(polySVM_Mod, newdata = car_train_factored[-folds[[i]],], 
                              type = "response")
   kf_misclass[i]<- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
polySVM_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(polySVM_classfier, newdata = car_test_factored, type = "response")
polySVM_test_ms<- mean(test_preds_class != car_test_factored$mpg)
```

### Support Vector Machine w/ Radial Kernel
```{r}
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
  #Train the SVM
  svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "rbfdot", 
                C = cost_vals[i], cross = 5, prob.model = TRUE)
  #Extract the 5fold CV est and store
  temp_p2 <- kernlab::cross(svm1)
  if (temp_p2<optimal_epe){
    optimal_epe<-temp_p2
    optimal_cost<-cost_vals[i]
  }
}
# finalized model
radialSVM_classfier <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "rbfdot", 
             C = optimal_cost, cross = 5, prob.model = TRUE)
# 1) training misclassification rate
train_preds_class <- predict(radialSVM_classfier, newdata = car_train_factored, type = "response")
radialSVM_train_ms<- mean(train_preds_class != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
  radialSVM_Mod <- ksvm(mpg ~ . , data = car_train_factored[folds[[i]],], type = "C-svc", 
                        kernel = "rbfdot",C = optimal_cost, cross = 5, 
                        prob.model = TRUE)
  preds <- predict(radialSVM_Mod, newdata = car_train_factored[-folds[[i]],], 
                              type = "response")
  kf_misclass[i]<- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
radialSVM_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(radialSVM_classfier, newdata = car_test_factored, type = "response")
radialSVM_test_ms<- mean(test_preds_class != car_test_factored$mpg)
```

### Classification Random Forest
```{r}
optimal_varNum<-0
optimal_oob<-100000000
for (i in 2:8){
  rfMod<-ranger(mpg ~ ., data = car_train_factored, num.trees = 1000, importance = "permutation", 
                mtry = i, classification = TRUE)
  temp_oob<-rfMod$prediction.error
  if (temp_oob < optimal_oob){
    optimal_oob<-temp_oob
    optimal_varNum<-i
  }
}
# finalized model
rf_classfier<-ranger(mpg ~ ., data = car_train_factored, num.trees = 1000, importance = "permutation", 
                     mtry = optimal_varNum, classification = TRUE)
# 1) training misclassification rate
train_preds_class <- predict(rf_classfier, data = car_train_factored)$predictions
rf_train_ms<- mean(train_preds_class != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
  rf_Mod<-ranger(mpg ~ ., data = car_train_factored[folds[[i]],], num.trees = 1000, 
                 importance = "permutation",mtry = optimal_varNum, classification = TRUE)
  preds <- predict(rf_Mod, data = car_train_factored[-folds[[i]],])$predictions
  kf_misclass[i]<- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
rf_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(rf_classfier, data = car_test_factored)$predictions
rf_test_ms<- mean(test_preds_class != car_test_factored$mpg)
```
### Summary
```{r}
model_list<-c("multiLogistic","QDA","KNN","linearSVC","polynomialSVM","radialSVM","randomForest")
train_ms_list<-c(multiLog_train_ms,QDA_train_ms,knn_train_ms,linearSVM_train_ms,polySVM_train_ms,radialSVM_train_ms,rf_train_ms)
cv_ms_list<-c(multiLog_kfold_ms,QDA_kfold_ms,knn_kfold_ms,linearSVM_kfold_ms,polySVM_kfold_ms,radialSVM_kfold_ms,rf_kfold_ms)
test_ms_list<-c(multiLog_test_ms,QDA_test_ms,knn_test_ms,linearSVM_test_ms,polySVM_test_ms,radialSVM_test_ms,rf_test_ms)
data.frame(model=model_list,trainingMSrate=train_ms_list,cvMSrate=cv_ms_list,testMSrate=test_ms_list)
```
From the result above, we can see that KNN classification is the worest model across all misclassification rates. Polynomial SVM wins in cv misclassification rate and random forest wins in both traing misclassification rate and test set misclassification rate.Though there are winners in each misclassification rate, the differences among the classification model are not larage overall. We do notice that there is a 0 training misclassification rate for random forest classification model. It's rare but technically possible for us to have 0 misclassification, but one potential explanation for this situation is that we are working with a small sample set with 0/1 output to predict on. It's possible that all classification models are somewhat "over" perform since we only have two classes on the output side. 