knn_classifier<- knn.cv(train=car_train_x,cl=car_train_y,k=optimal_n,prob=TRUE)
# 1) training misclassification rate
knn_train_ms<-mean(knn_classifier != car_train_y)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
knnMod<- knn(train=car_train_factored[folds[[i]],][,-1],
test=car_train_factored[-folds[[i]],][,-1],
cl=car_train_factored[folds[[2]],]$mpg,k=optimal_n,prob=TRUE)
kf_misclass[i]<-mean(knnMod != car_train_factored[-folds[[i]],]$mpg)
}
knn_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
knn_test_Mod<- knn(train=car_train_x,test=car_test_x,cl=car_train_y,k=optimal_n)
knn_test_ms<-mean(knn_test_Mod != car_test_y)
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "vanilladot",
C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
#Extract the 5fold CV est and store
temp_p2 <- kernlab::cross(svm1)
if (temp_p2<optimal_epe){
optimal_epe<-temp_p2
optimal_cost<-cost_vals[i]
}
}
# finalized model
linearSVM_classfier <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "vanilladot",
C = optimal_cost, cross = 5, prob.model = TRUE, kpar = list())
# 1) training misclassification rate
train_preds_class <- predict(linearSVM_classfier, newdata = car_train_factored, type = "response")
linearSVM_train_ms<- mean(train_preds_class != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
linearSVM_Mod <- ksvm(mpg ~ . , data = car_train_factored[folds[[i]],], type = "C-svc",
kernel = "vanilladot",C = optimal_cost, cross = 5,
prob.model = TRUE, kpar = list())
preds <- predict(linearSVM_Mod, newdata = car_train_factored[-folds[[i]],],
type = "response")
kf_misclass[i]<- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
linearSVM_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(linearSVM_classfier, newdata = car_test_factored, type = "response")
linearSVM_test_ms<- mean(test_preds_class != car_test_factored$mpg)
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for (j in 2:3){
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p2 <- kernlab::cross(svm1)
if (temp_p2<optimal_epe){
optimal_epe<-temp_p2
optimal_cost<-cost_vals[i]
optimal_degree<-j
}
}
}
# finalized model
polySVM_classfier <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = optimal_degree), C = optimal_cost, cross = 5, prob.model = TRUE)
# 1) training misclassification rate
train_preds_class <- predict(polySVM_classfier, newdata = car_train_factored, type = "response")
polySVM_train_ms<- mean(train_preds_class != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
polySVM_Mod <- ksvm(mpg ~ . , data = car_train_factored[folds[[i]],], type = "C-svc",
kernel = "polydot", kpar = list(degree = optimal_degree),
C = optimal_cost, cross = 5, prob.model = TRUE)
preds <- predict(polySVM_Mod, newdata = car_train_factored[-folds[[i]],],
type = "response")
kf_misclass[i]<- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
polySVM_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(polySVM_classfier, newdata = car_test_factored, type = "response")
polySVM_test_ms<- mean(test_preds_class != car_test_factored$mpg)
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 30))
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "rbfdot",
C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p2 <- kernlab::cross(svm1)
if (temp_p2<optimal_epe){
optimal_epe<-temp_p2
optimal_cost<-cost_vals[i]
}
}
# finalized model
radialSVM_classfier <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "rbfdot",
C = optimal_cost, cross = 5, prob.model = TRUE)
# 1) training misclassification rate
train_preds_class <- predict(radialSVM_classfier, newdata = car_train_factored, type = "response")
radialSVM_train_ms<- mean(train_preds_class != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
radialSVM_Mod <- ksvm(mpg ~ . , data = car_train_factored[folds[[i]],], type = "C-svc",
kernel = "rbfdot",C = optimal_cost, cross = 5,
prob.model = TRUE)
preds <- predict(radialSVM_Mod, newdata = car_train_factored[-folds[[i]],],
type = "response")
kf_misclass[i]<- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
radialSVM_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(radialSVM_classfier, newdata = car_test_factored, type = "response")
radialSVM_test_ms<- mean(test_preds_class != car_test_factored$mpg)
optimal_varNum<-0
optimal_oob<-100000000
for (i in 2:8){
rfMod<-ranger(mpg ~ ., data = car_train_factored, num.trees = 1000, importance = "permutation",
mtry = i, classification = TRUE)
temp_oob<-rfMod$prediction.error
if (temp_oob < optimal_oob){
optimal_oob<-temp_oob
optimal_varNum<-i
}
}
# finalized model
rf_classfier<-ranger(mpg ~ ., data = car_train_factored, num.trees = 1000, importance = "permutation",
mtry = optimal_varNum, classification = TRUE)
# 1) training misclassification rate
train_preds_class <- predict(rf_classfier, data = car_train_factored)$predictions
rf_train_ms<- mean(train_preds_class != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
rf_Mod<-ranger(mpg ~ ., data = car_train_factored[folds[[i]],], num.trees = 1000,
importance = "permutation",mtry = optimal_varNum, classification = TRUE)
preds <- predict(rf_Mod, data = car_train_factored[-folds[[i]],])$predictions
kf_misclass[i]<- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
rf_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(rf_classfier, data = car_test_factored)$predictions
rf_test_ms<- mean(test_preds_class != car_test_factored$mpg)
model_list<-c("multiLogistic","QDA","KNN","linearSVC","polynomialSVM","radialSVM","randomForest")
train_ms_list<-c(multiLog_train_ms,QDA_train_ms,knn_train_ms,linearSVM_train_ms,polySVM_train_ms,radialSVM_train_ms,rf_train_ms)
cv_ms_list<-c(multiLog_kfold_ms,QDA_kfold_ms,knn_kfold_ms,linearSVM_kfold_ms,polySVM_kfold_ms,radialSVM_kfold_ms,rf_kfold_ms)
test_ms_list<-c(multiLog_test_ms,QDA_test_ms,knn_test_ms,linearSVM_test_ms,polySVM_test_ms,radialSVM_test_ms,rf_test_ms)
data.frame(model=model_list,trainingMSrate=train_ms_list,cvMSrate=cv_ms_list,testMSrate=test_ms_list)
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
install.packages("kernlab")
install.packages("kernlab")
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
# add
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# data read-in
car_train <- read.csv('carsTrain.csv')
car_test <- read.csv('carsTest.csv')
svm_train <- read.csv('simSVMTrain.csv')
svm_test<-read.csv("simSVMTestF.csv")
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
# add
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# data read-in
car_train <- read.csv('carsTrain.csv')
car_test <- read.csv('carsTest.csv')
svm_train <- read.csv('simSVMTrain.csv')
svm_test<-read.csv("simSVMTestF.csv")
library(ggplot2)
library(tidyverse)
library(data.table)
library(kernlab)
# add
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
library(ranger)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# data read-in
car_train <- read.csv('carsTrain.csv')
car_test <- read.csv('carsTest.csv')
svm_train <- read.csv('simSVMTrain.csv')
svm_test<-read.csv("simSVMTestF.csv")
svm_m1 <- ksvm(class ~ X1 + X2 , data = svm_train, type = "C-svc", kernel = "vanilladot",
C = 10000, cross = 5, prob.model = TRUE)
neg_intercept <- b(svm_m1)
svobs <- svm_train[alphaindex(svm_m1)[[1]],] #support vector observations
beta1 <- sum(svobs[,'X1']*coef(svm_m1)[[1]])
beta2 <- sum(svobs[,'X2']*coef(svm_m1)[[1]])
#decision boundary
x1 <- c(0,0.2)
x2 <- c((-neg_intercept)/beta2 - beta1/beta2*0, (-neg_intercept)/beta2 - beta1/beta2*0.2)
p1p1_df1 <- data.frame(x1,x2)
#supporting vector margins
x1 <- c(0,0.2)
x2 <- c((1-neg_intercept)/beta2 - beta1/beta2*0, (1-neg_intercept)/beta2 - beta1/beta2*0.2)
p1p1_df2 <- data.frame(x1,x2)
x1 <- c(0,0.2)
x2 <- c((-1-neg_intercept)/beta2 - beta1/beta2*0, (-1-neg_intercept)/beta2 - beta1/beta2*0.2)
p1p1_df3 <- data.frame(x1,x2)
View(svobs)
alphaindex(svm_m1)
View(svm_train)
View(svm_test)
coef(svm_m1)
coef(svm_m1)[[1]]
View(svm_train)
reg_line1 <- lm(data=p1p1_df1, x2 ~ x1)
reg_line2 <- lm(data=p1p1_df2, x2 ~ x1)
reg_line3 <- lm(data=p1p1_df3, x2 ~ x1)
plot(svm_train$X1,svm_train$X2, col = as.numeric(svm_train$class) + 3, pch = "+", xlab = "X1", ylab = "X2")
abline(reg_line1, col='black')
abline(reg_line2, col='green')
abline(reg_line3, col='green')
svm_train$signed_dist <- svm_train$class*(-neg_intercept+beta1*svm_train$X1+beta2*svm_train$X2)
svm_train %>% filter(signed_dist <= 1)
svm_train$signed_dist <- svm_train$class*(-neg_intercept+beta1*svm_train$X1+beta2*svm_train$X2)
svm_train %>% filter(signed_dist < 1)
linearSVM_classfier <- ksvm(class ~ X1+X2, data = svm_train, type = "C-svc", kernel = "vanilladot",
C = 10000, cross = 5, prob.model = TRUE)
# 1) training misclassification rate
train_preds_class <- predict(linearSVM_classfier, newdata = svm_train, type = "response")
linearSVM_train_ms<- mean(train_preds_class != svm_train$class)
train_preds_class
View(svm_train)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(svm_train)), k = 10, type = "basic")
View(folds)
folds[["Fold02"]]
linearSVM_classfier <- ksvm(class ~ X1+X2, data = svm_train, type = "C-svc", kernel = "vanilladot",
C = 10000, cross = 5, prob.model = TRUE)
# 1) training misclassification rate
train_preds_class <- predict(linearSVM_classfier, newdata = svm_train, type = "response")
linearSVM_train_ms<- mean(train_preds_class != svm_train$class)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(svm_train)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
linearSVM_Mod <- ksvm(class ~ X1+X2 , data = svm_train[folds[[i]],], type = "C-svc",
kernel = "vanilladot",C = 10000, cross = 5,
prob.model = TRUE)
preds <- predict(linearSVM_Mod, newdata = svm_train[-folds[[i]],],
type = "response")
kf_misclass[i]<- mean(preds != svm_train[-folds[[i]],]$class)
}
linearSVM_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(linearSVM_classfier, newdata = svm_test, type = "response")
linearSVM_test_ms<- mean(test_preds_class != svm_test$class)
#conclusion
data.frame('name' = c('linearSVM_train_ms','linearSVM_kfold_ms','linearSVM_test_ms'),
'estimate' = c(linearSVM_train_ms,linearSVM_kfold_ms,linearSVM_test_ms))
linearSVM_Mod <- ksvm(class ~ X1+X2 , data = svm_train, type = "C-svc",
kernel = "vanilladot",C = 10000, cross = 5,
prob.model = TRUE)
cross(linearSVM_Mod)
#?
predict(linearSVM_classfier, newdata = svm_test, type = "probabilities")
#conclusion
data.frame('name' = c('linearSVM_train_ms','linearSVM_kfold_ms','linearSVM_test_ms'),
'estimate' = c(linearSVM_train_ms,linearSVM_kfold_ms,linearSVM_test_ms))
cross(linearSVM_Mod)
#conclusion
data.frame('name' = c('linearSVM_train_ms','linearSVM_kfold_ms','linearSVM_test_ms'),
'estimate' = c(linearSVM_train_ms,linearSVM_kfold_ms,linearSVM_test_ms))
train_ms <- c()
kfold_ms <- c()
test_ms <- c()
#Create a vector of potential C values
cost_vals <- exp(seq(-10,10,length = 50))
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(class ~ X1+X2 , data = svm_train, type = "C-svc", kernel = "vanilladot",
C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
# 1) training misclassification rate
train_preds_class <- predict(svm1, newdata = svm_train, type = "response")
train_ms[i] <- mean(train_preds_class != svm_train$class)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(svm_train)), k = 10, type = "basic")
kf_misclass <- c()
for(j in 1:length(folds)){
linearSVM_Mod <- ksvm(class ~ X1+X2 , data = svm_train[folds[[j]],], type = "C-svc",
kernel = "vanilladot",C = cost_vals[j], cross = 5,
prob.model = TRUE, kpar = list())
preds <- predict(linearSVM_Mod, newdata = svm_train[-folds[[j]],],
type = "response")
kf_misclass[j]<- mean(preds != svm_train[-folds[[j]],]$class)
}
kfold_ms[i] <- mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(svm1, newdata = svm_test, type = "response")
test_ms[i] <- mean(test_preds_class != svm_test$class)
}
p1p2_df <- data.frame(cost_vals,train_ms,kfold_ms,test_ms)
ggplot(data=p1p2_df) +
geom_line(aes(x=cost_vals,y=train_ms,col='train_ms')) +
geom_line(aes(x=cost_vals,y=kfold_ms,col='kfold_ms')) +
geom_line(aes(x=cost_vals,y=test_ms,col='test_ms')) +
scale_colour_manual(name="Legend",
values=c("red","blue","green")) +
ylab('error estimates') +
theme_bw()
#?
kfold_ms <- c()
train_ms <- c()
kfold_ms <- c()
test_ms <- c()
#Create a vector of potential C values
cost_vals <- exp(seq(-10,10,length = 50))
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(class ~ X1+X2 , data = svm_train, type = "C-svc", kernel = "vanilladot",
C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
# 1) training misclassification rate
train_preds_class <- predict(svm1, newdata = svm_train, type = "response")
train_ms[i] <- mean(train_preds_class != svm_train$class)
# 2) k-fold cv misclassification rate
#folds <- create_folds(seq(1,nrow(svm_train)), k = 10, type = "basic")
#kf_misclass <- c()
#for(j in 1:length(folds)){
# linearSVM_Mod <- ksvm(class ~ X1+X2 , data = svm_train[folds[[j]],], type = "C-svc",
#                      kernel = "vanilladot",C = cost_vals[j], cross = 5,
#                     prob.model = TRUE, kpar = list())
#  preds <- predict(linearSVM_Mod, newdata = svm_train[-folds[[j]],],
#                             type = "response")
#  kf_misclass[j]<- mean(preds != svm_train[-folds[[j]],]$class)
#}
#kfold_ms[i] <- mean(kf_misclass)
#?
kfold_ms[i] <- cross(svm1)
#
# 3) test set misclassification rate
test_preds_class <- predict(svm1, newdata = svm_test, type = "response")
test_ms[i] <- mean(test_preds_class != svm_test$class)
}
p1p2_df <- data.frame(cost_vals,train_ms,kfold_ms,test_ms)
ggplot(data=p1p2_df) +
geom_line(aes(x=cost_vals,y=train_ms,col='train_ms')) +
geom_line(aes(x=cost_vals,y=kfold_ms,col='kfold_ms')) +
geom_line(aes(x=cost_vals,y=test_ms,col='test_ms')) +
scale_colour_manual(name="Legend",
values=c("red","blue","green")) +
ylab('error estimates') +
theme_bw()
train_ms <- c()
kfold_ms <- c()
test_ms <- c()
#Create a vector of potential C values
cost_vals <- exp(seq(-10,10,length = 50))
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(class ~ X1+X2 , data = svm_train, type = "C-svc", kernel = "vanilladot",
C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
# 1) training misclassification rate
train_preds_class <- predict(svm1, newdata = svm_train, type = "response")
train_ms[i] <- mean(train_preds_class != svm_train$class)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(svm_train)), k = 10, type = "basic")
kf_misclass <- c()
for(j in 1:length(folds)){
linearSVM_Mod <- ksvm(class ~ X1+X2 , data = svm_train[folds[[j]],], type = "C-svc",
kernel = "vanilladot",C = cost_vals[j], cross = 5,
prob.model = TRUE, kpar = list())
preds <- predict(linearSVM_Mod, newdata = svm_train[-folds[[j]],],
type = "response")
kf_misclass[j]<- mean(preds != svm_train[-folds[[j]],]$class)
}
kfold_ms[i] <- mean(kf_misclass)
#?
#kfold_ms[i] <- cross(svm1) #上面这种方法kfoldEPE算出来比test大
#
# 3) test set misclassification rate
test_preds_class <- predict(svm1, newdata = svm_test, type = "response")
test_ms[i] <- mean(test_preds_class != svm_test$class)
}
p1p2_df <- data.frame(cost_vals,train_ms,kfold_ms,test_ms)
ggplot(data=p1p2_df) +
geom_line(aes(x=cost_vals,y=train_ms,col='train_ms')) +
geom_line(aes(x=cost_vals,y=kfold_ms,col='kfold_ms')) +
geom_line(aes(x=cost_vals,y=test_ms,col='test_ms')) +
scale_colour_manual(name="Legend",
values=c("red","blue","green")) +
ylab('error estimates') +
theme_bw()
p1p2_df <- data.frame(cost_vals,train_ms,kfold_ms,test_ms)
ggplot(data=p1p2_df) +
geom_line(aes(x=cost_vals,y=train_ms,col='train_ms')) +
geom_line(aes(x=cost_vals,y=kfold_ms,col='kfold_ms')) +
geom_line(aes(x=cost_vals,y=test_ms,col='test_ms')) +
scale_colour_manual(name="Legend",
values=c("red","blue","green")) +
ylab('error estimates') +
theme_bw()
train_ms <- c()
kfold_ms <- c()
test_ms <- c()
#Create a vector of potential C values
cost_vals <- exp(seq(-10,10,length = 50))
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(class ~ X1+X2 , data = svm_train, type = "C-svc", kernel = "vanilladot",
C = cost_vals[i], cross = 5, prob.model = TRUE, kpar = list())
# 1) training misclassification rate
train_preds_class <- predict(svm1, newdata = svm_train, type = "response")
train_ms[i] <- mean(train_preds_class != svm_train$class)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(svm_train)), k = 10, type = "basic")
kf_misclass <- c()
for(j in 1:length(folds)){
linearSVM_Mod <- ksvm(class ~ X1+X2 , data = svm_train[folds[[j]],], type = "C-svc",
kernel = "vanilladot",C = cost_vals[i], cross = 5,
prob.model = TRUE, kpar = list())
preds <- predict(linearSVM_Mod, newdata = svm_train[-folds[[j]],],
type = "response")
kf_misclass[j]<- mean(preds != svm_train[-folds[[j]],]$class)
}
kfold_ms[i] <- mean(kf_misclass)
#?
#kfold_ms[i] <- cross(svm1) #上面这种方法kfoldEPE算出来比test大
#
# 3) test set misclassification rate
test_preds_class <- predict(svm1, newdata = svm_test, type = "response")
test_ms[i] <- mean(test_preds_class != svm_test$class)
}
p1p2_df <- data.frame(cost_vals,train_ms,kfold_ms,test_ms)
ggplot(data=p1p2_df) +
geom_line(aes(x=cost_vals,y=train_ms,col='train_ms')) +
geom_line(aes(x=cost_vals,y=kfold_ms,col='kfold_ms')) +
geom_line(aes(x=cost_vals,y=test_ms,col='test_ms')) +
scale_colour_manual(name="Legend",
values=c("red","blue","green")) +
ylab('error estimates') +
theme_bw()
#data_prep
set.seed(1234)
car_test_factored<-car_test
car_test_factored$mpg<-factor(car_test$mpg)
car_train_factored<-car_train
car_train_factored$mpg<-factor(car_train$mpg)
linearSVM_classfier <- ksvm(class ~ X1+X2, data = svm_train, type = "C-svc", kernel = "vanilladot",
C = 10000, cross = 5, prob.model = TRUE)
# 1) training misclassification rate
train_preds_class <- predict(linearSVM_classfier, newdata = svm_train, type = "response")
linearSVM_train_ms<- mean(train_preds_class != svm_train$class)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(svm_train)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
linearSVM_Mod <- ksvm(class ~ X1+X2 , data = svm_train[folds[[i]],], type = "C-svc",
kernel = "vanilladot",C = 10000, cross = 5,
prob.model = TRUE)
preds <- predict(linearSVM_Mod, newdata = svm_train[-folds[[i]],],
type = "response")
kf_misclass[i]<- mean(preds != svm_train[-folds[[i]],]$class)
}
linearSVM_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
test_preds_class <- predict(linearSVM_classfier, newdata = svm_test, type = "response")
linearSVM_test_ms<- mean(test_preds_class != svm_test$class)
#conclusion
data.frame('name' = c('linearSVM_train_ms','linearSVM_kfold_ms','linearSVM_test_ms'),
'estimate' = c(linearSVM_train_ms,linearSVM_kfold_ms,linearSVM_test_ms))
print(p1p2_df)
p1p2_df[which.min(train_ms),]
print(p1p2_df)
View(p1p2_df)
p1p2_df[which.min(train_ms),]
p1p2_df[which.min(train_ms),][1]
p1p2_df[which.min(test_ms),][1]
p1p2_df[which.min(train_ms),][1]
View(svm_train)
View(car_train)
