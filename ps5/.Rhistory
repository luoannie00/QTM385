folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
library(ggplot2)
library(data.table)
library(tidyverse)
library(glmnet)
library(MASS)
library(splitTools)
# add
library(naivebayes)
library(nnet)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# data read in
wine_train <- read.csv('wineTrain.csv')
library(ggplot2)
library(data.table)
library(kernlab)
# add
library(MASS)
library(nnet)
library(FNN)
library(splitTools)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# 2) k-fold cv misclassification rate
set.seed(1234)
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
multiLogMod<- multinom(mpg ~ ., data=car_train_factored[folds[[i]],])
preds <- predict(multiLogMod, newdata =car_train_factored[-folds[[i]],])
#Proportion Misclassified
kf_misclass[i] <- mean(preds != car_train_factored$mpg)
}
multiLog_pred_ms2<-mean(kf_misclass)
multiLog_pred_ms2
multiLog_pred_ms3
preds
multiLogistic_classifier<- multinom(mpg ~ ., data=car_train_factored)
# 1) training misclassification rate
multiLog_pred<-predict(multiLogistic_classifier, newdata =car_train_factored)
multiLog_pred_ms1<- mean(multiLog_pred != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
set.seed(1234)
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
multiLogMod<- multinom(mpg ~ ., data=car_train_factored[folds[[i]],])
preds <- predict(multiLogMod, newdata =car_train_factored[-folds[[i]],])
#Proportion Misclassified
kf_misclass[i] <- mean(preds != car_train_factored$mpg)
}
multiLog_pred_ms2<-mean(kf_misclass)
# 3) test set misclassification rate
multiLog_pred<-predict(multiLogistic_classifier, newdata =car_test_factored)
multiLog_pred_ms3<- mean(multiLog_pred != car_test_factored$mpg)
multiLog_pred_ms1
multiLog_pred_ms2
multiLog_pred_ms3
kf_misclass
set.seed(1234)
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 5, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
multiLogMod<- multinom(mpg ~ ., data=car_train_factored[folds[[i]],])
preds <- predict(multiLogMod, newdata =car_train_factored[-folds[[i]],])
#Proportion Misclassified
kf_misclass[i] <- mean(preds != car_train_factored$mpg)
}
nrow(car_train_factored)
set.seed(1234)
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 100, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
multiLogMod<- multinom(mpg ~ ., data=car_train_factored[folds[[i]],])
preds <- predict(multiLogMod, newdata =car_train_factored[-folds[[i]],])
#Proportion Misclassified
kf_misclass[i] <- mean(preds != car_train_factored$mpg)
}
multiLog_pred_ms2
multiLog_pred_ms2<-mean(kf_misclass)
multiLog_pred_ms2
set.seed(1234)
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 1, type = "basic")
set.seed(1234)
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 2, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
multiLogMod<- multinom(mpg ~ ., data=car_train_factored[folds[[i]],])
preds <- predict(multiLogMod, newdata =car_train_factored[-folds[[i]],])
#Proportion Misclassified
kf_misclass[i] <- mean(preds != car_train_factored$mpg)
}
multiLog_pred_ms2<-mean(kf_misclass)
multiLog_pred_ms2
multiLogistic_classifier<- multinom(mpg ~ ., data=car_train_factored)
# 1) training misclassification rate
multiLog_pred<-predict(multiLogistic_classifier, newdata =car_train_factored)
multiLog_pred_ms1<- mean(multiLog_pred != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
set.seed(1234)
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
multiLogMod<- multinom(mpg ~ ., data=car_train_factored[folds[[i]],])
preds <- predict(multiLogMod, newdata =car_train_factored[-folds[[i]],])
#Proportion Misclassified
kf_misclass[i] <- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
multiLog_pred_ms2<-mean(kf_misclass)
# 3) test set misclassification rate
multiLog_pred<-predict(multiLogistic_classifier, newdata =car_test_factored)
multiLog_pred_ms3<- mean(multiLog_pred != car_test_factored$mpg)
multiLog_pred_ms2
multiLog_pred_ms3
#data_prep
set.seed(1234)
car_test_factored<-car_test
car_test_factored$mpg<-factor(car_test$mpg)
car_train_factored<-car_train
car_train_factored$mpg<-factor(car_train$mpg)
multiLogistic_classifier<- multinom(mpg ~ ., data=car_train_factored)
# 1) training misclassification rate
multiLog_pred<-predict(multiLogistic_classifier, newdata =car_train_factored)
multiLog_pred_ms1<- mean(multiLog_pred != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
multiLogMod<- multinom(mpg ~ ., data=car_train_factored[folds[[i]],])
preds <- predict(multiLogMod, newdata =car_train_factored[-folds[[i]],])
#Proportion Misclassified
kf_misclass[i] <- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
multiLog_pred_ms2<-mean(kf_misclass)
# 3) test set misclassification rate
multiLog_pred<-predict(multiLogistic_classifier, newdata =car_test_factored)
multiLog_pred_ms3<- mean(multiLog_pred != car_test_factored$mpg)
# QDA
QDA_classifier<-qda(mpg ~ ., data=car_train)
predict(QDA_classifier,newdata=car_test)
multiLogistic_classifier<- multinom(mpg ~ ., data=car_train_factored)
# 1) training misclassification rate
multiLog_pred<-predict(multiLogistic_classifier, newdata =car_train_factored)
multiLog_train_ms<- mean(multiLog_pred != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
multiLogMod<- multinom(mpg ~ ., data=car_train_factored[folds[[i]],])
preds <- predict(multiLogMod, newdata =car_train_factored[-folds[[i]],])
#Proportion Misclassified
kf_misclass[i] <- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
multiLog_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
multiLog_pred<-predict(multiLogistic_classifier, newdata =car_test_factored)
multiLog_test_ms<- mean(multiLog_pred != car_test_factored$mpg)
# QDA
QDA_classifier<-qda(mpg ~ ., data=car_train_factored)
# 1) training misclassification rate
QDA_pred<- predict(QDA_classifier,newdata=car_train_factored)$class
QDA_train_ms<- mean(QDA_pred != car_train_factored$mpg)
QDA_train_ms
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
QDAMod<- qda(mpg ~ ., data=car_train_factored[folds[[i]],])
preds <- predict(QDAMod, newdata =car_train_factored[-folds[[i]],])$class
#Proportion Misclassified
kf_misclass[i] <- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
QDA_kfold_ms<-mean(kf_misclass)
QDA_kfold_ms
# QDA
QDA_classifier<-qda(mpg ~ ., data=car_train_factored)
# 1) training misclassification rate
QDA_pred<- predict(QDA_classifier,newdata=car_train_factored)$class
QDA_train_ms<- mean(QDA_pred != car_train_factored$mpg)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
QDAMod<- qda(mpg ~ ., data=car_train_factored[folds[[i]],])
preds <- predict(QDAMod, newdata =car_train_factored[-folds[[i]],])$class
#Proportion Misclassified
kf_misclass[i] <- mean(preds != car_train_factored[-folds[[i]],]$mpg)
}
QDA_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
QDA_pred<- predict(QDA_classifier,newdata=car_test_factored)$class
QDA_test_ms<- mean(QDA_pred != car_test_factored$mpg)
# KNN
# data prep
car_train_x<- as.matrix(car_train[,-1])
car_train_y<-car_train_factored$mpg
# output
knn_classifier<- knn.cv(train=car_train_x,cl=car_train_y,k=13,prob=TRUE)
# 1) training misclassification rate
knn_pred<-mean(knn_classifier != car_train_y)
knn_pred
# 2) k-fold cv misclassification rate
knn_classifier<- knn.cv(train=car_train_x,cl=car_train_y,k=13,prob=FALSE)
# output
knn_classifier<- knn.cv(train=car_train_x,cl=car_train_y,k=13,prob=TRUE)
# 1) training misclassification rate
knn_train_ms<-mean(knn_classifier != car_train_y)
# 2) k-fold cv misclassification rate
knn_kfold_ms<- knn.cv(train=car_train_x,cl=car_train_y,k=13,prob=FALSE)
knn_kfold_ms
# 2) k-fold cv misclassification rate
knn_kfold_ms<- knn.cv(train=car_train_x,cl=car_train_y,k=13,prob=FALSE,use.all = TRUE)
# 2) k-fold cv misclassification rate
knn_kfold_ms<- knn.cv(train=car_train_x,cl=car_train_y,k=13,prob=FALSE)
knn_kfold_ms
knn_classifier
knn_kfold_ms
knn_classifier
knn_kfold_ms
knn_classifier
knn_kfold_ms
knn_classifier
knn_kfold_ms
knn_classifier
knn_kfold_ms
# 2) k-fold cv misclassification rate
knn_pred<- predict(knn_train_ms,newdata=car_test_factored)
# output
knn_classifier<- knn(train=car_train_x,cl=car_train_y,k=13,prob=TRUE)
car_train_x<- as.matrix(car_train[,-1])
car_train_y<-car_train_factored$mpg
car_test_x<- as.matrix(car_test[,-1])
car_test_y<-car_test_factored$mpg
# 3) test set misclassification rate
knn_classifier<- knn.cv(train=car_train_x,test=car_test_x,cl=car_train_y,k=13,prob=TRUE)
# 3) test set misclassification rate
knn_classifier<- knn(train=car_train_x,test=car_test_x,cl=car_train_y,k=13)
knn_classifier
knn_test_Mod<- knn(train=car_train_x,test=car_test_x,cl=car_train_y,k=13)
knn_train_ms<-mean(knn_test_Mod != car_test_y)
knn_train_ms
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
car_train_factored[folds[[2]]
car_train_factored[folds[[2]],]
car_train_factored[folds[[2]],]
car_train_factored[folds[[2]],][,-1]
car_train_factored[-folds[[i]],][,-1]
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
knnMod<- knn(train=car_train_factored[folds[[i]],][,-1],
test=car_train_factored[-folds[[i]],][,-1],
cl=car_train_factored[folds[[2]],]$mpg,k=13,prob=TRUE)
knn_train_ms<-mean(knnMod != car_train_factored[-folds[[i]],]$mpg)
}
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
knnMod<- knn(train=car_train_factored[folds[[i]],][,-1],
test=car_train_factored[-folds[[i]],][,-1],
cl=car_train_factored[folds[[2]],]$mpg,k=13,prob=TRUE)
kf_misclass[i]<-mean(knnMod != car_train_factored[-folds[[i]],]$mpg)
}
knn_kfold_ms<-mean(kf_misclass)
knn_kfold_ms
knn_train_ms
knn_train_ms
# KNN
# data prep
car_train_x<- as.matrix(car_train[,-1])
car_train_y<-car_train_factored$mpg
car_test_x<- as.matrix(car_test[,-1])
car_test_y<-car_test_factored$mpg
# find the optimal k neighbor
prob_cv_err<-c()
misclass_cv_err<-c()
k_test<- c(seq(1,200))
for (i in 1:length(k_test)){
# TRUE for prob of misclassification
knn_classifier_temp<- knn.cv(train=car_train_x,cl=car_train_y,k=k_test[i],prob=TRUE)
# Misclass rate
misclass_cv_err[i] <- mean(knn_classifier_temp != car_train_y)
#Prob rate for binary output
probs <- attr(knn_classifier_temp, "prob")# return prob of misclassification
correct_class <- as.numeric(knn_classifier_temp == car_train_y)# 1 if the same
prob_correct <- (correct_class*probs) + ((1 - correct_class)*(1 - probs))
prob_cv_err[i] <- mean(1 - prob_correct)
}
df<-data.frame(n=k_test,probError=prob_cv_err,misclassRate=misclass_cv_err)
df[which.min(df$probError),]
df[which.min(df$misclassRate),] # go with n=13
# output
knn_classifier<- knn.cv(train=car_train_x,cl=car_train_y,k=13,prob=TRUE)
# 1) training misclassification rate
knn_train_ms<-mean(knn_classifier != car_train_y)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
knnMod<- knn(train=car_train_factored[folds[[i]],][,-1],
test=car_train_factored[-folds[[i]],][,-1],
cl=car_train_factored[folds[[2]],]$mpg,k=13,prob=TRUE)
kf_misclass[i]<-mean(knnMod != car_train_factored[-folds[[i]],]$mpg)
}
knn_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
knn_test_Mod<- knn(train=car_train_x,test=car_test_x,cl=car_train_y,k=13)
knn_test_ms<-mean(knn_test_Mod != car_test_y)
knn_test_ms
knn_kfold_ms
knn_train_ms
# KNN
# data prep
car_train_x<- as.matrix(car_train[,-1])
car_train_y<-car_train_factored$mpg
car_test_x<- as.matrix(car_test[,-1])
car_test_y<-car_test_factored$mpg
# find the optimal k neighbor
prob_cv_err<-c()
misclass_cv_err<-c()
k_test<- c(seq(1,200))
for (i in 1:length(k_test)){
# TRUE for prob of misclassification
knn_classifier_temp<- knn.cv(train=car_train_x,cl=car_train_y,k=k_test[i],prob=TRUE)
# Misclass rate
misclass_cv_err[i] <- mean(knn_classifier_temp != car_train_y)
#Prob rate for binary output
probs <- attr(knn_classifier_temp, "prob")# return prob of misclassification
correct_class <- as.numeric(knn_classifier_temp == car_train_y)# 1 if the same
prob_correct <- (correct_class*probs) + ((1 - correct_class)*(1 - probs))
prob_cv_err[i] <- mean(1 - prob_correct)
}
plot(k_test, misclass_cv_err, type = "l", lwd = 3, xlab = "Number of Neighbors", ylab = "Loss")
lines(k_test, prob_cv_err, col = "red", lwd = 3)
legend("bottomright", c("Misclass","Prob. Incorrect"), col = c("black","red"), lwd = c(3,3))
df<-data.frame(n=k_test,probError=prob_cv_err,misclassRate=misclass_cv_err)
df[which.min(df$misclassRate),] # go with n=13
# output
knn_classifier<- knn.cv(train=car_train_x,cl=car_train_y,k=13,prob=TRUE)
# 1) training misclassification rate
knn_train_ms<-mean(knn_classifier != car_train_y)
# 2) k-fold cv misclassification rate
folds <- create_folds(seq(1,nrow(car_train_factored)), k = 10, type = "basic")
kf_misclass <- c()
for(i in 1:length(folds)){
knnMod<- knn(train=car_train_factored[folds[[i]],][,-1],
test=car_train_factored[-folds[[i]],][,-1],
cl=car_train_factored[folds[[2]],]$mpg,k=13,prob=TRUE)
kf_misclass[i]<-mean(knnMod != car_train_factored[-folds[[i]],]$mpg)
}
knn_kfold_ms<-mean(kf_misclass)
# 3) test set misclassification rate
knn_test_Mod<- knn(train=car_train_x,test=car_test_x,cl=car_train_y,k=13)
knn_test_ms<-mean(knn_test_Mod != car_test_y)
library(e1071)
df[which.min(df$misclassRate),]
optimal_n<-df[which.min(df$misclassRate),]$n # go with n=13
optimal_n
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 20))
cost_vals
library(kernlab)
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 20))
cost_vals
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 20))
for (j in 2:3){
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p2 <- cross(svm1)
if (temp_p2<optimal_epe){
optimal_epe<-temp_p2
optimal_cost<-cost_vals[i]
optimal_degree<-j
}
}
}
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = 2), C = cost_vals[2], cross = 5, prob.model = TRUE)
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 20))
for (j in 2:3){
for(i in 1:length(cost_vals)){
#Train the SVM
print(i)
print(j)
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p2 <- cross(svm1)
if (temp_p2<optimal_epe){
optimal_epe<-temp_p2
optimal_cost<-cost_vals[i]
optimal_degree<-j
}
}
}
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 20))
for (j in 2:3){
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = 3), C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p2 <- cross(svm1)
if (temp_p2<optimal_epe){
optimal_epe<-temp_p2
optimal_cost<-cost_vals[i]
optimal_degree<-j
}
}
}
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 20))
for (j in 2:3){
for(i in 1:length(cost_vals)){
#Train the SVM
print(i)
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = 3), C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p2 <- cross(svm1)
if (temp_p2<optimal_epe){
optimal_epe<-temp_p2
optimal_cost<-cost_vals[i]
optimal_degree<-j
}
}
}
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = 3), C = cost_vals[4], cross = 5, prob.model = TRUE)
#Extract the
cross(svm1)
svm1
cross(svm1)
kernlab::cross(svm1)
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = 3), C = cost_vals[4], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p2 <- kernlab::cross(svm1)
temp_p2
svm1
#Extract the 5fold CV est and store
temp_p2 <- cross(svm1)
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 20))
for (j in 2:3){
for(i in 1:length(cost_vals)){
#Train the SVM
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p2 <- kernlab::cross(svm1)
if (temp_p2<optimal_epe){
optimal_epe<-temp_p2
optimal_cost<-cost_vals[i]
optimal_degree<-j
}
}
}
optimal_cost
optimal_degree
cost_vals
# to find optimal cost and degree
optimal_cost<-0
optimal_degree<-0
optimal_epe<-1000000
#Create a vector of potential C values
cost_vals <- exp(seq(-3,3,length = 20))
for (j in 2:3){
for(i in 1:length(cost_vals)){
#Train the SVM
print(i)
print(j)
svm1 <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = j), C = cost_vals[i], cross = 5, prob.model = TRUE)
#Extract the 5fold CV est and store
temp_p2 <- kernlab::cross(svm1)
if (temp_p2<optimal_epe){
optimal_epe<-temp_p2
optimal_cost<-cost_vals[i]
optimal_degree<-j
}
}
}
#Train the final model
multiSVM_classfier <- ksvm(mpg ~ . , data = car_train_factored, type = "C-svc", kernel = "polydot",
kpar = list(degree = optimal_degree), C = optimal_cost, cross = 5, prob.model = TRUE)
