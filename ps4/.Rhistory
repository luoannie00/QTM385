install.packages("splitTools")
install.packages("naivebayes")
#setwd('/Users/billyge/Desktop/Emory Fall 2022/QTM 385-4/problem sets/ps github/ps4')
setwd('/Users/annie/Desktop/QTM385/QTM350_Github/QTM350 Github/QTM385/ps4')
cancer_train <- read.csv('cancerTrain.csv')
cancer_test <- read.csv('cancerTest.csv')
wine_train <- read.csv('wineTrain.csv')
p2p1_df1 <- data.frame(matrix(nrow = 10, ncol = 10))
malignant_df <- cancer_train %>% filter(Malignant == 1)
library(ggplot2)
library(data.table)
library(tidyverse)
library(glmnet)
library(MASS)
library(splitTools)
library(naivebayes)
library(nnet)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
malignant_df <- cancer_train %>% filter(Malignant == 1)
View(cancer_train)
View(wine_train)
View(malignant_df)
malignant_prob <- nrow(malignant_df)/500
non_malignant_df <- cancer_train %>% filter(Malignant == 0)
non_malignant_prob <- nrow(non_malignant_df)/500
for (i in 1:10) {
for (j in 1:10) {
temp_df1 <- malignant_df %>% filter(UCellSize == i & SECellSize == j)
temp_df2 <- non_malignant_df %>% filter(UCellSize == i & SECellSize == j)
if (nrow(temp_df1) == 0) {
p2p1_df1[i,j] <- NA
next
}
result_numerator <- nrow(temp_df1)/nrow(malignant_df) * malignant_prob
result_denominator <- result_numerator + nrow(temp_df2)/nrow(non_malignant_df) * non_malignant_prob
p2p1_df1[i,j] <- result_numerator/result_denominator
}
}
p2p1_df1
p2p1_df2 <- data.frame(matrix(nrow = 10, ncol = 10))
for (i in 1:10) {
for (j in 1:10) {
if (is.na(p2p1_df1[i,j])) {
next
}
else if (p2p1_df1[i,j] > 0.5) {
p2p1_df2[i,j] <- 1
}
else if (p2p1_df1[i,j] < 0.5) {
p2p1_df2[i,j] <- 0
}
#how to deal with it???
else if (p2p1_df1[i,j] == 0.5) {
p2p1_df2[i,j] <- 1
}
}
}
p2p1_df2
#logistic regression classifier
p2p2_logistic <- glm(data=cancer_train,
formula = Malignant ~ UCellSize + SECellSize,
family = binomial(link='logit'))
UCellSize <- rep(seq(1,10,1),10)
SECellSize <- rep(seq(1,10,1),each=10)
p2p2_df <- data.frame(UCellSize, SECellSize)
p2p2_df$logistic <- predict(p2p2_logistic, type='response', newdata=p2p2_df)
p2p2_df1 <- data.frame(matrix(nrow = 10, ncol = 10))
for (i in 1:10) {
for (j in 1:10) {
temp <- p2p2_df %>% filter(UCellSize==i, SECellSize==j)
p2p2_df1[i,j] <- temp[1,'logistic']
}
}
p2p2_df1
p2p2_df2 <- data.frame(matrix(nrow = 10, ncol = 10))
for (i in 1:10) {
for (j in 1:10) {
if (p2p2_df1[i,j] > 0.5) {
p2p2_df2[i,j] <- 1
}
else {
p2p2_df2[i,j] <- 0
}
}
}
p2p2_df2
#QDA classifier
p2p2_x <- cbind(cancer_train$UCellSize, cancer_train$SECellSize)
p2p2_y <- cancer_train$Malignant
p2p2_qda <- qda(x=p2p2_x, grouping=p2p2_y, formula = Malignant ~ UCellSize + SECellSize)
p2p2_testx <- cbind(p2p2_df$UCellSize, p2p2_df$SECellSize)
p2p2_df$qda <- predict(p2p2_qda, newdata=p2p2_testx)$posterior[,2]
p2p2_df3 <- data.frame(matrix(nrow = 10, ncol = 10))
for (i in 1:10) {
for (j in 1:10) {
temp <- p2p2_df %>% filter(UCellSize==i, SECellSize==j)
p2p2_df3[i,j] <- temp[1,'qda']
}
}
p2p2_df3
p2p2_df4 <- data.frame(matrix(nrow = 10, ncol = 10))
for (i in 1:10) {
for (j in 1:10) {
if (p2p2_df3[i,j] > 0.5) {
p2p2_df4[i,j] <- 1
}
else {
p2p2_df4[i,j] <- 0
}
}
}
p2p2_df4
p2p3_misclass <- c()
p2p3_deviance <- c()
cancer_test['pred_fromp1_prob'] <- NA
cancer_test['pred_fromp1_binary'] <- NA
misclass_count <- 0
null_count <- 0
for (x in 1:nrow(cancer_test)) {
i <- cancer_test[x,'UCellSize']
j <- cancer_test[x,'SECellSize']
if (is.na(p2p1_df1[i,j])) {
null_count <- null_count + 1
next
}
else {
cancer_test[x,'pred_fromp1_prob'] <- p2p1_df1[i,j]
cancer_test[x,'pred_fromp1_binary'] <- p2p1_df2[i,j]
if (cancer_test[x,'pred_fromp1_binary'] != cancer_test[x,'Malignant']) {
misclass_count <- misclass_count + 1
}
}
}
p2p3_misclass <- c(p2p3_misclass,
misclass_count/(nrow(cancer_test)-null_count))
binary <- cancer_test$pred_fromp1_binary
prob <- cancer_test$pred_fromp1_prob
p2p3_deviance <- c(p2p3_deviance,
-2 * sum((binary*log(prob)) + ((1-binary)*(log(1-prob))), na.rm=TRUE) )
cancer_test['pred_logistic_prob'] <- NA
cancer_test['pred_logistic_binary'] <- NA
misclass_count <- 0
for (x in 1:nrow(cancer_test)) {
i <- cancer_test[x,'UCellSize']
j <- cancer_test[x,'SECellSize']
cancer_test[x,'pred_logistic_prob'] <- p2p2_df1[i,j]
cancer_test[x,'pred_logistic_binary'] <- p2p2_df2[i,j]
if (cancer_test[x,'pred_logistic_binary'] != cancer_test[x,'Malignant']) {
misclass_count <- misclass_count + 1
}
}
p2p3_misclass <- c(p2p3_misclass, misclass_count/(nrow(cancer_test)))
binary <- cancer_test$pred_logistic_binary
prob <- cancer_test$pred_logistic_prob
p2p3_deviance <- c(p2p3_deviance,
-2 * sum((binary*log(prob)) + ((1-binary)*(log(1-prob)))) )
cancer_test['pred_qda_prob'] <- NA
cancer_test['pred_qda_binary'] <- NA
misclass_count <- 0
for (x in 1:nrow(cancer_test)) {
i <- cancer_test[x,'UCellSize']
j <- cancer_test[x,'SECellSize']
if (p2p2_df3[i,j] == 1) {
cancer_test[x,'pred_qda_prob'] <- 1-1e-07
}
else {
cancer_test[x,'pred_qda_prob'] <- p2p2_df3[i,j]
}
cancer_test[x,'pred_qda_binary'] <- p2p2_df4[i,j]
if (cancer_test[x,'pred_qda_binary'] != cancer_test[x,'Malignant']) {
misclass_count <- misclass_count + 1
}
}
p2p3_misclass <- c(p2p3_misclass, misclass_count/(nrow(cancer_test)))
binary <- cancer_test$pred_qda_binary
prob <- cancer_test$pred_qda_prob
p2p3_deviance <- c(p2p3_deviance,
-2 * sum((binary*log(prob)) + ((1-binary)*(log(1-prob)))) )
#conclusion
p2p3_m <- c('discrete model','continuous logisitic model','continuous qda model')
data.frame(p2p3_m, p2p3_misclass, p2p3_deviance)
# yes, we could see some *clear boundaries between the three class, though there a exists some outlier points in each class.
ggplot(data=wine_train,aes(x=Color, y= OD280,color=factor(Class)))+
geom_point()
View(wine_train)
# multinomial logistic regression
wine_train_factored<-wine_train
wine_train_factored$Class<-factor(wine_train_factored$Class)
multiLogistic_classifier<- multinom(Class ~ Color + OD280, data=wine_train_factored)
# LDA
LDA_classifier<-lda(Class ~ Color + OD280, data=wine_train)
# QDA
QDA_classifier<-qda(Class ~ Color + OD280, data=wine_train)
# Naive Bayes classifier: Gaussian naive bayes
# we won't apply KDE naive bayes here since the sample size is relatively small
naiveBayes_x<-data.frame("color"=wine_train$Color,"OD280"=wine_train $OD280)
naiveBayes_y<-as.character(wine_train$Class)
naiveBayes_classifier<-naive_bayes(x=naiveBayes_x,y=naiveBayes_y,usekernel=FALSE)
# grid evaluation preparation:
# build dataframe for full range Color[1:13] and OD280[1:4]
test_color <- seq(1,13,0.05)
test_OD280 <- seq(1,4,0.01)
wine_test_df_pre <- expand.grid(test_color,test_OD280)
wine_test_df<-data.frame("Color"=wine_test_df_pre$Var1,
"OD280"=wine_test_df_pre$Var2)
# prediction from based on each classifier
# multinomial logistic regression
multiLogistic_pred<-data.frame("Color"=wine_test_df$Color,
"OD280"=wine_test_df$OD280,
"predClass"=predict(multiLogistic_classifier, newdata =wine_test_df))
ggplot(data=multiLogistic_pred,
aes(x=Color, y= OD280,color=factor(predClass)))+
geom_point()+
labs(title="multinomial logistic regression classifier")
# LDA
LDA_classifier_pred<-data.frame("Color"=wine_test_df$Color,
"OD280"=wine_test_df$OD280,
"predClass"=predict(LDA_classifier,wine_test_df)$class)
ggplot(data=LDA_classifier_pred,
aes(x=Color, y= OD280,color=factor(predClass)))+
geom_point()+
labs(title="LDA classifier")
# QDA
QDA_classifier_pred<-data.frame("Color"=wine_test_df$Color,
"OD280"=wine_test_df$OD280,
"predClass"=predict(QDA_classifier,wine_test_df)$class)
ggplot(data=QDA_classifier_pred,
aes(x=Color, y= OD280,color=factor(predClass)))+
geom_point()+
labs(title="QDA classifier")
# Naive Bayes classifier: Gaussian naive bayes
naiveBayes_classifier_pred<-data.frame("Color"=wine_test_df$Color,
"OD280"=wine_test_df$OD280,
"predClass"=predict(naiveBayes_classifier, wine_test_df, type= "class"))
#predict(naiveBayes_classifier, wine_test_df, type= "prob")
ggplot(data=naiveBayes_classifier_pred,
aes(x=Color, y= OD280,color=factor(predClass)))+
geom_point()+
labs(title="naiveBayes classifier")
classifier_kfold_checker<-function(check){
# data recode & prep
data<-wine_train # for other models
data$Class<-factor(data$Class)
# split wine_train randomly into 10-fold
set.seed(123)
folds <- create_folds(seq(1,nrow(data)), k = 10, type = "basic")
#Deviance and Misclass Prop holder
kf_deviance <- c()
kf_misclass <- c()
for(i in 1:length(folds)){
if (check == "multiLog"){
multiLogMod<- multinom(Class ~ ., data=data[folds[[i]],])
preds <- predict(multiLogMod, newdata =data[-folds[[i]],],type="probs")
preds_class <- predict(multiLogMod, newdata =data[-folds[[i]],],type="class")
}
else if(check == "LDA"){
LDA_classifier<-lda(Class ~ ., data=data[folds[[i]],])
preds <- predict(LDA_classifier,data[-folds[[i]],])$posterior
}
else if (check == "QDA"){
QDA_classifier<-qda(Class ~ ., data=data[folds[[i]],])
preds <- predict(QDA_classifier,data[-folds[[i]],])$posterior
}
else if (check == "NB"){
naiveBayes_classifier_F<-naive_bayes(as.character(Class) ~ . , data=data,usekernel=FALSE)
temp_test_pre<- data[-folds[[i]],]
temp_test<-data.frame(temp_test_pre[,-1])
preds <-predict(naiveBayes_classifier, temp_test, type= "prob")
}
#Deviance
kf_deviance[i] <-
-2 * sum(log(rowSums(model.matrix( ~ 0 + data$Class[-folds[[i]]])*preds)))
#Proportion Misclassified
kf_misclass[i] <- mean(round(preds) != data$Class[-folds[[i]]])
}
return(c(mean(kf_deviance),mean(kf_misclass)))
}
multiLog<-classifier_kfold_checker("multiLog")
LDA<-classifier_kfold_checker("LDA")
QDA<-classifier_kfold_checker("QDA")
NB<-classifier_kfold_checker("NB")
result<-data.frame(model= c("multiLog","LDA","QDA","NB"),deviance=c(multiLog[1],LDA[1],QDA[1],NB[1]),
misclass=c(multiLog[2],LDA[2],QDA[2],NB[2]))
# grid evaluation preparation:
# build dataframe for full range Color[1:13] and OD280[1:4]
test_color <- seq(1,13,0.05)
test_OD280 <- seq(1,4,0.01)
wine_test_df_pre <- expand.grid(test_color,test_OD280)
wine_test_df<-data.frame("Color"=wine_test_df_pre$Var1,
"OD280"=wine_test_df_pre$Var2)
# prediction from based on each classifier
# multinomial logistic regression
multiLogistic_pred<-data.frame("Color"=wine_test_df$Color,
"OD280"=wine_test_df$OD280,
"predClass"=predict(multiLogistic_classifier, newdata =wine_test_df))
ggplot(data=multiLogistic_pred,
aes(x=Color, y= OD280,color=factor(predClass)))+
geom_point()+
labs(title="multinomial logistic regression classifier")
# LDA
LDA_classifier_pred<-data.frame("Color"=wine_test_df$Color,
"OD280"=wine_test_df$OD280,
"predClass"=predict(LDA_classifier,wine_test_df)$class)
ggplot(data=LDA_classifier_pred,
aes(x=Color, y= OD280,color=factor(predClass)))+
geom_point()+
labs(title="LDA classifier")
# QDA
QDA_classifier_pred<-data.frame("Color"=wine_test_df$Color,
"OD280"=wine_test_df$OD280,
"predClass"=predict(QDA_classifier,wine_test_df)$class)
ggplot(data=QDA_classifier_pred,
aes(x=Color, y= OD280,color=factor(predClass)))+
geom_point()+
labs(title="QDA classifier")
# Naive Bayes classifier: Gaussian naive bayes
naiveBayes_classifier_pred<-data.frame("Color"=wine_test_df$Color,
"OD280"=wine_test_df$OD280,
"predClass"=predict(naiveBayes_classifier_F, wine_test_df, type= "class"))
