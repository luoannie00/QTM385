
---
title: 'Problem Set #4'
author: "Kevin McAlister"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: no
    toc_depth: '2'
  prettydoc::html_pretty:
    df_print: kable
    highlight: github
    theme: leonids
    toc: no
    toc_depth: 2
    toc_float:
      collapsed: no
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
---

```{r, include=FALSE}
library(ggplot2)
library(data.table)
library(tidyverse)
library(glmnet)
library(MASS)
library(splitTools)
library(naivebayes)
library(nnet)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE, fig.width = 16/2, fig.height = 9/2, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

```{r}
# data read-in
cancer_train <- read.csv('cancerTrain.csv')
cancer_test <- read.csv('cancerTest.csv')
wine_train <- read.csv('wineTrain.csv')
```
Collaborators: Billy Ge, Latifa Tan, Annie Luo

***

## Problem 1: The Multivariate Normal Distribution (20 pts.)
$$
\begin{eqnarray}
\frac{\partial \ell \ell}{\partial \mu} &=& - \frac{1}{2} \sum \limits_{i = 1}^N [\frac{\partial(\boldsymbol x_i - \boldsymbol \mu )' \Sigma^{-1} (\boldsymbol{x}_i - \boldsymbol \mu)}{\partial \mu}]\\
&=& - \frac{1}{2} \sum \limits_{i = 1}^N [2*\Sigma^{-1}*(\boldsymbol{x}_i - \boldsymbol \mu)*(-1)]\\
&=& \sum \limits_{i = 1}^N \Sigma^{-1}*(\boldsymbol{x}_i - \boldsymbol \mu)\\
0 &=& \Sigma^{-1}*\sum \limits_{i = 1}^N (\boldsymbol{x}_i - \boldsymbol \mu)\\
\Sigma* 0 &=& \Sigma\Sigma^{-1}*\sum \limits_{i = 1}^N (\boldsymbol{x}_i - \boldsymbol \mu)\\
0 &=& \sum \limits_{i = 1}^N (\boldsymbol{x}_i - \boldsymbol \mu)\\
0 &=& \sum \limits_{i = 1}^N \boldsymbol{x}_i - N*\mu\\
\hat{\mu} &=& \frac{1}{N} \sum \limits_{i = 1}^N \boldsymbol{x}_i
\end{eqnarray}
$$

$$
\begin{eqnarray}
\frac{\partial \ell \ell}{\partial \Sigma^{-1}} &=& - \frac{N}{2}*[\frac{\partial \log \text{det}(\Sigma)}{\partial \Sigma^{-1}}] -\frac{1}{2} \sum \limits_{i = 1}^N [\frac{\partial (\boldsymbol x_i - \boldsymbol \mu )' \Sigma^{-1} (\boldsymbol{x}_i - \boldsymbol \mu)}{\partial \Sigma^{-1}}]\\
&=& - \frac{N}{2}*(-\Sigma) - \frac{1}{2} \sum \limits_{i = 1}^N[(\boldsymbol{x}_i - \boldsymbol \mu)(\boldsymbol{x}_i - \boldsymbol \mu)']\\
N*\Sigma &=& \sum \limits_{i = 1}^N[(\boldsymbol{x}_i - \boldsymbol \mu)(\boldsymbol{x}_i - \boldsymbol \mu)']\\
\hat{\Sigma} &=& \frac{1}{N}\sum \limits_{i = 1}^N[(\boldsymbol{x}_i - \boldsymbol \mu)(\boldsymbol{x}_i - \boldsymbol \mu)']
\end{eqnarray}
$$

## Problem 2: Cancer Data (40 Points)
<<<<<<< Updated upstream
=======

In a number of data sets, the predictors are a collection of ordered categorical ratings of objects.  These predictors are then used to classify objects into categories.  In class, we discussed plausibly continuous predictors (like income) and unordered categorical predictors (like manufacturing country).  However, we did not spend time discussing ordered categorical predictors.  This problem will see you work with two ordered categorical predictors - rating of uniformity of cell size (`UCellSize`: 1-10 with 10 being most irregular) and single epithelial cell size (`SECellSize`: 1-10 with 10 being largest) - trying to predict whether or not a cell is cancerous (`Malignant` is one if cancerous, 0 otherwise).

The problem is that there is no in-between: we must either treat the predictor as an unordered outcome and lose any information that comes from seeing how the predictor increases or decreases while preserving its discreteness **or** treat the predictor as continuous preserving any ordering but losing its discrete nature.  Each choice comes with some downside, so we'll see how each one works on the training data set and use it to assess predictive accuracy on the test data set.

```{r}
#setwd('/Users/billyge/Desktop/Emory Fall 2022/QTM 385-4/problem sets/ps github/ps4')
setwd('/Users/annie/Desktop/QTM385/QTM350_Github/QTM350 Github/QTM385/ps4')
cancer_train <- read.csv('cancerTrain.csv')
cancer_test <- read.csv('cancerTest.csv')
wine_train <- read.csv('wineTrain.csv')
```

>>>>>>> Stashed changes
### Part 1 (15 points)
```{r}
p2p1_df1 <- data.frame(matrix(nrow = 10, ncol = 10))

malignant_df <- cancer_train %>% filter(Malignant == 1)
malignant_prob <- nrow(malignant_df)/500
non_malignant_df <- cancer_train %>% filter(Malignant == 0)
non_malignant_prob <- nrow(non_malignant_df)/500

for (i in 1:10) {
  for (j in 1:10) {
    temp_df1 <- malignant_df %>% filter(UCellSize == i & SECellSize == j)
    temp_df2 <- non_malignant_df %>% filter(UCellSize == i & SECellSize == j)
    
    if (nrow(temp_df1) == 0) {
      p2p1_df1[i,j] <- NA
      next
    }
    
    result_numerator <- nrow(temp_df1)/nrow(malignant_df) * malignant_prob
    result_denominator <- result_numerator + nrow(temp_df2)/nrow(non_malignant_df) * non_malignant_prob
    p2p1_df1[i,j] <- result_numerator/result_denominator
  }
}

p2p1_df1
```

```{r}
p2p1_df2 <- data.frame(matrix(nrow = 10, ncol = 10))

for (i in 1:10) {
  for (j in 1:10) {
    if (is.na(p2p1_df1[i,j])) {
      next
    }
    else if (p2p1_df1[i,j] > 0.5) {
      p2p1_df2[i,j] <- 1
    }
    else if (p2p1_df1[i,j] < 0.5) {
      p2p1_df2[i,j] <- 0
    }
    else if (p2p1_df1[i,j] == 0.5) {
      p2p1_df2[i,j] <- 1
    }
  }
}

p2p1_df2
```

There are many classes that have probability of cancer occurrence of 1. They typically take place when both predictors have large values. Hence, the general relationship between the two predictors and the probability that a cell is cancerous is positive, meaning that the probability of cancer occurrence increases as the values of UCellSize and SECellSize increases.

It does appear that weâ€™ve missed some information by treating this problem in an unordered discrete fashion, because there are many NA entries in our probability lookup table. The given (UCellSize, SECellSize) pair has NA probability of cancer occurrence and NA Bayes' classifier when there is no such observation pair given the response variable is malignant in the training dataset.


### Part 2 (15 points)
```{r}
#logistic regression classifier
p2p2_logistic <- glm(data=cancer_train, 
                     formula = Malignant ~ UCellSize + SECellSize,
                     family = binomial(link='logit'))

UCellSize <- rep(seq(1,10,1),10)
SECellSize <- rep(seq(1,10,1),each=10)
p2p2_df <- data.frame(UCellSize, SECellSize)
p2p2_df$logistic <- predict(p2p2_logistic, type='response', newdata=p2p2_df)

p2p2_df1 <- data.frame(matrix(nrow = 10, ncol = 10))
for (i in 1:10) {
  for (j in 1:10) {
    temp <- p2p2_df %>% filter(UCellSize==i, SECellSize==j)
    p2p2_df1[i,j] <- temp[1,'logistic']
  }
}

p2p2_df1
```

```{r}
p2p2_df2 <- data.frame(matrix(nrow = 10, ncol = 10))
for (i in 1:10) {
  for (j in 1:10) {
    if (p2p2_df1[i,j] > 0.5) {
      p2p2_df2[i,j] <- 1
    }
    else {
      p2p2_df2[i,j] <- 0
    }
  }
}

p2p2_df2
```

```{r}
#QDA classifier
p2p2_x <- cbind(cancer_train$UCellSize, cancer_train$SECellSize)
p2p2_y <- cancer_train$Malignant
p2p2_qda <- qda(x=p2p2_x, grouping=p2p2_y, formula = Malignant ~ UCellSize + SECellSize)

p2p2_testx <- cbind(p2p2_df$UCellSize, p2p2_df$SECellSize)
p2p2_df$qda <- predict(p2p2_qda, newdata=p2p2_testx)$posterior[,2]
p2p2_df3 <- data.frame(matrix(nrow = 10, ncol = 10))
for (i in 1:10) {
  for (j in 1:10) {
    temp <- p2p2_df %>% filter(UCellSize==i, SECellSize==j)
    p2p2_df3[i,j] <- temp[1,'qda']
  }
}

p2p2_df3
```

```{r}
p2p2_df4 <- data.frame(matrix(nrow = 10, ncol = 10))
for (i in 1:10) {
  for (j in 1:10) {
    if (p2p2_df3[i,j] > 0.5) {
      p2p2_df4[i,j] <- 1
    }
    else {
      p2p2_df4[i,j] <- 0
    }
  }
}

p2p2_df4
```

The main difference between tables in part 2 compared to those in part 1 is the NA entry. We don't see any NA entries in tables from part 2. By treating the predictors as continuous when they are truly discrete, it does not seem that we have lost anything. But, the decision boundaries in continuous logistic regression model and continuous qda model are very clean, whereas the decision boundary in the discrete model is more wiggly.


### Part 3 (10 points)
```{r}
p2p3_misclass <- c()
p2p3_deviance <- c()
```

```{r}
cancer_test['pred_fromp1_prob'] <- NA
cancer_test['pred_fromp1_binary'] <- NA
misclass_count <- 0
null_count <- 0

for (x in 1:nrow(cancer_test)) {
  i <- cancer_test[x,'UCellSize']
  j <- cancer_test[x,'SECellSize']
  if (is.na(p2p1_df1[i,j])) {
    null_count <- null_count + 1
    next
  }
  else {
    cancer_test[x,'pred_fromp1_prob'] <- p2p1_df1[i,j]
    cancer_test[x,'pred_fromp1_binary'] <- p2p1_df2[i,j]
    if (cancer_test[x,'pred_fromp1_binary'] != cancer_test[x,'Malignant']) {
      misclass_count <- misclass_count + 1
    }
  }
}

p2p3_misclass <- c(p2p3_misclass, 
                   misclass_count/(nrow(cancer_test)-null_count))

binary <- cancer_test$pred_fromp1_binary
prob <- cancer_test$pred_fromp1_prob
p2p3_deviance <- c(p2p3_deviance,
                   -2 * sum((binary*log(prob)) + ((1-binary)*(log(1-prob))), na.rm=TRUE) )
```

```{r}
cancer_test['pred_logistic_prob'] <- NA
cancer_test['pred_logistic_binary'] <- NA

misclass_count <- 0
for (x in 1:nrow(cancer_test)) {
  i <- cancer_test[x,'UCellSize']
  j <- cancer_test[x,'SECellSize']
  cancer_test[x,'pred_logistic_prob'] <- p2p2_df1[i,j]
  cancer_test[x,'pred_logistic_binary'] <- p2p2_df2[i,j]
  if (cancer_test[x,'pred_logistic_binary'] != cancer_test[x,'Malignant']) {
    misclass_count <- misclass_count + 1
  }
}

p2p3_misclass <- c(p2p3_misclass, misclass_count/(nrow(cancer_test)))

binary <- cancer_test$pred_logistic_binary
prob <- cancer_test$pred_logistic_prob
p2p3_deviance <- c(p2p3_deviance,
                   -2 * sum((binary*log(prob)) + ((1-binary)*(log(1-prob)))) )
```

```{r}
cancer_test['pred_qda_prob'] <- NA
cancer_test['pred_qda_binary'] <- NA

misclass_count <- 0
for (x in 1:nrow(cancer_test)) {
  i <- cancer_test[x,'UCellSize']
  j <- cancer_test[x,'SECellSize']
  if (p2p2_df3[i,j] == 1) {
    cancer_test[x,'pred_qda_prob'] <- 1-1e-07
  }
  else {
    cancer_test[x,'pred_qda_prob'] <- p2p2_df3[i,j]
  }
  cancer_test[x,'pred_qda_binary'] <- p2p2_df4[i,j]
  if (cancer_test[x,'pred_qda_binary'] != cancer_test[x,'Malignant']) {
    misclass_count <- misclass_count + 1
  }
}

p2p3_misclass <- c(p2p3_misclass, misclass_count/(nrow(cancer_test)))

binary <- cancer_test$pred_qda_binary
prob <- cancer_test$pred_qda_prob
p2p3_deviance <- c(p2p3_deviance,
                   -2 * sum((binary*log(prob)) + ((1-binary)*(log(1-prob)))) )
```

```{r}
#conclusion
p2p3_m <- c('discrete model','continuous logisitic model','continuous qda model')
data.frame(p2p3_m, p2p3_misclass, p2p3_deviance)
```

In terms of the misclassification rate, all three models are similar. The discrete model has a larger value in misclassification rate because it has a smaller denominator in calculating the misclassification rate (due to removing the NA entries).

In terms of deviance, the continuous qda model performs best, the discrete model ranks next, and the continuous logistic model is the worst.

We expect the unordered discrete model to perform better than the ordered continuous models when there is not strong association between predictors. If that is the case, the probability distribution of predicting the response variable is very wiggly, where the decision boundary cannot be modeled by logistic or qda methods. Otherwise, it is not worth to apply the discrete model since its largest weakness is the NA entries when the training dataset is small.


## Problem 3: Wine Data (40 points)
### Part 1 (15 points)
#### Show the predictors(color&OD280) values and approximate boundaries
```{r}
# yes, we could see some *clear boundaries between the three class, though there a exists some outlier points in each class.
ggplot(data=wine_train,aes(x=Color, y= OD280,color=factor(Class)))+
  geom_point()
```

#### Implementation of Classifiers for Color and OD280
```{r}
# multinomial logistic regression 
wine_train_factored<-wine_train
wine_train_factored$Class<-factor(wine_train_factored$Class)
multiLogistic_classifier<- multinom(Class ~ Color + OD280, data=wine_train_factored)
# LDA 
LDA_classifier<-lda(Class ~ Color + OD280, data=wine_train)
# QDA 
QDA_classifier<-qda(Class ~ Color + OD280, data=wine_train)
# Naive Bayes classifier: Gaussian naive bayes
# we won't apply KDE naive bayes here since the sample size is relatively small
naiveBayes_x<-data.frame("color"=wine_train$Color,"OD280"=wine_train $OD280)
naiveBayes_y<-as.character(wine_train$Class)
naiveBayes_classifier_F<-naive_bayes(as.character(Class) ~ Color + OD280, data=wine_train,usekernel=FALSE)

```

#### grid evaluation and minimun bounding box
From the graphs below, we could see that the decision boundary is approximately the same over all classifiers.The QDA and naive bayes classifier might doing better on capturing the true decision boundary, as both have quadratic shape of boundries.
```{r}
# grid evaluation preparation: 
# build dataframe for full range Color[1:13] and OD280[1:4]
test_color <- seq(1,13,0.05)
test_OD280 <- seq(1,4,0.01)
wine_test_df_pre <- expand.grid(test_color,test_OD280)
wine_test_df<-data.frame("Color"=wine_test_df_pre$Var1,
                         "OD280"=wine_test_df_pre$Var2)
# prediction from based on each classifier
# multinomial logistic regression 
multiLogistic_pred<-data.frame("Color"=wine_test_df$Color,
                               "OD280"=wine_test_df$OD280,
  "predClass"=predict(multiLogistic_classifier, newdata =wine_test_df))
ggplot(data=multiLogistic_pred,
       aes(x=Color, y= OD280,color=factor(predClass)))+
  geom_point()+
  labs(title="multinomial logistic regression classifier")+
  geom_point(data=wine_train,aes(x=Color, y= OD280,color=factor(Class)))
  
# LDA
LDA_classifier_pred<-data.frame("Color"=wine_test_df$Color,
                                "OD280"=wine_test_df$OD280,
                "predClass"=predict(LDA_classifier,wine_test_df)$class)
ggplot(data=LDA_classifier_pred,
       aes(x=Color, y= OD280,color=factor(predClass)))+
  geom_point()+
  labs(title="LDA classifier")+
  geom_point(data=wine_train,aes(x=Color, y= OD280,color=factor(Class)))

# QDA
QDA_classifier_pred<-data.frame("Color"=wine_test_df$Color,
                                "OD280"=wine_test_df$OD280,
        "predClass"=predict(QDA_classifier,wine_test_df)$class)
ggplot(data=QDA_classifier_pred,
       aes(x=Color, y= OD280,color=factor(predClass)))+
  geom_point()+
  labs(title="QDA classifier")+
  geom_point(data=wine_train,aes(x=Color, y= OD280,color=factor(Class)))

# Naive Bayes classifier: Gaussian naive bayes
naiveBayes_classifier_pred<-data.frame("Color"=wine_test_df$Color,
                                "OD280"=wine_test_df$OD280,
"predClass"=predict(naiveBayes_classifier_F, wine_test_df, type= "class"))
#predict(naiveBayes_classifier, wine_test_df, type= "prob")
ggplot(data=naiveBayes_classifier_pred,
       aes(x=Color, y= OD280,color=factor(predClass)))+
  geom_point()+
  labs(title="naiveBayes classifier")+
  geom_point(data=wine_train,aes(x=Color, y= OD280,color=factor(Class)))
```

### Part 2 (15 points)
#### 10-fold estimate of the deviance and the misclassification rate over all variables
The misclassification rates for all classifiers are quite close to one another(maybe they are slightly different if we have more decimal digits but they looks really the same with what we got here). From the deviance perspect, we could see that QDA performs the best here(with LDA have a bit higher deviance), while multinomial logistic regression classifier have a relatively huge deviance.[but also do notice that how the deviance enlarge the differences among the models]
```{r}
classifier_kfold_checker<-function(check){
  # data recode & prep
  data<-wine_train # for other models
  data$Class<-factor(data$Class)
  # split wine_train randomly into 10-fold
<<<<<<< Updated upstream
  set.seed(1234)
=======
  set.seed(1027)
>>>>>>> Stashed changes
  folds <- create_folds(seq(1,nrow(data)), k = 10, type = "basic")
  #Deviance and Misclass Prop holder
  kf_deviance <- c()
  kf_misclass <- c()
  for(i in 1:length(folds)){
    if (check == "multiLog"){
      multiLogMod<- multinom(Class ~ ., data=data[folds[[i]],])
      preds <- predict(multiLogMod, newdata =data[-folds[[i]],],type="probs")
    }
    else if(check == "LDA"){
      LDA_classifier<-lda(Class ~ ., data=data[folds[[i]],])
      preds <- predict(LDA_classifier,data[-folds[[i]],])$posterior
    }
    else if (check == "QDA"){
      QDA_classifier<-qda(Class ~ ., data=data[folds[[i]],])
      preds <- predict(QDA_classifier,data[-folds[[i]],])$posterior
    }
    else if (check == "NB"){
      naiveBayes_classifier<-naive_bayes(as.character(Class) ~ . , data=data,usekernel=FALSE)
      temp_test_pre<- data[-folds[[i]],]
      temp_test<-data.frame(temp_test_pre[,-1])
      preds <-predict(naiveBayes_classifier, temp_test, type= "prob")
    }
    #Deviance
    kf_deviance[i] <- 
      -2 * sum(log(rowSums(model.matrix( ~ 0 + data$Class[-folds[[i]]])*preds)))
    #Proportion Misclassified
    kf_misclass[i] <- mean(round(preds) != data$Class[-folds[[i]]])
  }
  return(c(mean(kf_deviance),mean(kf_misclass)))
}
multiLog<-classifier_kfold_checker("multiLog")
LDA<-classifier_kfold_checker("LDA")
QDA<-classifier_kfold_checker("QDA")
NB<-classifier_kfold_checker("NB")
result<-data.frame(model= c("multiLog","LDA","QDA","NB"),deviance=c(multiLog[1],LDA[1],QDA[1],NB[1]),
                   misclass=c(multiLog[2],LDA[2],QDA[2],NB[2]))
result
```


### Part 3 
Both NB and QDA can be viewed as methods trying to maximize the log odds of posterior probability $\log( \frac{Pr(Y=k | X=x)}{Pr(Y=K | X=x)})$ with different assumptions. QDA relies on the multivariate normal density (MND) assumption where, for each class, the predictors are drawn from a multivariate normal density function. In other words, QDA assumes that the marginal distribution of predictors are normal. With this assumption, QDA models the log value in a quadratic way in the sense that it allows interaction of pairs of predictors. NB relies on the assumption of independence among predictors and thus doesn't allow interaction terms. NB models the log value in a generative additive way, and thus is more flexible. From a loss perspective, NB is likely to outperform QDA when the predictor dependencies are not very important in discriminating classes, as QDA has the advantage of capturing such dependencies but is not very flexible in functional forms, while NB is more flexible in an additive way but does not model any interactions among predictors.
<<<<<<< Updated upstream
In the MLE calculated above with MND assumption required by QDA, we can see that the $\hat{\Sigma}$ is a matrix whose size is determined by the length of predictors vector. More specifically, $\hat{\Sigma}$ matrix is a $P \times P$ covariance matrix, where $P$ is the number of predictors. When $P$ gets too large, the computation of $\hat{\Sigma}$ can be very costly. Therefore, from a computational perspective, when there are too many predictors, NB is likely to outperform QDA.
=======
In the MLE calculated above with MND assumption required by QDA, we can see that the $\hat{\Sigma}$ is a matrix whose size is determined by the length of predictors vector. More specifically, $\hat{\Sigma}$ matrix is a $P \times P$ covariance matrix, where $P$ is the number of predictors. Even though there are $p \times \frac{P-1}{2}$ free parameters due to symmatry of the variance-covariance matrix, when $P$ gets too large, the computation of $\hat{\Sigma}$ can still be very costly. Therefore, from a computational perspective, when there are too many predictors, NB is likely to outperform QDA.
  
***

## Computational Resources

This problem set covers a number of different methods for creating classifiers.  Here, I'll discuss a few computational resources that will assist you when completing this problem set.

### Logistic Regression

In R, logistic regression is pretty easy to implement.  Here, we just use the `glm()` function included in base R.  For training logistic regression models, the only argument that makes the setup any different than standard linear models is `family = binomial(link = "logit")` which tells the function to compute the outcome using the logistic link function.

When producing predictions using `glm()`, `predict` works in the same way as with `lm()`.  However, there is an additional argument, `type = `.  The default type of prediction is the `link` prediction which produces log-odds for a standard logistic regression model.  Setting `type = 'response'` returns the probability that the observation belongs to class 1.  You can get the probability that the observation belongs to class 0 by subtracting the returned probability from 1.

### Multinomial Logistic Regression

There are many implementations of multinomial logistic regression that all have similar flavors.  My preferred implementation is the `multinom()` function in the `nnet` package.  `multinom` functions much like `lm()` in that it allows you to use formula notation to implement your model (e.g. `y ~ ., data = df` to run a multinomial logistic regression on the response using all other variables as features).  The key difference here is that `y` **must be coded as a factor**.  You can convert the output to a factor by using `factor()`.  The regression will then run on the $K$-level factor.

The coefficients for multinomial logistic regression models aren't all that meaningful, so we'll just talk about the predictions methods.  To create predictions using `multinom()`, you just use `predict()` as with `lm()`.  There are two types of predictions that can be produced - `type = 'class'` produces the class with the highest probability while `type = 'prob'` produces a matrix with $K$ columns that shows the predicted probability that each observation belongs to each class.  Using these probabilities, you can produce any error metrics you wish.

### Quadratic Discriminant Analysis

QDA can be implemented easily in R using the `qda()` function in the `MASS` package.  Again, QDA can be implemented using the familiar formula framework.  As with multinomial logistic regression, the response variable should be converted to a factor.

QDA has a predict method that, by default, will return a list with two elements - the first is the class associated with the highest probability of occurrence and the second is a $K$ column matrix that includes the probabilities that the observation belongs to each class.

### Naive Bayes

There are lots of implementations of a naive Bayes' classifier in R.  My preferred implementation is `naive_bayes` in the `naivebayes` package.  Again, NB can be implemented using the standard formula language.  `y` should be coded as a factor (though this implementation accepts character vectors or logicals).  There is one argument which you might find useful in later applications of NB - `usekernel`.  The default argument of `usekernel = FALSE` assumes that any numeric features should be modeled using conditionally independent normal distributions.  If `usekernel = TRUE`, then the marginal distribution for each feature will be constructed using a kernel density estimator.  In theory, with large N, this would provide better estimates of the probability that we would see a value given the class label.  However, using KDE on small N data sets can lead to overfitting.

NB's predict method closely mimics that of `multinom` in `nnet`.  There are two types: `class` which returns the class with the highest probability of occurrence and `prob` which returns a $K$-column matrix of probabilities that each observation belongs to each class.

### K-Fold Cross Validation

All of the classifiers used in the third question do not admit a clean AIC, BIC, or analytical formula for CV metrics.  Therefore, you'll need to use K-fold cross validation to get a reasonable estimate of the expected prediction error.  For these problems, I'm asking you to compute two different metrics: misclassification rate with respect to the Bayes' classifer and the deviance - $-2 \times \sum \limits_{i = 1}^N \log P(\hat{y}_i = y_i)$.

I think that the easiest way to compute these K-fold metrics is to program the split and evaluation yourself.  Using packages like `caret` limit the fit metrics that can be returned without significant additional work.  For R users, there's a really great package called `splitTools` that does the hard work of creating folds for you.  In this package, the function `create_folds` takes a vector of indices and returns a list with $K$ elements.  Each list element contains a vector of indices that correspond to that fold's **training data**.  The omitted indices correspond to that fold's **validation set**.  Once this list is created, you can loop over the list.  For example:

```{r, include=TRUE, eval = FALSE}
#Let data be a N x P data frame
#Make a random matrix
set.seed(12345)
data <- as.data.frame(matrix(ncol = 100, nrow = 1000, rnorm(100000)))
#Binary Outcome
data$V100 <- round(runif(1000))
folds <- splitTools::create_folds(seq(1,nrow(data)), k = 10, type = "basic")
#Deviance and Misclass Prop holder
kf_deviance <- c()
kf_misclass <- c()
for(i in 1:length(folds)){
  #Compute logit model using all but holdouts
  logit_mod <- glm(V100 ~ ., data = data[folds[[i]],], 
                   family = binomial(link = "logit"))
  #Predict holdouts
  preds <- predict(logit_mod, newdata = data[-folds[[i]],], type = "response")
  #Deviance
  kf_deviance[i] <- -2*sum((data$V100[-folds[[i]]]*log(preds)) + ((1 - data$V100[-folds[[i]]])*(log(1 - preds))))
  #Proportion Misclassified
  kf_misclass[i] <- mean(round(preds) != data$V100[-folds[[i]]])
}
```



>>>>>>> Stashed changes
